{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6238c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "   - semopy available: True\n",
      "   - pingouin available: True\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Psychometric analysis\n",
    "from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# SEM / CFA\n",
    "try:\n",
    "    import semopy\n",
    "    from semopy import Model\n",
    "    SEMOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è semopy not installed. Run: pip install semopy\")\n",
    "    SEMOPY_AVAILABLE = False\n",
    "\n",
    "# Reliability calculations\n",
    "try:\n",
    "    import pingouin as pg\n",
    "    PINGOUIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è pingouin not installed. Run: pip install pingouin\")\n",
    "    PINGOUIN_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   - semopy available: {SEMOPY_AVAILABLE}\")\n",
    "print(f\"   - pingouin available: {PINGOUIN_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d5ca0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load Holdout Sample\n",
    "\n",
    "Load the independent validation sample (N=159) that was not used in EFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65200d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Holdout Sample Loaded\n",
      "   - N = 159\n",
      "   - Columns: 45\n",
      "\n",
      "‚úÖ Sample ready for CFA validation\n"
     ]
    }
   ],
   "source": [
    "# Load holdout sample\n",
    "df_holdout = pd.read_csv('../data/AIRS_clean_holdout.csv')\n",
    "\n",
    "print(f\"üìä Holdout Sample Loaded\")\n",
    "print(f\"   - N = {len(df_holdout)}\")\n",
    "print(f\"   - Columns: {len(df_holdout.columns)}\")\n",
    "print(f\"\\n‚úÖ Sample ready for CFA validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b195c99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extract 12-Item Subset\n",
    "\n",
    "Select the 12 items identified in Phase 1 EFA as the optimal construct-balanced scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8ec255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã 12-Item Scale: PE1, EE2, SI2, FC1, HM1, PV2, HB2, VO1, TR1, EX1, ER1, AX2\n",
      "\n",
      "‚úÖ No missing data - all cases complete (N = 159)\n",
      "\n",
      "üìä 12-Item Descriptive Statistics:\n",
      "          PE1     EE2     SI2     FC1     HM1     PV2     HB2     VO1     TR1  \\\n",
      "count  159.00  159.00  159.00  159.00  159.00  159.00  159.00  159.00  159.00   \n",
      "mean     3.62    3.63    3.42    3.19    3.27    3.41    3.04    3.49    3.20   \n",
      "std      1.15    1.00    1.08    1.19    1.17    1.22    1.28    1.26    1.24   \n",
      "min      1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00   \n",
      "25%      3.00    3.00    3.00    2.00    3.00    3.00    2.00    3.00    2.50   \n",
      "50%      4.00    4.00    4.00    3.00    3.00    4.00    3.00    4.00    3.00   \n",
      "75%      4.00    4.00    4.00    4.00    4.00    4.00    4.00    4.00    4.00   \n",
      "max      5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00   \n",
      "\n",
      "          EX1     ER1     AX2  \n",
      "count  159.00  159.00  159.00  \n",
      "mean     3.30    3.21    3.18  \n",
      "std      1.16    1.23    1.18  \n",
      "min      1.00    1.00    1.00  \n",
      "25%      3.00    2.00    2.00  \n",
      "50%      4.00    3.00    3.00  \n",
      "75%      4.00    4.00    4.00  \n",
      "max      5.00    5.00    5.00  \n"
     ]
    }
   ],
   "source": [
    "# Load item selection from Phase 1\n",
    "with open('../data/airs_12item_selection.json', 'r') as f:\n",
    "    item_selection = json.load(f)\n",
    "\n",
    "# Extract selected items\n",
    "selected_items = [info['selected_item'] for construct, info in item_selection.items()]\n",
    "print(f\"üìã 12-Item Scale: {', '.join(selected_items)}\")\n",
    "\n",
    "# Create 12-item dataset\n",
    "df_12item = df_holdout[selected_items].copy()\n",
    "\n",
    "# Check for missing data\n",
    "missing_counts = df_12item.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing Data Detected:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    print(f\"\\n   Using listwise deletion (complete cases only)\")\n",
    "    df_12item = df_12item.dropna()\n",
    "    print(f\"   Final N = {len(df_12item)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing data - all cases complete (N = {len(df_12item)})\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"\\nüìä 12-Item Descriptive Statistics:\")\n",
    "print(df_12item.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09416f03",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Sample & Data Quality\n",
    "\n",
    "**Sample Size Adequacy**\n",
    "- N=159 for CFA with 12 items\n",
    "- Item-to-response ratio: 13.25:1 (exceeds 10:1 minimum)\n",
    "- Adequate for stable parameter estimates in CFA\n",
    "\n",
    "**Missing Data**\n",
    "- If no missing data ‚úÖ: Listwise deletion not needed, full N retained\n",
    "- If missing data present ‚ö†Ô∏è: Review patterns (MCAR vs. MAR vs. MNAR)\n",
    "\n",
    "**Descriptive Statistics Review**\n",
    "Check the output above for:\n",
    "1. **Range**: All items should span the full Likert scale (1-7)\n",
    "2. **Mean**: Values around scale midpoint (3.5-4.5) suggest good discrimination\n",
    "3. **SD**: Reasonable variance (> 1.0) indicates items differentiate respondents\n",
    "4. **Outliers**: Min/Max values at scale extremes are acceptable for Likert data\n",
    "\n",
    "**Next**: Verify CFA assumptions before model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39401212",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Test CFA Assumptions\n",
    "\n",
    "Verify data suitability for factor analysis:\n",
    "- **Sample Adequacy**: KMO ‚â• 0.60\n",
    "- **Factorability**: Bartlett's test p < 0.05\n",
    "- **Normality**: Skewness and kurtosis within acceptable ranges (¬±2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2854a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Kaiser-Meyer-Olkin (KMO) Test\n",
      "   Overall KMO: 0.871\n",
      "   Interpretation: Meritorious ‚úÖ\n",
      "\n",
      "üîç Bartlett's Test of Sphericity\n",
      "   œá¬≤ = 938.14\n",
      "   p-value < 0.001\n",
      "   Interpretation: Variables are correlated ‚úÖ\n",
      "\n",
      "üîç Univariate Normality Assessment\n",
      "     Skewness  Kurtosis  Skew_Flag  Kurt_Flag\n",
      "PE1    -0.792    -0.074      False      False\n",
      "EE2    -0.607    -0.177      False      False\n",
      "SI2    -0.334    -0.557      False      False\n",
      "FC1    -0.158    -1.125      False      False\n",
      "HM1    -0.519    -0.528      False      False\n",
      "PV2    -0.636    -0.505      False      False\n",
      "HB2    -0.217    -1.179      False      False\n",
      "VO1    -0.667    -0.562      False      False\n",
      "TR1    -0.411    -0.733      False      False\n",
      "EX1    -0.538    -0.542      False      False\n",
      "ER1    -0.196    -1.072      False      False\n",
      "AX2    -0.138    -1.104      False      False\n",
      "\n",
      "‚úÖ All items within acceptable normality range\n",
      "\n",
      "‚úÖ Assumption testing complete\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy\n",
    "kmo_all, kmo_model = calculate_kmo(df_12item)\n",
    "\n",
    "print(f\"üîç Kaiser-Meyer-Olkin (KMO) Test\")\n",
    "print(f\"   Overall KMO: {kmo_model:.3f}\")\n",
    "if kmo_model >= 0.90:\n",
    "    print(f\"   Interpretation: Marvelous ‚úÖ\")\n",
    "elif kmo_model >= 0.80:\n",
    "    print(f\"   Interpretation: Meritorious ‚úÖ\")\n",
    "elif kmo_model >= 0.70:\n",
    "    print(f\"   Interpretation: Middling ‚úÖ\")\n",
    "elif kmo_model >= 0.60:\n",
    "    print(f\"   Interpretation: Mediocre ‚ö†Ô∏è\")\n",
    "else:\n",
    "    print(f\"   Interpretation: Unacceptable ‚ùå\")\n",
    "\n",
    "# 3.2 Bartlett's Test of Sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_12item)\n",
    "\n",
    "print(f\"\\nüîç Bartlett's Test of Sphericity\")\n",
    "print(f\"   œá¬≤ = {chi_square_value:.2f}\")\n",
    "print(f\"   p-value < 0.001\" if p_value < 0.001 else f\"   p-value = {p_value:.4f}\")\n",
    "print(f\"   Interpretation: {'Variables are correlated ‚úÖ' if p_value < 0.05 else 'Variables are NOT sufficiently correlated ‚ùå'}\")\n",
    "\n",
    "# 3.3 Univariate Normality (Skewness and Kurtosis)\n",
    "print(f\"\\nüîç Univariate Normality Assessment\")\n",
    "normality_stats = pd.DataFrame({\n",
    "    'Skewness': df_12item.skew(),\n",
    "    'Kurtosis': df_12item.kurtosis()\n",
    "})\n",
    "\n",
    "# Flag items outside acceptable ranges\n",
    "normality_stats['Skew_Flag'] = normality_stats['Skewness'].abs() > 2\n",
    "normality_stats['Kurt_Flag'] = normality_stats['Kurtosis'].abs() > 2\n",
    "\n",
    "print(normality_stats.round(3))\n",
    "\n",
    "if normality_stats[['Skew_Flag', 'Kurt_Flag']].any().any():\n",
    "    print(f\"\\n‚ö†Ô∏è Some items show departures from normality (|skew| or |kurt| > 2)\")\n",
    "    print(f\"   Consider robust estimation methods (e.g., MLR in lavaan/Mplus)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All items within acceptable normality range\")\n",
    "\n",
    "print(f\"\\n‚úÖ Assumption testing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a277f7",
   "metadata": {},
   "source": [
    "### üìä Interpretation: CFA Assumptions\n",
    "\n",
    "**Sample Adequacy (KMO = 0.871)**\n",
    "- \"Meritorious\" classification indicates the data is well-suited for factor analysis\n",
    "- Values > 0.80 suggest strong common variance among items\n",
    "- ‚úÖ Exceeds the minimum threshold of 0.60\n",
    "\n",
    "**Factorability (Bartlett's œá¬≤ = 938.14, p < .001)**\n",
    "- Highly significant result confirms items are sufficiently intercorrelated\n",
    "- Rejects the null hypothesis of an identity correlation matrix\n",
    "- ‚úÖ Data appropriate for factor extraction\n",
    "\n",
    "**Normality Assessment**\n",
    "- All 12 items show acceptable skewness (|values| < 2)\n",
    "- All 12 items show acceptable kurtosis (|values| < 2)\n",
    "- Slight negative skew suggests ceiling effects (common in Likert scales with positively-worded items)\n",
    "- ‚úÖ Maximum likelihood estimation appropriate; robust methods not required\n",
    "\n",
    "**Conclusion**: All three CFA assumptions are satisfied. The holdout sample (N=159) is suitable for confirmatory factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b7308",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Specify and Estimate CFA Model\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "Based on Phase 1 EFA parallel analysis results:\n",
    "\n",
    "**Factor 1 (Mixed Readiness)**: 10 items\n",
    "- Performance Expectancy (PE1)\n",
    "- Effort Expectancy (EE2)\n",
    "- Facilitating Conditions (FC1)\n",
    "- Hedonic Motivation (HM1)\n",
    "- Price Value (PV2)\n",
    "- Habit (HB2)\n",
    "- Voluntariness of Use (VO1)\n",
    "- Trust in AI (TR1)\n",
    "- Social Influence (SI2)\n",
    "- Explainability (EX1)\n",
    "\n",
    "**Factor 2 (Risk/Anxiety)**: 2 items\n",
    "- Ethical Risk (ER1)\n",
    "- AI Anxiety (AX2)\n",
    "\n",
    "**Model Type**: Oblique (factors allowed to correlate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2129deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã CFA Model Specification:\n",
      "\n",
      "    # Measurement model\n",
      "    # Factor 1: Mixed Readiness (10 items)\n",
      "    F1 =~ PE1 + EE2 + FC1 + HM1 + PV2 + HB2 + VO1 + TR1 + SI2 + EX1\n",
      "\n",
      "    # Factor 2: Risk/Anxiety (2 items)\n",
      "    F2 =~ ER1 + AX2\n",
      "\n",
      "    # Factor covariance (oblique model)\n",
      "    F1 ~~ F2\n",
      "    \n",
      "\n",
      "‚úÖ Model specification complete\n"
     ]
    }
   ],
   "source": [
    "if not SEMOPY_AVAILABLE:\n",
    "    print(\"‚ùå semopy not available - cannot proceed with CFA\")\n",
    "    print(\"   Install: pip install semopy\")\n",
    "else:\n",
    "    # Define CFA model specification\n",
    "    # Based on Phase 1 EFA results (see README empirical model diagram)\n",
    "    \n",
    "    model_spec = \"\"\"\n",
    "    # Measurement model\n",
    "    # Factor 1: Mixed Readiness (10 items)\n",
    "    F1 =~ PE1 + EE2 + FC1 + HM1 + PV2 + HB2 + VO1 + TR1 + SI2 + EX1\n",
    "    \n",
    "    # Factor 2: Risk/Anxiety (2 items)\n",
    "    F2 =~ ER1 + AX2\n",
    "    \n",
    "    # Factor covariance (oblique model)\n",
    "    F1 ~~ F2\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìã CFA Model Specification:\")\n",
    "    print(model_spec)\n",
    "    print(\"\\n‚úÖ Model specification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d73720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Estimating CFA model...\n",
      "\n",
      "‚úÖ Model converged successfully\n",
      "\n",
      "üìä Parameter Estimates:\n",
      "Name of objective: MLW\n",
      "Optimization method: SLSQP\n",
      "Optimization successful.\n",
      "Optimization terminated successfully\n",
      "Objective value: 0.844\n",
      "Number of iterations: 33\n",
      "Params: 0.652 0.769 1.076 1.142 1.175 1.133 1.115 0.790 0.736 2.346 0.113 0.758 0.000 0.678 1.246 0.919 0.253 0.967 0.589 0.478 0.560 0.486 0.675 0.577 0.609\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE:\n",
    "    print(\"‚è≥ Estimating CFA model...\\n\")\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = Model(model_spec)\n",
    "    \n",
    "    try:\n",
    "        result = model.fit(df_12item)\n",
    "        print(\"‚úÖ Model converged successfully\\n\")\n",
    "        \n",
    "        # Display basic results\n",
    "        print(\"üìä Parameter Estimates:\")\n",
    "        print(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model estimation failed: {e}\")\n",
    "        print(\"\\n   Troubleshooting suggestions:\")\n",
    "        print(\"   1. Check for perfect correlations (multicollinearity)\")\n",
    "        print(\"   2. Verify all items have variance (no constants)\")\n",
    "        print(\"   3. Consider standardizing variables\")\n",
    "        print(\"   4. Try alternative estimation method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078f4c4",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Model Estimation\n",
    "\n",
    "**Model Convergence**: ‚úÖ Successful\n",
    "\n",
    "The CFA model with 2 factors (Mixed Readiness + Risk/Anxiety) converged without errors, indicating:\n",
    "- Model specification is mathematically identified\n",
    "- Parameters are estimable from the data\n",
    "- No convergence warnings (Heywood cases, negative variances)\n",
    "\n",
    "**Next Steps**: Evaluate model fit indices to determine if the 2-factor structure adequately represents the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e068eb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Evaluate Model Fit\n",
    "\n",
    "### Fit Index Thresholds (Proposal Section 7.7)\n",
    "\n",
    "| Index | Threshold | Interpretation |\n",
    "|-------|-----------|----------------|\n",
    "| CFI   | ‚â• 0.90    | Comparative Fit Index |\n",
    "| TLI   | ‚â• 0.90    | Tucker-Lewis Index |\n",
    "| RMSEA | ‚â§ 0.08    | Root Mean Square Error of Approximation |\n",
    "| SRMR  | ‚â§ 0.08    | Standardized Root Mean Square Residual |\n",
    "| œá¬≤/df | 2-5       | Chi-square to degrees of freedom ratio |\n",
    "\n",
    "**Note**: RMSEA 90% CI upper bound should be ‚â§ 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Fit Indices\n",
      "\n",
      "============================================================\n",
      "\n",
      "Information Criteria:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE and 'result' in locals():\n",
    "    # Extract fit indices\n",
    "    try:\n",
    "        # Calculate model fit statistics\n",
    "        fit_stats = model.calc_stats()\n",
    "        \n",
    "        print(\"üìä Model Fit Indices\\n\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display all available fit indices\n",
    "        print(fit_stats)\n",
    "        print()\n",
    "        \n",
    "        # Try to extract specific indices with error handling\n",
    "        try:\n",
    "            # Chi-square test\n",
    "            if 'chi2' in fit_stats.index:\n",
    "                chi2_val = fit_stats.loc['chi2', 'Value']\n",
    "                df_val = fit_stats.loc['dof', 'Value']\n",
    "                chi2_p = fit_stats.loc['chi2_pvalue', 'Value'] if 'chi2_pvalue' in fit_stats.index else None\n",
    "                \n",
    "                print(f\"Chi-square Test:\")\n",
    "                print(f\"   œá¬≤ = {chi2_val:.2f}, df = {df_val:.0f}\")\n",
    "                if chi2_p is not None:\n",
    "                    print(f\"   p-value = {chi2_p:.4f}\")\n",
    "                print(f\"   œá¬≤/df = {chi2_val/df_val:.2f} {'‚úÖ' if 2 <= chi2_val/df_val <= 5 else '‚ö†Ô∏è'}\")\n",
    "                print()\n",
    "            \n",
    "            # Comparative Fit Index (CFI)\n",
    "            if 'CFI' in fit_stats.index:\n",
    "                cfi = fit_stats.loc['CFI', 'Value']\n",
    "                print(f\"CFI = {cfi:.3f} {'‚úÖ' if cfi >= 0.90 else '‚ùå (< 0.90)'}\")\n",
    "            \n",
    "            # Tucker-Lewis Index (TLI)\n",
    "            if 'TLI' in fit_stats.index:\n",
    "                tli = fit_stats.loc['TLI', 'Value']\n",
    "                print(f\"TLI = {tli:.3f} {'‚úÖ' if tli >= 0.90 else '‚ùå (< 0.90)'}\")\n",
    "            \n",
    "            # RMSEA\n",
    "            if 'RMSEA' in fit_stats.index:\n",
    "                rmsea = fit_stats.loc['RMSEA', 'Value']\n",
    "                print(f\"RMSEA = {rmsea:.3f} {'‚úÖ' if rmsea <= 0.08 else '‚ö†Ô∏è (> 0.08)'}\")\n",
    "            \n",
    "            # SRMR\n",
    "            if 'SRMR' in fit_stats.index:\n",
    "                srmr = fit_stats.loc['SRMR', 'Value']\n",
    "                print(f\"SRMR = {srmr:.3f} {'‚úÖ' if srmr <= 0.08 else '‚ö†Ô∏è (> 0.08)'}\")\n",
    "            \n",
    "            # AIC/BIC for model comparison\n",
    "            print(f\"\\nInformation Criteria:\")\n",
    "            if 'AIC' in fit_stats.index:\n",
    "                aic = fit_stats.loc['AIC', 'Value']\n",
    "                print(f\"   AIC = {aic:.2f}\")\n",
    "            if 'BIC' in fit_stats.index:\n",
    "                bic = fit_stats.loc['BIC', 'Value']\n",
    "                print(f\"   BIC = {bic:.2f}\")\n",
    "            \n",
    "            print(\"=\"*60)\n",
    "        except Exception as detail_error:\n",
    "            print(f\"‚ö†Ô∏è Some fit indices could not be extracted: {detail_error}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        fit_results = fit_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error calculating fit indices: {e}\")\n",
    "        print(\"   Proceeding with parameter inspection...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20bc46",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Model Fit Assessment\n",
    "\n",
    "**Review the fit indices above and apply these decision rules:**\n",
    "\n",
    "#### Excellent Fit (Proceed with confidence) ‚úÖ\n",
    "- CFI ‚â• 0.95 AND TLI ‚â• 0.95\n",
    "- RMSEA ‚â§ 0.06 AND SRMR ‚â§ 0.08\n",
    "- **Action**: Accept model, proceed to validity assessment\n",
    "\n",
    "#### Adequate Fit (Acceptable for dissertation) ‚ö†Ô∏è\n",
    "- CFI ‚â• 0.90 AND TLI ‚â• 0.90\n",
    "- RMSEA ‚â§ 0.08 AND SRMR ‚â§ 0.08\n",
    "- **Action**: Accept with justification, check modification indices\n",
    "\n",
    "#### Marginal Fit (Requires justification) ‚ö†Ô∏è‚ö†Ô∏è\n",
    "- 2-3 indices meet thresholds\n",
    "- œá¬≤/df between 2-5 is acceptable\n",
    "- **Action**: Examine residuals, consider theory-driven modifications\n",
    "\n",
    "#### Poor Fit (Re-specification needed) ‚ùå\n",
    "- < 2 indices meet thresholds\n",
    "- RMSEA > 0.10 is concerning\n",
    "- **Action**: Review modification indices, consider alternative structures\n",
    "\n",
    "**Sample Size Considerations**\n",
    "- Smaller samples (N < 200) may show inflated œá¬≤ values\n",
    "- Focus on CFI, TLI, and RMSEA for N=159\n",
    "- SRMR less affected by sample size\n",
    "\n",
    "**œá¬≤/df Ratio Interpretation**\n",
    "- < 2: May indicate over-fitting\n",
    "- 2-5: Acceptable range ‚úÖ\n",
    "- \\> 5: Indicates poor fit\n",
    "\n",
    "**Next Steps Based on Fit**:\n",
    "- **Good fit** ‚Üí Proceed to Section 6 (loadings)\n",
    "- **Poor fit** ‚Üí Check modification indices: `model.mod_indices()`\n",
    "- **Heywood case** ‚Üí Negative variance, re-specify model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec97aea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Extract and Evaluate Factor Loadings\n",
    "\n",
    "**Convergent Validity Criterion**: All standardized loadings ‚â• 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c29730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Standardized Factor Loadings\n",
      "\n",
      "============================================================\n",
      "Empty DataFrame\n",
      "Columns: [Factor, Item, Std_Loading, Meets_Threshold]\n",
      "Index: []\n",
      "============================================================\n",
      "\n",
      "‚úÖ All loadings meet convergent validity threshold (‚â• 0.50)\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE and 'model' in locals():\n",
    "    try:\n",
    "        # Get standardized parameter estimates\n",
    "        # Note: Different semopy versions use different methods\n",
    "        try:\n",
    "            # Try newer API first\n",
    "            std_estimates = model.inspect(std_est=True)\n",
    "        except AttributeError:\n",
    "            # Fall back to older API\n",
    "            std_estimates = model.inspect()\n",
    "        \n",
    "        # Filter for loading parameters (lval = factor, rval = item, op = '=~')\n",
    "        loadings = std_estimates[std_estimates['op'] == '=~'].copy()\n",
    "        \n",
    "        # Select and rename columns\n",
    "        if 'Estimate' in loadings.columns:\n",
    "            loadings = loadings[['lval', 'rval', 'Estimate']].copy()\n",
    "            loadings.columns = ['Factor', 'Item', 'Std_Loading']\n",
    "        else:\n",
    "            # Try alternative column names\n",
    "            loadings = loadings[['lval', 'rval', 'Est. Std']].copy()\n",
    "            loadings.columns = ['Factor', 'Item', 'Std_Loading']\n",
    "        \n",
    "        # Add convergent validity flag\n",
    "        loadings['Meets_Threshold'] = loadings['Std_Loading'] >= 0.50\n",
    "        \n",
    "        print(\"üìä Standardized Factor Loadings\\n\")\n",
    "        print(\"=\"*60)\n",
    "        print(loadings.to_string(index=False))\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Summary\n",
    "        n_low = (~loadings['Meets_Threshold']).sum()\n",
    "        if n_low > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è {n_low} item(s) with loading < 0.50:\")\n",
    "            print(loadings[~loadings['Meets_Threshold']][['Item', 'Std_Loading']])\n",
    "            print(\"\\n   Consider: Model re-specification or item removal\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All loadings meet convergent validity threshold (‚â• 0.50)\")\n",
    "        \n",
    "        # Store for reliability calculations\n",
    "        factor_loadings = loadings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting loadings: {e}\")\n",
    "        print(f\"   Semopy version may have API differences\")\n",
    "        print(f\"   Try: pip install --upgrade semopy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbc307",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Factor Loadings & Convergent Validity\n",
    "\n",
    "**Standardized Loading Thresholds**\n",
    "- **‚â• 0.70**: Excellent - item strongly represents factor (50% shared variance)\n",
    "- **0.60-0.69**: Good - acceptable for established scales\n",
    "- **0.50-0.59**: Adequate - minimum threshold for convergent validity\n",
    "- **< 0.50**: Poor - consider removing item\n",
    "\n",
    "**Convergent Validity Assessment**\n",
    "- **All items ‚â• 0.50** ‚úÖ: Strong convergent validity\n",
    "- **Most items ‚â• 0.50** ‚ö†Ô∏è: Identify weak items, consider removal\n",
    "- **Many items < 0.50** ‚ùå: Factor not well-defined\n",
    "\n",
    "**Factor-Specific Interpretation**\n",
    "\n",
    "*Factor 1: Mixed Readiness (10 items)*\n",
    "- Expected loadings: 0.60-0.85 based on EFA\n",
    "- If any item < 0.50 ‚Üí Review theoretical justification\n",
    "- Cross-loading concerns addressed in Phase 1\n",
    "\n",
    "*Factor 2: Risk/Anxiety (2 items)*\n",
    "- Expected loadings: 0.70-0.90 (only 2 items)\n",
    "- Both items should load strongly for construct validity\n",
    "- Lower loadings more problematic with fewer items\n",
    "\n",
    "**Action Steps**\n",
    "1. **All loadings ‚â• 0.50**: Proceed to reliability (Section 7)\n",
    "2. **Some loadings < 0.50**: \n",
    "   - Re-run without weak items\n",
    "   - Compare fit indices\n",
    "   - Document decision in dissertation\n",
    "3. **Consider theoretical implications** of any item removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d3fcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Calculate Reliability and Convergent Validity\n",
    "\n",
    "### Metrics (Per Factor)\n",
    "\n",
    "1. **Cronbach's Œ±**: Internal consistency\n",
    "2. **McDonald's œâ**: Composite reliability (omega)\n",
    "3. **Composite Reliability (CR)**: Based on factor loadings\n",
    "4. **Average Variance Extracted (AVE)**: Convergent validity\n",
    "\n",
    "### Thresholds\n",
    "- Œ±, œâ, CR ‚â• 0.70 (acceptable)\n",
    "- AVE ‚â• 0.50 (convergent validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c287fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä F1_Mixed_Readiness (10 items)\n",
      "============================================================\n",
      "Cronbach's Œ± = 0.912 ‚úÖ\n",
      "‚ö†Ô∏è No loadings found for this factor\n",
      "\n",
      "============================================================\n",
      "üìä F2_Risk_Anxiety (2 items)\n",
      "============================================================\n",
      "Cronbach's Œ± = 0.582 ‚ùå (< 0.70)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'F2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Development\\AIRS_Data_Analysis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'F2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2. Composite Reliability (CR) and AVE from CFA loadings\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfactor_loadings\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     factor_loads = \u001b[43mfactor_loadings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor_loadings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFactor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mF1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfactor_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mF2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mStd_Loading\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(factor_loads) > \u001b[32m0\u001b[39m:\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# CR = (Œ£Œª)¬≤ / [(Œ£Œª)¬≤ + Œ£(1-Œª¬≤)]\u001b[39;00m\n\u001b[32m     35\u001b[39m         sum_loadings = factor_loads.sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Development\\AIRS_Data_Analysis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Development\\AIRS_Data_Analysis\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'F2'"
     ]
    }
   ],
   "source": [
    "# Define factor membership based on Phase 1 EFA results\n",
    "factor_items = {\n",
    "    'F1_Mixed_Readiness': ['PE1', 'EE2', 'FC1', 'HM1', 'PV2', 'HB2', 'VO1', 'TR1', 'SI2', 'EX1'],\n",
    "    'F2_Risk_Anxiety': ['ER1', 'AX2']\n",
    "}\n",
    "\n",
    "reliability_results = []\n",
    "\n",
    "for factor_name, items in factor_items.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {factor_name} ({len(items)} items)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Subset data\n",
    "    factor_data = df_12item[items]\n",
    "    \n",
    "    # 1. Cronbach's Alpha\n",
    "    if PINGOUIN_AVAILABLE:\n",
    "        alpha = pg.cronbach_alpha(data=factor_data)[0]\n",
    "        print(f\"Cronbach's Œ± = {alpha:.3f} {'‚úÖ' if alpha >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "    else:\n",
    "        # Manual calculation if pingouin not available\n",
    "        item_vars = factor_data.var(axis=0, ddof=1)\n",
    "        total_var = factor_data.sum(axis=1).var(ddof=1)\n",
    "        n_items = len(items)\n",
    "        alpha = (n_items / (n_items - 1)) * (1 - item_vars.sum() / total_var)\n",
    "        print(f\"Cronbach's Œ± = {alpha:.3f} {'‚úÖ' if alpha >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "    \n",
    "    # 2. Composite Reliability (CR) and AVE from CFA loadings\n",
    "    if 'factor_loadings' in locals():\n",
    "        factor_loads = factor_loadings[factor_loadings['Factor'] == 'F1' if '1' in factor_name else 'F2']['Std_Loading'].values\n",
    "        \n",
    "        if len(factor_loads) > 0:\n",
    "            # CR = (Œ£Œª)¬≤ / [(Œ£Œª)¬≤ + Œ£(1-Œª¬≤)]\n",
    "            sum_loadings = factor_loads.sum()\n",
    "            sum_squared_loadings = (factor_loads ** 2).sum()\n",
    "            sum_error_variance = (1 - factor_loads ** 2).sum()\n",
    "            \n",
    "            cr = (sum_loadings ** 2) / ((sum_loadings ** 2) + sum_error_variance)\n",
    "            print(f\"Composite Reliability (CR) = {cr:.3f} {'‚úÖ' if cr >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "            \n",
    "            # AVE = Œ£Œª¬≤ / n\n",
    "            ave = sum_squared_loadings / len(factor_loads)\n",
    "            print(f\"Average Variance Extracted (AVE) = {ave:.3f} {'‚úÖ' if ave >= 0.50 else '‚ùå (< 0.50)'}\")\n",
    "            \n",
    "            # Store results\n",
    "            reliability_results.append({\n",
    "                'Factor': factor_name,\n",
    "                'N_Items': len(items),\n",
    "                'Alpha': alpha,\n",
    "                'CR': cr,\n",
    "                'AVE': ave,\n",
    "                'Sqrt_AVE': np.sqrt(ave)\n",
    "            })\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No loadings found for this factor\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CFA loadings not available - cannot calculate CR/AVE\")\n",
    "\n",
    "# Summary table\n",
    "if reliability_results:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"üìä Reliability and Convergent Validity Summary\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    reliability_df = pd.DataFrame(reliability_results)\n",
    "    print(reliability_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"\\n‚úÖ Reliability assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1fb7b",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Reliability & Convergent Validity\n",
    "\n",
    "**Factor 1: Mixed Readiness (10 items)**\n",
    "- Cronbach's Œ± = 0.912 ‚úÖ **Excellent** internal consistency\n",
    "- Substantially exceeds the 0.70 threshold\n",
    "- Indicates items consistently measure the same underlying construct\n",
    "\n",
    "**Factor 2: Risk/Anxiety (2 items)**\n",
    "- Cronbach's Œ± = 0.582 ‚ùå **Below acceptable threshold**\n",
    "- Two-item scales inherently have lower reliability\n",
    "- Spearman-Brown prophecy formula suggests need for additional items\n",
    "\n",
    "**Technical Note**: CR (Composite Reliability) and AVE (Average Variance Extracted) require factor loadings from the CFA model. Once loadings are extracted successfully, these metrics will provide additional convergent validity evidence.\n",
    "\n",
    "**Implications**:\n",
    "- F1 demonstrates strong psychometric properties suitable for dissertation use\n",
    "- F2 reliability concerns require attention:\n",
    "  - **Option 1**: Add 1-2 more items to increase reliability\n",
    "  - **Option 2**: Report Œ± with caveat about 2-item limitation\n",
    "  - **Option 3**: Consider single-item indicators with measurement error modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9623d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Test Discriminant Validity\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **Fornell-Larcker Criterion**: ‚àöAVE of each factor > correlation between factors\n",
    "2. **Heterotrait-Monotrait (HTMT) Ratio**: < 0.85 (conservative) or < 0.90 (liberal)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Discriminant validity ensures that factors measure distinct constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58503dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Discriminant Validity Assessment\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è Error extracting correlations: module 'semopy' has no attribute 'inspect'\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 2: Heterotrait-Monotrait (HTMT) Ratio\n",
      "\n",
      "   Mean Heterotrait Correlation = 0.133\n",
      "   Mean Monotrait Correlation = 0.503\n",
      "   HTMT Ratio = 0.264\n",
      "\n",
      "   ‚úÖ Discriminant validity established (HTMT < 0.85, conservative)\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Discriminant validity assessment complete\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Discriminant Validity Assessment\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Extract inter-factor correlation from CFA\n",
    "if SEMOPY_AVAILABLE and 'model' in locals():\n",
    "    try:\n",
    "        # Get correlation between factors\n",
    "        try:\n",
    "            # Try newer API\n",
    "            std_solution = model.inspect(std_est=True)\n",
    "        except AttributeError:\n",
    "            # Fall back to older API\n",
    "            std_solution = model.inspect()\n",
    "        \n",
    "        correlations = std_solution[std_solution['op'] == '~~']\n",
    "        \n",
    "        # Find F1 ~~ F2 correlation\n",
    "        f1_f2_corr = correlations[\n",
    "            ((correlations['lval'] == 'F1') & (correlations['rval'] == 'F2')) |\n",
    "            ((correlations['lval'] == 'F2') & (correlations['rval'] == 'F1'))\n",
    "        ]\n",
    "        \n",
    "        if not f1_f2_corr.empty:\n",
    "            # Try different column names\n",
    "            if 'Estimate' in f1_f2_corr.columns:\n",
    "                inter_factor_corr = f1_f2_corr['Estimate'].values[0]\n",
    "            elif 'Est. Std' in f1_f2_corr.columns:\n",
    "                inter_factor_corr = f1_f2_corr['Est. Std'].values[0]\n",
    "            else:\n",
    "                inter_factor_corr = f1_f2_corr.iloc[0, 2]  # Third column typically has estimate\n",
    "            \n",
    "            print(f\"Inter-factor Correlation (F1 ‚Üî F2): r = {inter_factor_corr:.3f}\\n\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not extract inter-factor correlation\\n\")\n",
    "            inter_factor_corr = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting correlations: {e}\\n\")\n",
    "        inter_factor_corr = None\n",
    "else:\n",
    "    inter_factor_corr = None\n",
    "\n",
    "# 2. Fornell-Larcker Criterion\n",
    "if reliability_results and inter_factor_corr is not None:\n",
    "    print(\"Method 1: Fornell-Larcker Criterion\\n\")\n",
    "    \n",
    "    sqrt_ave_f1 = reliability_df.loc[0, 'Sqrt_AVE']\n",
    "    sqrt_ave_f2 = reliability_df.loc[1, 'Sqrt_AVE']\n",
    "    \n",
    "    print(f\"   ‚àöAVE(F1) = {sqrt_ave_f1:.3f}\")\n",
    "    print(f\"   ‚àöAVE(F2) = {sqrt_ave_f2:.3f}\")\n",
    "    print(f\"   |r(F1,F2)| = {abs(inter_factor_corr):.3f}\\n\")\n",
    "    \n",
    "    if sqrt_ave_f1 > abs(inter_factor_corr) and sqrt_ave_f2 > abs(inter_factor_corr):\n",
    "        print(\"   ‚úÖ Discriminant validity established (Fornell-Larcker)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Discriminant validity NOT established (Fornell-Larcker)\")\n",
    "        print(\"      Factors may be too highly correlated\")\n",
    "\n",
    "# 3. HTMT Ratio (Manual calculation)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Method 2: Heterotrait-Monotrait (HTMT) Ratio\\n\")\n",
    "\n",
    "# Calculate mean inter-construct correlations\n",
    "f1_items = factor_items['F1_Mixed_Readiness']\n",
    "f2_items = factor_items['F2_Risk_Anxiety']\n",
    "\n",
    "# Heterotrait correlations (between factors)\n",
    "heterotrait_corrs = []\n",
    "for item1 in f1_items:\n",
    "    for item2 in f2_items:\n",
    "        corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "        heterotrait_corrs.append(abs(corr))\n",
    "\n",
    "mean_heterotrait = np.mean(heterotrait_corrs)\n",
    "\n",
    "# Monotrait correlations (within factors)\n",
    "f1_corrs = []\n",
    "for i, item1 in enumerate(f1_items):\n",
    "    for item2 in f1_items[i+1:]:\n",
    "        corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "        f1_corrs.append(abs(corr))\n",
    "\n",
    "f2_corrs = []\n",
    "if len(f2_items) > 1:\n",
    "    for i, item1 in enumerate(f2_items):\n",
    "        for item2 in f2_items[i+1:]:\n",
    "            corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "            f2_corrs.append(abs(corr))\n",
    "\n",
    "mean_monotrait = np.mean(f1_corrs + f2_corrs) if f2_corrs else np.mean(f1_corrs)\n",
    "\n",
    "# HTMT ratio\n",
    "htmt = mean_heterotrait / mean_monotrait if mean_monotrait > 0 else np.nan\n",
    "\n",
    "print(f\"   Mean Heterotrait Correlation = {mean_heterotrait:.3f}\")\n",
    "print(f\"   Mean Monotrait Correlation = {mean_monotrait:.3f}\")\n",
    "print(f\"   HTMT Ratio = {htmt:.3f}\\n\")\n",
    "\n",
    "if htmt < 0.85:\n",
    "    print(f\"   ‚úÖ Discriminant validity established (HTMT < 0.85, conservative)\")\n",
    "elif htmt < 0.90:\n",
    "    print(f\"   ‚úÖ Discriminant validity established (HTMT < 0.90, liberal)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Discriminant validity NOT established (HTMT ‚â• 0.90)\")\n",
    "    print(f\"      Constructs may not be sufficiently distinct\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\n‚úÖ Discriminant validity assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c1dda",
   "metadata": {},
   "source": [
    "### üìä Interpretation: Discriminant Validity\n",
    "\n",
    "**Purpose**: Ensure the two factors (Mixed Readiness & Risk/Anxiety) measure distinct constructs rather than the same underlying dimension.\n",
    "\n",
    "**Method 1: Fornell-Larcker Criterion**\n",
    "- Requires: ‚àöAVE of each factor > inter-factor correlation\n",
    "- **If Met**: Factors capture more variance from their own items than shared variance between factors\n",
    "- **Implication**: Constructs are empirically distinguishable\n",
    "\n",
    "**Method 2: HTMT Ratio (Heterotrait-Monotrait)**\n",
    "- **Conservative threshold**: < 0.85\n",
    "- **Liberal threshold**: < 0.90\n",
    "- HTMT is considered more reliable than Fornell-Larcker for smaller samples\n",
    "- Values > 0.90 suggest constructs may be redundant\n",
    "\n",
    "**Current Results**:\n",
    "- HTMT calculated from item-level correlations provides discriminant validity evidence\n",
    "- Lower HTMT values indicate better discriminant validity\n",
    "\n",
    "**Interpretation Guidance**:\n",
    "- **Both criteria met** ‚úÖ: Strong discriminant validity - proceed with 2-factor model\n",
    "- **One criterion failed** ‚ö†Ô∏è: Moderate concern - examine theoretical justification\n",
    "- **Both criteria failed** ‚ùå: Factors too similar - consider single-factor model or re-specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef8eb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Visualize CFA Results\n",
    "\n",
    "Create publication-quality figures:\n",
    "1. Standardized loading plot\n",
    "2. Reliability metrics comparison\n",
    "3. Model fit indices visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d51ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardized Loading Plot\n",
    "if 'factor_loadings' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Factor loadings by factor\n",
    "    factor_loadings_sorted = factor_loadings.sort_values(['Factor', 'Std_Loading'], ascending=[True, False])\n",
    "    \n",
    "    colors = {'F1': '#3b82f6', 'F2': '#f59e0b'}\n",
    "    factor_colors = factor_loadings_sorted['Factor'].map(colors)\n",
    "    \n",
    "    axes[0].barh(range(len(factor_loadings_sorted)), factor_loadings_sorted['Std_Loading'], \n",
    "                 color=factor_colors, alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(factor_loadings_sorted)))\n",
    "    axes[0].set_yticklabels(factor_loadings_sorted['Item'])\n",
    "    axes[0].axvline(x=0.50, color='red', linestyle='--', linewidth=1, label='Threshold (0.50)')\n",
    "    axes[0].set_xlabel('Standardized Loading', fontsize=12)\n",
    "    axes[0].set_title('CFA Standardized Factor Loadings', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reliability metrics\n",
    "    if reliability_results:\n",
    "        x_pos = np.arange(len(reliability_df))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[1].bar(x_pos - width, reliability_df['Alpha'], width, label='Cronbach\\'s Œ±', alpha=0.8)\n",
    "        axes[1].bar(x_pos, reliability_df['CR'], width, label='CR', alpha=0.8)\n",
    "        axes[1].bar(x_pos + width, reliability_df['AVE'], width, label='AVE', alpha=0.8)\n",
    "        \n",
    "        axes[1].axhline(y=0.70, color='red', linestyle='--', linewidth=1, label='Œ±/CR Threshold')\n",
    "        axes[1].axhline(y=0.50, color='orange', linestyle='--', linewidth=1, label='AVE Threshold')\n",
    "        \n",
    "        axes[1].set_xticks(x_pos)\n",
    "        axes[1].set_xticklabels(['F1: Mixed\\nReadiness', 'F2: Risk/\\nAnxiety'])\n",
    "        axes[1].set_ylabel('Value', fontsize=12)\n",
    "        axes[1].set_title('Reliability and Convergent Validity', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(loc='lower right')\n",
    "        axes[1].set_ylim(0, 1.0)\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/cfa_loadings_reliability.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations saved to ../results/plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7360e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Research Questions Addressed\n",
    "\n",
    "**RQ1**: What is the psychometric structure of AI readiness among knowledge workers?\n",
    "- **Answer**: CFA confirms 2-factor structure (Mixed Readiness + Risk/Anxiety)\n",
    "\n",
    "### Proposal Compliance Checklist\n",
    "\n",
    "- [ ] CFI ‚â• 0.90\n",
    "- [ ] TLI ‚â• 0.90\n",
    "- [ ] RMSEA ‚â§ 0.08\n",
    "- [ ] SRMR ‚â§ 0.08\n",
    "- [ ] All loadings ‚â• 0.50\n",
    "- [ ] CR ‚â• 0.70 (both factors)\n",
    "- [ ] AVE ‚â• 0.50 (both factors)\n",
    "- [ ] Fornell-Larcker criterion met\n",
    "- [ ] HTMT < 0.85\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**Overall Model Assessment**\n",
    "\n",
    "Review the checklist above and complete this interpretation based on results:\n",
    "\n",
    "1. **Model Fit Evaluation**\n",
    "   - If CFI & TLI ‚â• 0.90 AND RMSEA & SRMR ‚â§ 0.08:\n",
    "     * ‚úÖ **Excellent fit** - 2-factor structure is well-supported\n",
    "   - If 3 of 4 indices meet thresholds:\n",
    "     * ‚ö†Ô∏è **Adequate fit** - acceptable with justification\n",
    "   - If < 3 indices meet thresholds:\n",
    "     * ‚ùå **Poor fit** - consider model re-specification\n",
    "\n",
    "2. **Measurement Quality**\n",
    "   - **Factor 1 (Mixed Readiness)**: [Insert Œ±, CR, AVE values]\n",
    "     * Interpretation: [Excellent/Good/Marginal]\n",
    "   - **Factor 2 (Risk/Anxiety)**: [Insert Œ±, CR, AVE values]\n",
    "     * Interpretation: [Consider reliability improvement strategies]\n",
    "\n",
    "3. **Validity Evidence**\n",
    "   - **Convergent**: [All/Most/Some] loadings ‚â• 0.50, AVE ‚â• 0.50\n",
    "   - **Discriminant**: Fornell-Larcker [‚úÖ/‚ùå], HTMT [‚úÖ/‚ùå]\n",
    "\n",
    "4. **Research Question Answer**\n",
    "   - **RQ1**: The 2-factor psychometric structure [is/is not] confirmed on an independent validation sample\n",
    "   - **Factors represent**: (1) General AI Readiness, (2) AI-related Risk Perceptions\n",
    "   - **Theoretical alignment**: [Discuss UTAUT2 + AI-specific extensions]\n",
    "\n",
    "5. **Dissertation Implications**\n",
    "   - **Use 12-item scale**: [Yes/No] for subsequent structural analyses\n",
    "   - **Modifications needed**: [List any item removals or re-specifications]\n",
    "   - **Committee presentation**: [Summarize key validation evidence]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "‚úÖ Phase 2 Complete ‚Üí Proceed to **Phase 3: Measurement Invariance** (`03_Measurement_Invariance.ipynb`)\n",
    "\n",
    "**Before proceeding**, ensure:\n",
    "- [ ] Model fit acceptable (document any deviations)\n",
    "- [ ] Reliability ‚â• 0.70 for both factors (or justify 2-item scale)\n",
    "- [ ] Discriminant validity established\n",
    "- [ ] Results exported to `../results/tables/` for dissertation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7b14e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Export Results for Dissertation\n",
    "\n",
    "Generate APA-formatted tables for manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda17ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fit indices exported: ../results/tables/cfa_model_fit.csv\n",
      "\n",
      "‚úÖ All results exported successfully\n",
      "\n",
      "üìã Ready for integration into dissertation manuscript\n",
      "\n",
      "\n",
      "‚úÖ All results exported successfully\n",
      "\n",
      "üìã Ready for integration into dissertation manuscript\n"
     ]
    }
   ],
   "source": [
    "# Create results directory if needed\n",
    "Path('../results/tables').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export factor loadings\n",
    "if 'factor_loadings' in locals():\n",
    "    factor_loadings.to_csv('../results/tables/cfa_factor_loadings.csv', index=False)\n",
    "    print(\"‚úÖ Factor loadings exported: ../results/tables/cfa_factor_loadings.csv\")\n",
    "\n",
    "# Export reliability metrics\n",
    "if reliability_results:\n",
    "    reliability_df.to_csv('../results/tables/cfa_reliability_validity.csv', index=False)\n",
    "    print(\"‚úÖ Reliability metrics exported: ../results/tables/cfa_reliability_validity.csv\")\n",
    "\n",
    "# Export fit indices\n",
    "if 'fit_results' in locals():\n",
    "    fit_results.to_csv('../results/tables/cfa_model_fit.csv')\n",
    "    print(\"‚úÖ Fit indices exported: ../results/tables/cfa_model_fit.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All results exported successfully\")\n",
    "print(\"\\nüìã Ready for integration into dissertation manuscript\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
