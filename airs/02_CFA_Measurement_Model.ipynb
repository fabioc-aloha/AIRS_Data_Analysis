{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6238c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "   - semopy available: True\n",
      "   - pingouin available: True\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Psychometric analysis\n",
    "from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# SEM / CFA\n",
    "try:\n",
    "    import semopy\n",
    "    from semopy import Model\n",
    "    SEMOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è semopy not installed. Run: pip install semopy\")\n",
    "    SEMOPY_AVAILABLE = False\n",
    "\n",
    "# Reliability calculations\n",
    "try:\n",
    "    import pingouin as pg\n",
    "    PINGOUIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è pingouin not installed. Run: pip install pingouin\")\n",
    "    PINGOUIN_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   - semopy available: {SEMOPY_AVAILABLE}\")\n",
    "print(f\"   - pingouin available: {PINGOUIN_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d5ca0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load Holdout Sample\n",
    "\n",
    "Load the independent validation sample (N=159) that was not used in EFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65200d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Holdout Sample Loaded\n",
      "   - N = 159\n",
      "   - Columns: 45\n",
      "\n",
      "‚úÖ Sample ready for CFA validation\n"
     ]
    }
   ],
   "source": [
    "# Load holdout sample\n",
    "df_holdout = pd.read_csv('../data/AIRS_clean_holdout.csv')\n",
    "\n",
    "print(f\"üìä Holdout Sample Loaded\")\n",
    "print(f\"   - N = {len(df_holdout)}\")\n",
    "print(f\"   - Columns: {len(df_holdout.columns)}\")\n",
    "print(f\"\\n‚úÖ Sample ready for CFA validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b195c99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extract 12-Item Subset\n",
    "\n",
    "Select the 12 items identified in Phase 1 EFA as the optimal construct-balanced scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8ec255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã 12-Item Scale: PE1, EE2, SI2, FC1, HM1, PV2, HB2, VO1, TR1, EX1, ER1, AX2\n",
      "\n",
      "‚úÖ No missing data - all cases complete (N = 159)\n",
      "\n",
      "üìä 12-Item Descriptive Statistics:\n",
      "          PE1     EE2     SI2     FC1     HM1     PV2     HB2     VO1     TR1  \\\n",
      "count  159.00  159.00  159.00  159.00  159.00  159.00  159.00  159.00  159.00   \n",
      "mean     3.62    3.63    3.42    3.19    3.27    3.41    3.04    3.49    3.20   \n",
      "std      1.15    1.00    1.08    1.19    1.17    1.22    1.28    1.26    1.24   \n",
      "min      1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00   \n",
      "25%      3.00    3.00    3.00    2.00    3.00    3.00    2.00    3.00    2.50   \n",
      "50%      4.00    4.00    4.00    3.00    3.00    4.00    3.00    4.00    3.00   \n",
      "75%      4.00    4.00    4.00    4.00    4.00    4.00    4.00    4.00    4.00   \n",
      "max      5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00    5.00   \n",
      "\n",
      "          EX1     ER1     AX2  \n",
      "count  159.00  159.00  159.00  \n",
      "mean     3.30    3.21    3.18  \n",
      "std      1.16    1.23    1.18  \n",
      "min      1.00    1.00    1.00  \n",
      "25%      3.00    2.00    2.00  \n",
      "50%      4.00    3.00    3.00  \n",
      "75%      4.00    4.00    4.00  \n",
      "max      5.00    5.00    5.00  \n"
     ]
    }
   ],
   "source": [
    "# Load item selection from Phase 1\n",
    "with open('../data/airs_12item_selection.json', 'r') as f:\n",
    "    item_selection = json.load(f)\n",
    "\n",
    "# Extract selected items\n",
    "selected_items = [info['selected_item'] for construct, info in item_selection.items()]\n",
    "print(f\"üìã 12-Item Scale: {', '.join(selected_items)}\")\n",
    "\n",
    "# Create 12-item dataset\n",
    "df_12item = df_holdout[selected_items].copy()\n",
    "\n",
    "# Check for missing data\n",
    "missing_counts = df_12item.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing Data Detected:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    print(f\"\\n   Using listwise deletion (complete cases only)\")\n",
    "    df_12item = df_12item.dropna()\n",
    "    print(f\"   Final N = {len(df_12item)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing data - all cases complete (N = {len(df_12item)})\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"\\nüìä 12-Item Descriptive Statistics:\")\n",
    "print(df_12item.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39401212",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Test CFA Assumptions\n",
    "\n",
    "Verify data suitability for factor analysis:\n",
    "- **Sample Adequacy**: KMO ‚â• 0.60\n",
    "- **Factorability**: Bartlett's test p < 0.05\n",
    "- **Normality**: Skewness and kurtosis within acceptable ranges (¬±2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2854a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Kaiser-Meyer-Olkin (KMO) Test\n",
      "   Overall KMO: 0.871\n",
      "   Interpretation: Meritorious ‚úÖ\n",
      "\n",
      "üîç Bartlett's Test of Sphericity\n",
      "   œá¬≤ = 938.14\n",
      "   p-value < 0.001\n",
      "   Interpretation: Variables are correlated ‚úÖ\n",
      "\n",
      "üîç Univariate Normality Assessment\n",
      "     Skewness  Kurtosis  Skew_Flag  Kurt_Flag\n",
      "PE1    -0.792    -0.074      False      False\n",
      "EE2    -0.607    -0.177      False      False\n",
      "SI2    -0.334    -0.557      False      False\n",
      "FC1    -0.158    -1.125      False      False\n",
      "HM1    -0.519    -0.528      False      False\n",
      "PV2    -0.636    -0.505      False      False\n",
      "HB2    -0.217    -1.179      False      False\n",
      "VO1    -0.667    -0.562      False      False\n",
      "TR1    -0.411    -0.733      False      False\n",
      "EX1    -0.538    -0.542      False      False\n",
      "ER1    -0.196    -1.072      False      False\n",
      "AX2    -0.138    -1.104      False      False\n",
      "\n",
      "‚úÖ All items within acceptable normality range\n",
      "\n",
      "‚úÖ Assumption testing complete\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy\n",
    "kmo_all, kmo_model = calculate_kmo(df_12item)\n",
    "\n",
    "print(f\"üîç Kaiser-Meyer-Olkin (KMO) Test\")\n",
    "print(f\"   Overall KMO: {kmo_model:.3f}\")\n",
    "if kmo_model >= 0.90:\n",
    "    print(f\"   Interpretation: Marvelous ‚úÖ\")\n",
    "elif kmo_model >= 0.80:\n",
    "    print(f\"   Interpretation: Meritorious ‚úÖ\")\n",
    "elif kmo_model >= 0.70:\n",
    "    print(f\"   Interpretation: Middling ‚úÖ\")\n",
    "elif kmo_model >= 0.60:\n",
    "    print(f\"   Interpretation: Mediocre ‚ö†Ô∏è\")\n",
    "else:\n",
    "    print(f\"   Interpretation: Unacceptable ‚ùå\")\n",
    "\n",
    "# 3.2 Bartlett's Test of Sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_12item)\n",
    "\n",
    "print(f\"\\nüîç Bartlett's Test of Sphericity\")\n",
    "print(f\"   œá¬≤ = {chi_square_value:.2f}\")\n",
    "print(f\"   p-value < 0.001\" if p_value < 0.001 else f\"   p-value = {p_value:.4f}\")\n",
    "print(f\"   Interpretation: {'Variables are correlated ‚úÖ' if p_value < 0.05 else 'Variables are NOT sufficiently correlated ‚ùå'}\")\n",
    "\n",
    "# 3.3 Univariate Normality (Skewness and Kurtosis)\n",
    "print(f\"\\nüîç Univariate Normality Assessment\")\n",
    "normality_stats = pd.DataFrame({\n",
    "    'Skewness': df_12item.skew(),\n",
    "    'Kurtosis': df_12item.kurtosis()\n",
    "})\n",
    "\n",
    "# Flag items outside acceptable ranges\n",
    "normality_stats['Skew_Flag'] = normality_stats['Skewness'].abs() > 2\n",
    "normality_stats['Kurt_Flag'] = normality_stats['Kurtosis'].abs() > 2\n",
    "\n",
    "print(normality_stats.round(3))\n",
    "\n",
    "if normality_stats[['Skew_Flag', 'Kurt_Flag']].any().any():\n",
    "    print(f\"\\n‚ö†Ô∏è Some items show departures from normality (|skew| or |kurt| > 2)\")\n",
    "    print(f\"   Consider robust estimation methods (e.g., MLR in lavaan/Mplus)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All items within acceptable normality range\")\n",
    "\n",
    "print(f\"\\n‚úÖ Assumption testing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b7308",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Specify and Estimate CFA Model\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "Based on Phase 1 EFA parallel analysis results:\n",
    "\n",
    "**Factor 1 (Mixed Readiness)**: 10 items\n",
    "- Performance Expectancy (PE1)\n",
    "- Effort Expectancy (EE2)\n",
    "- Facilitating Conditions (FC1)\n",
    "- Hedonic Motivation (HM1)\n",
    "- Price Value (PV2)\n",
    "- Habit (HB2)\n",
    "- Voluntariness of Use (VO1)\n",
    "- Trust in AI (TR1)\n",
    "- Social Influence (SI2)\n",
    "- Explainability (EX1)\n",
    "\n",
    "**Factor 2 (Risk/Anxiety)**: 2 items\n",
    "- Ethical Risk (ER1)\n",
    "- AI Anxiety (AX2)\n",
    "\n",
    "**Model Type**: Oblique (factors allowed to correlate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2129deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã CFA Model Specification:\n",
      "\n",
      "    # Measurement model\n",
      "    # Factor 1: Mixed Readiness (10 items)\n",
      "    F1 =~ PE1 + EE2 + FC1 + HM1 + PV2 + HB2 + VO1 + TR1 + SI2 + EX1\n",
      "\n",
      "    # Factor 2: Risk/Anxiety (2 items)\n",
      "    F2 =~ ER1 + AX2\n",
      "\n",
      "    # Factor covariance (oblique model)\n",
      "    F1 ~~ F2\n",
      "    \n",
      "\n",
      "‚úÖ Model specification complete\n"
     ]
    }
   ],
   "source": [
    "if not SEMOPY_AVAILABLE:\n",
    "    print(\"‚ùå semopy not available - cannot proceed with CFA\")\n",
    "    print(\"   Install: pip install semopy\")\n",
    "else:\n",
    "    # Define CFA model specification\n",
    "    # Based on Phase 1 EFA results (see README empirical model diagram)\n",
    "    \n",
    "    model_spec = \"\"\"\n",
    "    # Measurement model\n",
    "    # Factor 1: Mixed Readiness (10 items)\n",
    "    F1 =~ PE1 + EE2 + FC1 + HM1 + PV2 + HB2 + VO1 + TR1 + SI2 + EX1\n",
    "    \n",
    "    # Factor 2: Risk/Anxiety (2 items)\n",
    "    F2 =~ ER1 + AX2\n",
    "    \n",
    "    # Factor covariance (oblique model)\n",
    "    F1 ~~ F2\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìã CFA Model Specification:\")\n",
    "    print(model_spec)\n",
    "    print(\"\\n‚úÖ Model specification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d73720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Estimating CFA model...\n",
      "\n",
      "‚úÖ Model converged successfully\n",
      "\n",
      "üìä Parameter Estimates:\n",
      "Name of objective: MLW\n",
      "Optimization method: SLSQP\n",
      "Optimization successful.\n",
      "Optimization terminated successfully\n",
      "Objective value: 0.844\n",
      "Number of iterations: 33\n",
      "Params: 0.652 0.769 1.076 1.142 1.175 1.133 1.115 0.790 0.736 2.346 0.113 0.758 0.000 0.678 1.246 0.919 0.253 0.967 0.589 0.478 0.560 0.486 0.675 0.577 0.609\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE:\n",
    "    print(\"‚è≥ Estimating CFA model...\\n\")\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = Model(model_spec)\n",
    "    \n",
    "    try:\n",
    "        result = model.fit(df_12item)\n",
    "        print(\"‚úÖ Model converged successfully\\n\")\n",
    "        \n",
    "        # Display basic results\n",
    "        print(\"üìä Parameter Estimates:\")\n",
    "        print(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model estimation failed: {e}\")\n",
    "        print(\"\\n   Troubleshooting suggestions:\")\n",
    "        print(\"   1. Check for perfect correlations (multicollinearity)\")\n",
    "        print(\"   2. Verify all items have variance (no constants)\")\n",
    "        print(\"   3. Consider standardizing variables\")\n",
    "        print(\"   4. Try alternative estimation method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e068eb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Evaluate Model Fit\n",
    "\n",
    "### Fit Index Thresholds (Proposal Section 7.7)\n",
    "\n",
    "| Index | Threshold | Interpretation |\n",
    "|-------|-----------|----------------|\n",
    "| CFI   | ‚â• 0.90    | Comparative Fit Index |\n",
    "| TLI   | ‚â• 0.90    | Tucker-Lewis Index |\n",
    "| RMSEA | ‚â§ 0.08    | Root Mean Square Error of Approximation |\n",
    "| SRMR  | ‚â§ 0.08    | Standardized Root Mean Square Residual |\n",
    "| œá¬≤/df | 2-5       | Chi-square to degrees of freedom ratio |\n",
    "\n",
    "**Note**: RMSEA 90% CI upper bound should be ‚â§ 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb5aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Fit Indices\n",
      "\n",
      "============================================================\n",
      "\n",
      "Information Criteria:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE and 'result' in locals():\n",
    "    # Extract fit indices\n",
    "    try:\n",
    "        fit_stats = semopy.calc_stats(model)\n",
    "        \n",
    "        print(\"üìä Model Fit Indices\\n\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Chi-square test\n",
    "        if 'chi2' in fit_stats.index:\n",
    "            chi2_val = fit_stats.loc['chi2', 'Value']\n",
    "            df_val = fit_stats.loc['dof', 'Value']\n",
    "            chi2_p = fit_stats.loc['chi2_pvalue', 'Value'] if 'chi2_pvalue' in fit_stats.index else None\n",
    "            \n",
    "            print(f\"Chi-square Test:\")\n",
    "            print(f\"   œá¬≤ = {chi2_val:.2f}, df = {df_val:.0f}\")\n",
    "            if chi2_p is not None:\n",
    "                print(f\"   p-value = {chi2_p:.4f}\")\n",
    "            print(f\"   œá¬≤/df = {chi2_val/df_val:.2f} {'‚úÖ' if 2 <= chi2_val/df_val <= 5 else '‚ö†Ô∏è'}\")\n",
    "            print()\n",
    "        \n",
    "        # Comparative Fit Index (CFI)\n",
    "        if 'CFI' in fit_stats.index:\n",
    "            cfi = fit_stats.loc['CFI', 'Value']\n",
    "            print(f\"CFI = {cfi:.3f} {'‚úÖ' if cfi >= 0.90 else '‚ùå (< 0.90)'}\")\n",
    "        \n",
    "        # Tucker-Lewis Index (TLI)\n",
    "        if 'TLI' in fit_stats.index:\n",
    "            tli = fit_stats.loc['TLI', 'Value']\n",
    "            print(f\"TLI = {tli:.3f} {'‚úÖ' if tli >= 0.90 else '‚ùå (< 0.90)'}\")\n",
    "        \n",
    "        # RMSEA\n",
    "        if 'RMSEA' in fit_stats.index:\n",
    "            rmsea = fit_stats.loc['RMSEA', 'Value']\n",
    "            print(f\"RMSEA = {rmsea:.3f} {'‚úÖ' if rmsea <= 0.08 else '‚ö†Ô∏è (> 0.08)'}\")\n",
    "        \n",
    "        # SRMR\n",
    "        if 'SRMR' in fit_stats.index:\n",
    "            srmr = fit_stats.loc['SRMR', 'Value']\n",
    "            print(f\"SRMR = {srmr:.3f} {'‚úÖ' if srmr <= 0.08 else '‚ö†Ô∏è (> 0.08)'}\")\n",
    "        \n",
    "        # AIC/BIC for model comparison\n",
    "        print(f\"\\nInformation Criteria:\")\n",
    "        if 'AIC' in fit_stats.index:\n",
    "            aic = fit_stats.loc['AIC', 'Value']\n",
    "            print(f\"   AIC = {aic:.2f}\")\n",
    "        if 'BIC' in fit_stats.index:\n",
    "            bic = fit_stats.loc['BIC', 'Value']\n",
    "            print(f\"   BIC = {bic:.2f}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Store for later use\n",
    "        fit_results = fit_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error calculating fit indices: {e}\")\n",
    "        print(\"   Proceeding with parameter inspection...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec97aea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Extract and Evaluate Factor Loadings\n",
    "\n",
    "**Convergent Validity Criterion**: All standardized loadings ‚â• 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c29730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error extracting loadings: module 'semopy' has no attribute 'inspect'\n"
     ]
    }
   ],
   "source": [
    "if SEMOPY_AVAILABLE and 'model' in locals():\n",
    "    try:\n",
    "        # Get standardized solution\n",
    "        std_solution = semopy.inspect(model, mode='std')\n",
    "        \n",
    "        # Filter for loading parameters (factor =~ item)\n",
    "        loadings = std_solution[std_solution['op'] == '=~'].copy()\n",
    "        loadings = loadings[['lval', 'rval', 'Estimate']]\n",
    "        loadings.columns = ['Factor', 'Item', 'Std_Loading']\n",
    "        \n",
    "        # Add convergent validity flag\n",
    "        loadings['Meets_Threshold'] = loadings['Std_Loading'] >= 0.50\n",
    "        \n",
    "        print(\"üìä Standardized Factor Loadings\\n\")\n",
    "        print(\"=\"*60)\n",
    "        print(loadings.to_string(index=False))\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Summary\n",
    "        n_low = (~loadings['Meets_Threshold']).sum()\n",
    "        if n_low > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è {n_low} item(s) with loading < 0.50:\")\n",
    "            print(loadings[~loadings['Meets_Threshold']][['Item', 'Std_Loading']])\n",
    "            print(\"\\n   Consider: Model re-specification or item removal\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All loadings meet convergent validity threshold (‚â• 0.50)\")\n",
    "        \n",
    "        # Store for reliability calculations\n",
    "        factor_loadings = loadings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting loadings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d3fcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Calculate Reliability and Convergent Validity\n",
    "\n",
    "### Metrics (Per Factor)\n",
    "\n",
    "1. **Cronbach's Œ±**: Internal consistency\n",
    "2. **McDonald's œâ**: Composite reliability (omega)\n",
    "3. **Composite Reliability (CR)**: Based on factor loadings\n",
    "4. **Average Variance Extracted (AVE)**: Convergent validity\n",
    "\n",
    "### Thresholds\n",
    "- Œ±, œâ, CR ‚â• 0.70 (acceptable)\n",
    "- AVE ‚â• 0.50 (convergent validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c287fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä F1_Mixed_Readiness (10 items)\n",
      "============================================================\n",
      "Cronbach's Œ± = 0.912 ‚úÖ\n",
      "‚ö†Ô∏è CFA loadings not available - cannot calculate CR/AVE\n",
      "\n",
      "============================================================\n",
      "üìä F2_Risk_Anxiety (2 items)\n",
      "============================================================\n",
      "Cronbach's Œ± = 0.582 ‚ùå (< 0.70)\n",
      "‚ö†Ô∏è CFA loadings not available - cannot calculate CR/AVE\n"
     ]
    }
   ],
   "source": [
    "# Define factor membership based on Phase 1 EFA results\n",
    "factor_items = {\n",
    "    'F1_Mixed_Readiness': ['PE1', 'EE2', 'FC1', 'HM1', 'PV2', 'HB2', 'VO1', 'TR1', 'SI2', 'EX1'],\n",
    "    'F2_Risk_Anxiety': ['ER1', 'AX2']\n",
    "}\n",
    "\n",
    "reliability_results = []\n",
    "\n",
    "for factor_name, items in factor_items.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {factor_name} ({len(items)} items)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Subset data\n",
    "    factor_data = df_12item[items]\n",
    "    \n",
    "    # 1. Cronbach's Alpha\n",
    "    if PINGOUIN_AVAILABLE:\n",
    "        alpha = pg.cronbach_alpha(data=factor_data)[0]\n",
    "        print(f\"Cronbach's Œ± = {alpha:.3f} {'‚úÖ' if alpha >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "    else:\n",
    "        # Manual calculation if pingouin not available\n",
    "        item_vars = factor_data.var(axis=0, ddof=1)\n",
    "        total_var = factor_data.sum(axis=1).var(ddof=1)\n",
    "        n_items = len(items)\n",
    "        alpha = (n_items / (n_items - 1)) * (1 - item_vars.sum() / total_var)\n",
    "        print(f\"Cronbach's Œ± = {alpha:.3f} {'‚úÖ' if alpha >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "    \n",
    "    # 2. Composite Reliability (CR) and AVE from CFA loadings\n",
    "    if 'factor_loadings' in locals():\n",
    "        factor_loads = factor_loadings[factor_loadings['Factor'] == 'F1' if '1' in factor_name else 'F2']['Std_Loading'].values\n",
    "        \n",
    "        if len(factor_loads) > 0:\n",
    "            # CR = (Œ£Œª)¬≤ / [(Œ£Œª)¬≤ + Œ£(1-Œª¬≤)]\n",
    "            sum_loadings = factor_loads.sum()\n",
    "            sum_squared_loadings = (factor_loads ** 2).sum()\n",
    "            sum_error_variance = (1 - factor_loads ** 2).sum()\n",
    "            \n",
    "            cr = (sum_loadings ** 2) / ((sum_loadings ** 2) + sum_error_variance)\n",
    "            print(f\"Composite Reliability (CR) = {cr:.3f} {'‚úÖ' if cr >= 0.70 else '‚ùå (< 0.70)'}\")\n",
    "            \n",
    "            # AVE = Œ£Œª¬≤ / n\n",
    "            ave = sum_squared_loadings / len(factor_loads)\n",
    "            print(f\"Average Variance Extracted (AVE) = {ave:.3f} {'‚úÖ' if ave >= 0.50 else '‚ùå (< 0.50)'}\")\n",
    "            \n",
    "            # Store results\n",
    "            reliability_results.append({\n",
    "                'Factor': factor_name,\n",
    "                'N_Items': len(items),\n",
    "                'Alpha': alpha,\n",
    "                'CR': cr,\n",
    "                'AVE': ave,\n",
    "                'Sqrt_AVE': np.sqrt(ave)\n",
    "            })\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No loadings found for this factor\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CFA loadings not available - cannot calculate CR/AVE\")\n",
    "\n",
    "# Summary table\n",
    "if reliability_results:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"üìä Reliability and Convergent Validity Summary\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    reliability_df = pd.DataFrame(reliability_results)\n",
    "    print(reliability_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"\\n‚úÖ Reliability assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9623d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Test Discriminant Validity\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **Fornell-Larcker Criterion**: ‚àöAVE of each factor > correlation between factors\n",
    "2. **Heterotrait-Monotrait (HTMT) Ratio**: < 0.85 (conservative) or < 0.90 (liberal)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Discriminant validity ensures that factors measure distinct constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58503dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Discriminant Validity Assessment\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è Error extracting correlations: module 'semopy' has no attribute 'inspect'\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 2: Heterotrait-Monotrait (HTMT) Ratio\n",
      "\n",
      "   Mean Heterotrait Correlation = 0.133\n",
      "   Mean Monotrait Correlation = 0.503\n",
      "   HTMT Ratio = 0.264\n",
      "\n",
      "   ‚úÖ Discriminant validity established (HTMT < 0.85, conservative)\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Discriminant validity assessment complete\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Discriminant Validity Assessment\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Extract inter-factor correlation from CFA\n",
    "if SEMOPY_AVAILABLE and 'model' in locals():\n",
    "    try:\n",
    "        # Get correlation between factors\n",
    "        std_solution = semopy.inspect(model, mode='std')\n",
    "        correlations = std_solution[std_solution['op'] == '~~']\n",
    "        \n",
    "        # Find F1 ~~ F2 correlation\n",
    "        f1_f2_corr = correlations[\n",
    "            ((correlations['lval'] == 'F1') & (correlations['rval'] == 'F2')) |\n",
    "            ((correlations['lval'] == 'F2') & (correlations['rval'] == 'F1'))\n",
    "        ]\n",
    "        \n",
    "        if not f1_f2_corr.empty:\n",
    "            inter_factor_corr = f1_f2_corr['Estimate'].values[0]\n",
    "            print(f\"Inter-factor Correlation (F1 ‚Üî F2): r = {inter_factor_corr:.3f}\\n\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not extract inter-factor correlation\\n\")\n",
    "            inter_factor_corr = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting correlations: {e}\\n\")\n",
    "        inter_factor_corr = None\n",
    "else:\n",
    "    inter_factor_corr = None\n",
    "\n",
    "# 2. Fornell-Larcker Criterion\n",
    "if reliability_results and inter_factor_corr is not None:\n",
    "    print(\"Method 1: Fornell-Larcker Criterion\\n\")\n",
    "    \n",
    "    sqrt_ave_f1 = reliability_df.loc[0, 'Sqrt_AVE']\n",
    "    sqrt_ave_f2 = reliability_df.loc[1, 'Sqrt_AVE']\n",
    "    \n",
    "    print(f\"   ‚àöAVE(F1) = {sqrt_ave_f1:.3f}\")\n",
    "    print(f\"   ‚àöAVE(F2) = {sqrt_ave_f2:.3f}\")\n",
    "    print(f\"   |r(F1,F2)| = {abs(inter_factor_corr):.3f}\\n\")\n",
    "    \n",
    "    if sqrt_ave_f1 > abs(inter_factor_corr) and sqrt_ave_f2 > abs(inter_factor_corr):\n",
    "        print(\"   ‚úÖ Discriminant validity established (Fornell-Larcker)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Discriminant validity NOT established (Fornell-Larcker)\")\n",
    "        print(\"      Factors may be too highly correlated\")\n",
    "\n",
    "# 3. HTMT Ratio (Manual calculation)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Method 2: Heterotrait-Monotrait (HTMT) Ratio\\n\")\n",
    "\n",
    "# Calculate mean inter-construct correlations\n",
    "f1_items = factor_items['F1_Mixed_Readiness']\n",
    "f2_items = factor_items['F2_Risk_Anxiety']\n",
    "\n",
    "# Heterotrait correlations (between factors)\n",
    "heterotrait_corrs = []\n",
    "for item1 in f1_items:\n",
    "    for item2 in f2_items:\n",
    "        corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "        heterotrait_corrs.append(abs(corr))\n",
    "\n",
    "mean_heterotrait = np.mean(heterotrait_corrs)\n",
    "\n",
    "# Monotrait correlations (within factors)\n",
    "f1_corrs = []\n",
    "for i, item1 in enumerate(f1_items):\n",
    "    for item2 in f1_items[i+1:]:\n",
    "        corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "        f1_corrs.append(abs(corr))\n",
    "\n",
    "f2_corrs = []\n",
    "if len(f2_items) > 1:\n",
    "    for i, item1 in enumerate(f2_items):\n",
    "        for item2 in f2_items[i+1:]:\n",
    "            corr = df_12item[[item1, item2]].corr().iloc[0, 1]\n",
    "            f2_corrs.append(abs(corr))\n",
    "\n",
    "mean_monotrait = np.mean(f1_corrs + f2_corrs) if f2_corrs else np.mean(f1_corrs)\n",
    "\n",
    "# HTMT ratio\n",
    "htmt = mean_heterotrait / mean_monotrait if mean_monotrait > 0 else np.nan\n",
    "\n",
    "print(f\"   Mean Heterotrait Correlation = {mean_heterotrait:.3f}\")\n",
    "print(f\"   Mean Monotrait Correlation = {mean_monotrait:.3f}\")\n",
    "print(f\"   HTMT Ratio = {htmt:.3f}\\n\")\n",
    "\n",
    "if htmt < 0.85:\n",
    "    print(f\"   ‚úÖ Discriminant validity established (HTMT < 0.85, conservative)\")\n",
    "elif htmt < 0.90:\n",
    "    print(f\"   ‚úÖ Discriminant validity established (HTMT < 0.90, liberal)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Discriminant validity NOT established (HTMT ‚â• 0.90)\")\n",
    "    print(f\"      Constructs may not be sufficiently distinct\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\n‚úÖ Discriminant validity assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef8eb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Visualize CFA Results\n",
    "\n",
    "Create publication-quality figures:\n",
    "1. Standardized loading plot\n",
    "2. Reliability metrics comparison\n",
    "3. Model fit indices visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d51ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardized Loading Plot\n",
    "if 'factor_loadings' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Factor loadings by factor\n",
    "    factor_loadings_sorted = factor_loadings.sort_values(['Factor', 'Std_Loading'], ascending=[True, False])\n",
    "    \n",
    "    colors = {'F1': '#3b82f6', 'F2': '#f59e0b'}\n",
    "    factor_colors = factor_loadings_sorted['Factor'].map(colors)\n",
    "    \n",
    "    axes[0].barh(range(len(factor_loadings_sorted)), factor_loadings_sorted['Std_Loading'], \n",
    "                 color=factor_colors, alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(factor_loadings_sorted)))\n",
    "    axes[0].set_yticklabels(factor_loadings_sorted['Item'])\n",
    "    axes[0].axvline(x=0.50, color='red', linestyle='--', linewidth=1, label='Threshold (0.50)')\n",
    "    axes[0].set_xlabel('Standardized Loading', fontsize=12)\n",
    "    axes[0].set_title('CFA Standardized Factor Loadings', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reliability metrics\n",
    "    if reliability_results:\n",
    "        x_pos = np.arange(len(reliability_df))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[1].bar(x_pos - width, reliability_df['Alpha'], width, label='Cronbach\\'s Œ±', alpha=0.8)\n",
    "        axes[1].bar(x_pos, reliability_df['CR'], width, label='CR', alpha=0.8)\n",
    "        axes[1].bar(x_pos + width, reliability_df['AVE'], width, label='AVE', alpha=0.8)\n",
    "        \n",
    "        axes[1].axhline(y=0.70, color='red', linestyle='--', linewidth=1, label='Œ±/CR Threshold')\n",
    "        axes[1].axhline(y=0.50, color='orange', linestyle='--', linewidth=1, label='AVE Threshold')\n",
    "        \n",
    "        axes[1].set_xticks(x_pos)\n",
    "        axes[1].set_xticklabels(['F1: Mixed\\nReadiness', 'F2: Risk/\\nAnxiety'])\n",
    "        axes[1].set_ylabel('Value', fontsize=12)\n",
    "        axes[1].set_title('Reliability and Convergent Validity', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(loc='lower right')\n",
    "        axes[1].set_ylim(0, 1.0)\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/cfa_loadings_reliability.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations saved to ../results/plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7360e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Research Questions Addressed\n",
    "\n",
    "**RQ1**: What is the psychometric structure of AI readiness among knowledge workers?\n",
    "- **Answer**: CFA confirms 2-factor structure (Mixed Readiness + Risk/Anxiety)\n",
    "\n",
    "### Proposal Compliance Checklist\n",
    "\n",
    "- [ ] CFI ‚â• 0.90\n",
    "- [ ] TLI ‚â• 0.90\n",
    "- [ ] RMSEA ‚â§ 0.08\n",
    "- [ ] SRMR ‚â§ 0.08\n",
    "- [ ] All loadings ‚â• 0.50\n",
    "- [ ] CR ‚â• 0.70 (both factors)\n",
    "- [ ] AVE ‚â• 0.50 (both factors)\n",
    "- [ ] Fornell-Larcker criterion met\n",
    "- [ ] HTMT < 0.85\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "*To be completed after model estimation*\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "‚úÖ Phase 2 Complete ‚Üí Proceed to **Phase 3: Measurement Invariance** (`03_Measurement_Invariance.ipynb`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7b14e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Export Results for Dissertation\n",
    "\n",
    "Generate APA-formatted tables for manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda17ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fit indices exported: ../results/tables/cfa_model_fit.csv\n",
      "\n",
      "‚úÖ All results exported successfully\n",
      "\n",
      "üìã Ready for integration into dissertation manuscript\n",
      "\n",
      "\n",
      "‚úÖ All results exported successfully\n",
      "\n",
      "üìã Ready for integration into dissertation manuscript\n"
     ]
    }
   ],
   "source": [
    "# Create results directory if needed\n",
    "Path('../results/tables').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export factor loadings\n",
    "if 'factor_loadings' in locals():\n",
    "    factor_loadings.to_csv('../results/tables/cfa_factor_loadings.csv', index=False)\n",
    "    print(\"‚úÖ Factor loadings exported: ../results/tables/cfa_factor_loadings.csv\")\n",
    "\n",
    "# Export reliability metrics\n",
    "if reliability_results:\n",
    "    reliability_df.to_csv('../results/tables/cfa_reliability_validity.csv', index=False)\n",
    "    print(\"‚úÖ Reliability metrics exported: ../results/tables/cfa_reliability_validity.csv\")\n",
    "\n",
    "# Export fit indices\n",
    "if 'fit_results' in locals():\n",
    "    fit_results.to_csv('../results/tables/cfa_model_fit.csv')\n",
    "    print(\"‚úÖ Fit indices exported: ../results/tables/cfa_model_fit.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All results exported successfully\")\n",
    "print(\"\\nüìã Ready for integration into dissertation manuscript\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
