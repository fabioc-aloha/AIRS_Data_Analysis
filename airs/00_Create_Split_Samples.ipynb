{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f31c49",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1b9467fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dca19f",
   "metadata": {},
   "source": [
    "## 1.1 Load and Preprocess Raw Survey Data\n",
    "\n",
    "Clean the raw survey export file by removing metadata rows and mapping column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d1aff34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded: 325 responses\n",
      "Raw columns: 54\n",
      "\n",
      "First 5 column names: ['Session ID', 'Publish ID', 'Collector', 'Language', 'Start Date (America/Denver)']\n"
     ]
    }
   ],
   "source": [
    "# Load raw survey export with text labels (skip first 2 metadata rows, use row 3 as header)\n",
    "df_raw = pd.read_csv('../data/AIRS---AI-Readiness-Scale-labels.csv', skiprows=2)\n",
    "\n",
    "print(f\"Raw data loaded: {len(df_raw)} responses\")\n",
    "print(f\"Raw columns: {len(df_raw.columns)}\")\n",
    "print(f\"\\nFirst 5 column names: {df_raw.columns[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3642f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Columns renamed and selected\n",
      "Cleaned dataset: 325 rows × 39 columns\n",
      "Columns: ['Duration_seconds', 'Progress', 'PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2']\n"
     ]
    }
   ],
   "source": [
    "# Column mapping from survey question names to variable codes\n",
    "column_mapping = {\n",
    "    'Duration (seconds)': 'Duration_seconds',\n",
    "    'Progress': 'Progress',  # Keep for potential filtering\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | AI tools help me accomplish tasks more quickly': 'PE1',\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | Using AI improves the quality of my work or studies': 'PE2',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Learning to use AI tools is easy for me': 'EE1',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Interacting with AI tools is clear and understandable': 'EE2',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | People whose opinions I value encourage me to use AI tools': 'SI1',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | Leaders in my organization or school support the use of AI tools': 'SI2',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | I have access to training or tutorials for the AI tools I use': 'FC1',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | The AI tools I use are compatible with other tools or systems I use': 'FC2',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | Using AI tools is stimulating and engaging': 'HM1',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | AI tools make my work or studies more interesting': 'HM2',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | I get more value from AI tools than the effort they require': 'PV1',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | Using AI tools is worth the learning curve': 'PV2',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | Using AI tools has become a habit for me': 'HB1',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | I tend to rely on AI tools by default when I need help with tasks': 'HB2',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I choose to use AI tools in my work because I find them helpful, not because I am required to': 'VO1',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I could choose not to use AI tools in my work or studies if I preferred.': 'VO2',\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust AI tools to provide reliable information': 'TR1',\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust the AI tools that are available to me': 'TR2',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I understand how the AI tools I use generate their outputs': 'EX1',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I prefer AI tools that explain their recommendations': 'EX2',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I worry that AI tools could replace jobs in my field': 'ER1',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I am concerned about privacy risks when using AI tools': 'ER2',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I feel uneasy about the increasing use of AI': 'AX1',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I worry that I may be left behind if I do not keep up with AI': 'AX2',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I am ready to use more AI tools in my work or studies': 'BI1',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I would recommend AI tools to others': 'BI2',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I see AI as an important part of my future': 'BI3',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I plan to increase my use of AI tools in the next six months': 'BI4',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Microsoft 365 Copilot or Microsoft Copilot': 'Usage_MSCopilot',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | ChatGPT': 'Usage_ChatGPT',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Google Gemini': 'Usage_Gemini',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Other AI tools (for example, Claude, Perplexity, Grok)': 'Usage_Other',\n",
    "    'What is your current status?': 'Role',\n",
    "    'What is your highest level of education completed?': 'Education',\n",
    "    'Which industry or field best describes your primary area of work or study?': 'Industry',\n",
    "    'How many years of work or study experience do you have in your field?': 'Experience',\n",
    "    'Do you identify as a person with a disability (for example, vision, mobility, neurodivergence)?': 'Disability'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df = df_raw.rename(columns=column_mapping)\n",
    "\n",
    "# Select only the mapped columns (including Duration_seconds and Progress)\n",
    "mapped_cols = list(column_mapping.values())\n",
    "available_cols = [col for col in mapped_cols if col in df.columns]\n",
    "df = df[available_cols].copy()\n",
    "\n",
    "print(f\"✓ Columns renamed and selected\")\n",
    "print(f\"Cleaned dataset: {len(df)} rows × {len(df.columns)} columns\")\n",
    "print(f\"Columns: {df.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "45ee5acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Likert items converted from text labels to numeric (1-5)\n",
      "✓ Usage frequency items converted from text labels to numeric (1-5)\n",
      "✓ Demographic items validated (already text labels)\n"
     ]
    }
   ],
   "source": [
    "# Convert duration from seconds to minutes\n",
    "df['Duration_minutes'] = df['Duration_seconds'] / 60\n",
    "df = df.drop('Duration_seconds', axis=1)\n",
    "\n",
    "# Likert items are already numeric (1-5) in the export\n",
    "likert_items = ['PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2', \n",
    "                'HM1', 'HM2', 'PV1', 'PV2', 'HB1', 'HB2', 'VO1', 'VO2',\n",
    "                'TR1', 'TR2', 'EX1', 'EX2', 'ER1', 'ER2', 'AX1', 'AX2',\n",
    "                'BI1', 'BI2', 'BI3', 'BI4']\n",
    "\n",
    "# Usage frequency items are text labels in the export\n",
    "# Map to numeric scale: Never=1, Rarely=2, Sometimes=3, Often=4, Daily=5\n",
    "usage_items = ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
    "\n",
    "usage_mapping = {\n",
    "    'Never': 1,\n",
    "    'Rarely': 2,\n",
    "    'Sometimes': 3,\n",
    "    'Often': 4,\n",
    "    'Daily': 5\n",
    "}\n",
    "\n",
    "for item in usage_items:\n",
    "    df[item] = df[item].map(usage_mapping)\n",
    "\n",
    "# Likert items are text labels in the export\n",
    "# Map to numeric scale: Strongly disagree=1, Disagree=2, Neutral=3, Agree=4, Strongly agree=5\n",
    "likert_mapping = {\n",
    "    'Strongly disagree': 1,\n",
    "    'Disagree': 2,\n",
    "    'Neutral': 3,\n",
    "    'Agree': 4,\n",
    "    'Strongly agree': 5\n",
    "}\n",
    "\n",
    "for item in likert_items:\n",
    "    df[item] = df[item].map(likert_mapping)\n",
    "\n",
    "# Demographics are already text labels - clean up formatting for consistency\n",
    "df['Role'] = df['Role'].str.strip()\n",
    "df['Education'] = df['Education'].str.strip()\n",
    "df['Experience'] = df['Experience'].str.strip()\n",
    "df['Disability'] = df['Disability'].str.strip()\n",
    "\n",
    "print(\"✓ Likert items converted from text labels to numeric (1-5)\")\n",
    "print(\"✓ Usage frequency items converted from text labels to numeric (1-5)\")\n",
    "print(\"✓ Demographic items validated (already text labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fd20279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Derived variables calculated:\n",
      "  - AI Adoption: 292 adopters, 33 non-adopters\n",
      "  - Usage Intensity: {'Medium': 107, 'Low': 95, 'High': 90, 'Non-User': 33}\n",
      "  - Work Context: {'Professional': 180, 'Academic-Student': 120, 'Academic-Faculty': 25}\n"
     ]
    }
   ],
   "source": [
    "# Calculate derived variables\n",
    "\n",
    "# 1. AI Adoption (binary: 1 = any AI usage, 0 = no AI usage)\n",
    "# Logic: If ALL usage == 1 (Never), then AI_Adoption = 0, else 1\n",
    "df['AI_Adoption'] = ((df['Usage_MSCopilot'] > 1) | \n",
    "                     (df['Usage_ChatGPT'] > 1) | \n",
    "                     (df['Usage_Gemini'] > 1) | \n",
    "                     (df['Usage_Other'] > 1)).astype(int)\n",
    "\n",
    "# 2. Total usage score (sum of all usage frequencies)\n",
    "df['Total_Usage_Score'] = df[usage_items].sum(axis=1)\n",
    "\n",
    "# 3. AI Adoption Level (categorical: None/Single/Multiple based on number of tools used)\n",
    "tools_used = (df[usage_items] > 1).sum(axis=1)  # Count tools with usage > 1 (more than Never)\n",
    "df['AI_Adoption_Level'] = tools_used.apply(lambda x: 'None' if x == 0 else \n",
    "                                                      'Single' if x == 1 else \n",
    "                                                      'Multiple')\n",
    "\n",
    "# 4. Primary Tool (which tool is used most frequently)\n",
    "tool_names = ['Microsoft 365 Copilot', 'ChatGPT', 'Gemini', 'Other AI Tool']\n",
    "def get_primary_tool(row):\n",
    "    if row['AI_Adoption'] == 0:\n",
    "        return 'None'\n",
    "    usage_cols = ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
    "    # Only consider tools with usage > 1 (more than Never)\n",
    "    active_tools = {tool_names[i]: row[col] for i, col in enumerate(usage_cols) if row[col] > 1}\n",
    "    if not active_tools:\n",
    "        return 'None'\n",
    "    return max(active_tools, key=active_tools.get)\n",
    "\n",
    "df['Primary_Tool'] = df.apply(get_primary_tool, axis=1)\n",
    "\n",
    "# 5. Experience Level (categorical based on years of experience)\n",
    "# Label export uses different text: \"1 to 3 years\" vs \"1-3 years\"\n",
    "experience_level_mapping = {\n",
    "    'Less than 1 year': 'Entry',\n",
    "    '1 to 3 years': 'Early',\n",
    "    '4 to 6 years': 'Mid',\n",
    "    '7 to 10 years': 'Senior',\n",
    "    '11 or more years': 'Expert'\n",
    "}\n",
    "df['Experience_Level'] = df['Experience'].map(experience_level_mapping)\n",
    "\n",
    "# 6. Work Context (map role + industry to professional/academic)\n",
    "# Label export uses employment status + industry to determine academic affiliation\n",
    "def map_work_context(row):\n",
    "    role = row['Role']\n",
    "    industry = row['Industry']\n",
    "    \n",
    "    if pd.isna(role):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    role_lower = str(role).lower()\n",
    "    industry_lower = str(industry).lower() if pd.notna(industry) else ''\n",
    "    \n",
    "    # Students are Academic-Student\n",
    "    if 'student' in role_lower:\n",
    "        return 'Academic-Student'\n",
    "    # Education industry workers who are employed are likely Academic-Faculty\n",
    "    # Use 'employed -' (with space-dash) to match \"Employed - *\" but not \"Not currently employed\"\n",
    "    elif 'education' in industry_lower and ('employed -' in role_lower or 'freelancer' in role_lower):\n",
    "        return 'Academic-Faculty'\n",
    "    # Everyone else is Professional\n",
    "    else:\n",
    "        return 'Professional'\n",
    "\n",
    "df['Work_Context'] = df.apply(map_work_context, axis=1)\n",
    "\n",
    "# 7. Usage Intensity (categorical based on total usage score)\n",
    "# Scale: 1-5 per tool × 4 tools = range 4-20\n",
    "# Thresholds: ≤4=Non-User, ≤8=Low, ≤12=Medium, >12=High\n",
    "def map_usage_intensity(score):\n",
    "    if score <= 4:  # All \"Never\" = 4\n",
    "        return 'Non-User'\n",
    "    elif score <= 8:  # Low usage\n",
    "        return 'Low'\n",
    "    elif score <= 12:  # Medium usage\n",
    "        return 'Medium'\n",
    "    else:  # High usage (13-20)\n",
    "        return 'High'\n",
    "\n",
    "df['Usage_Intensity'] = df['Total_Usage_Score'].apply(map_usage_intensity)\n",
    "\n",
    "print(\"✓ Derived variables calculated:\")\n",
    "print(f\"  - AI Adoption: {df['AI_Adoption'].sum()} adopters, {(df['AI_Adoption'] == 0).sum()} non-adopters\")\n",
    "print(f\"  - Usage Intensity: {df['Usage_Intensity'].value_counts().to_dict()}\")\n",
    "print(f\"  - Work Context: {df['Work_Context'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "17dbd05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "✅ PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "Cleaned dataset: 325 rows × 45 columns\n",
      "Saved to: ../data/AIRS_clean.csv\n",
      "\n",
      "Ready for split-sample creation\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns for better organization\n",
    "column_order = ['Duration_minutes'] + likert_items + \\\n",
    "               ['Role', 'Education', 'Industry', 'Experience', 'Disability'] + \\\n",
    "               usage_items + \\\n",
    "               ['AI_Adoption', 'AI_Adoption_Level', 'Primary_Tool', 'Experience_Level', \n",
    "                'Work_Context', 'Usage_Intensity', 'Total_Usage_Score']\n",
    "\n",
    "df = df[column_order]\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('../data/AIRS_clean.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Cleaned dataset: {len(df)} rows × {len(df.columns)} columns\")\n",
    "print(f\"Saved to: ../data/AIRS_clean.csv\")\n",
    "print(f\"\\nReady for split-sample creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61af51d",
   "metadata": {},
   "source": [
    "## 1.2 Load Cleaned Data for Split-Sample Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dba989d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample: N = 325\n",
      "Variables: 45\n",
      "\n",
      "First few columns: ['Duration_minutes', 'PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2', 'HM1']\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "df = pd.read_csv('../data/AIRS_clean.csv')\n",
    "\n",
    "print(f\"Total sample: N = {len(df)}\")\n",
    "print(f\"Variables: {len(df.columns)}\")\n",
    "print(f\"\\nFirst few columns: {df.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7cc4e",
   "metadata": {},
   "source": [
    "## 2. Pre-Split Distributions\n",
    "\n",
    "Document baseline distributions before splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dd844fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRE-SPLIT DISTRIBUTIONS\n",
      "======================================================================\n",
      "\n",
      "1. Work Context:\n",
      "Work_Context\n",
      "Professional        180\n",
      "Academic-Student    120\n",
      "Academic-Faculty     25\n",
      "Name: count, dtype: int64\n",
      "Proportions: {'Professional': 55.4, 'Academic-Student': 36.9, 'Academic-Faculty': 7.7}\n",
      "\n",
      "2. AI Adoption:\n",
      "Adopters (1): 292 (89.8%)\n",
      "Non-Adopters (0): 33 (10.2%)\n",
      "\n",
      "3. Usage Intensity:\n",
      "Usage_Intensity\n",
      "Medium      107\n",
      "Low          95\n",
      "High         90\n",
      "Non-User     33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. Experience Level:\n",
      "Experience_Level\n",
      "Expert    110\n",
      "Entry      65\n",
      "Early      57\n",
      "Mid        53\n",
      "Senior     40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PRE-SPLIT DISTRIBUTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Work Context\n",
    "print(\"\\n1. Work Context:\")\n",
    "work_context_dist = df['Work_Context'].value_counts()\n",
    "print(work_context_dist)\n",
    "print(f\"Proportions: {(work_context_dist / len(df) * 100).round(1).to_dict()}\")\n",
    "\n",
    "# AI Adoption\n",
    "print(\"\\n2. AI Adoption:\")\n",
    "ai_adoption_dist = df['AI_Adoption'].value_counts()\n",
    "print(f\"Adopters (1): {ai_adoption_dist.get(1, 0)} ({ai_adoption_dist.get(1, 0)/len(df)*100:.1f}%)\")\n",
    "print(f\"Non-Adopters (0): {ai_adoption_dist.get(0, 0)} ({ai_adoption_dist.get(0, 0)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Usage Intensity\n",
    "print(\"\\n3. Usage Intensity:\")\n",
    "usage_intensity_dist = df['Usage_Intensity'].value_counts()\n",
    "print(usage_intensity_dist)\n",
    "\n",
    "# Experience Level\n",
    "print(\"\\n4. Experience Level:\")\n",
    "experience_dist = df['Experience_Level'].value_counts()\n",
    "print(experience_dist)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7cfad",
   "metadata": {},
   "source": [
    "## 3. Create Stratified Split\n",
    "\n",
    "Stratify by **Work Context × AI Adoption** to ensure balanced representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "46b733e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratification groups (Work Context × AI Adoption):\n",
      "stratify_key\n",
      "Academic-Faculty_0      3\n",
      "Academic-Faculty_1     22\n",
      "Academic-Student_0      5\n",
      "Academic-Student_1    115\n",
      "Professional_0         25\n",
      "Professional_1        155\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique strata: 6\n"
     ]
    }
   ],
   "source": [
    "# Create stratification variable (Work Context × AI Adoption)\n",
    "df['stratify_key'] = df['Work_Context'].astype(str) + '_' + df['AI_Adoption'].astype(str)\n",
    "\n",
    "print(\"Stratification groups (Work Context × AI Adoption):\")\n",
    "print(df['stratify_key'].value_counts().sort_index())\n",
    "print(f\"\\nTotal unique strata: {df['stratify_key'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fdf5a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPLIT-SAMPLE CREATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Development sample: N = 162 (49.8%)\n",
      "Holdout sample:     N = 163 (50.2%)\n",
      "Total:              N = 325\n",
      "\n",
      "✓ Stratified by: Work Context × AI Adoption\n",
      "✓ Random seed: 42 (reproducible)\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified 50/50 split\n",
    "df_dev, df_holdout = train_test_split(\n",
    "    df,\n",
    "    test_size=0.50,\n",
    "    stratify=df['stratify_key'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLIT-SAMPLE CREATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDevelopment sample: N = {len(df_dev)} ({len(df_dev)/len(df)*100:.1f}%)\")\n",
    "print(f\"Holdout sample:     N = {len(df_holdout)} ({len(df_holdout)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total:              N = {len(df)}\")\n",
    "print(\"\\n✓ Stratified by: Work Context × AI Adoption\")\n",
    "print(f\"✓ Random seed: {RANDOM_SEED} (reproducible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54397d65",
   "metadata": {},
   "source": [
    "## 4. Validation: Post-Split Balance\n",
    "\n",
    "Verify stratification succeeded with chi-square tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2942b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POST-SPLIT BALANCE TESTS (Chi-Square)\n",
      "======================================================================\n",
      "\n",
      "Work_Context:\n",
      "                  Development  Holdout\n",
      "Work_Context                          \n",
      "Academic-Faculty           13       12\n",
      "Academic-Student           60       60\n",
      "Professional               89       91\n",
      "\n",
      "χ² = 0.059, df = 2, p = 0.9709\n",
      "✓ Balanced (p > 0.05): No significant difference between samples\n",
      "\n",
      "AI_Adoption:\n",
      "             Development  Holdout\n",
      "AI_Adoption                      \n",
      "0                     17       16\n",
      "1                    145      147\n",
      "\n",
      "χ² = 0.000, df = 1, p = 0.9851\n",
      "✓ Balanced (p > 0.05): No significant difference between samples\n"
     ]
    }
   ],
   "source": [
    "def test_balance(variable_name, dev_sample, holdout_sample):\n",
    "    \"\"\"\n",
    "    Test if variable distribution differs between development and holdout samples.\n",
    "    H0: Distributions are equal (balanced split)\n",
    "    \"\"\"\n",
    "    # Create proper contingency table\n",
    "    dev_counts = dev_sample[variable_name].value_counts().sort_index()\n",
    "    holdout_counts = holdout_sample[variable_name].value_counts().sort_index()\n",
    "    \n",
    "    contingency_table = pd.DataFrame({\n",
    "        'Development': dev_counts,\n",
    "        'Holdout': holdout_counts\n",
    "    }).fillna(0)\n",
    "    \n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"\\n{variable_name}:\")\n",
    "    print(contingency_table)\n",
    "    print(f\"\\nχ² = {chi2:.3f}, df = {dof}, p = {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > 0.05:\n",
    "        print(f\"✓ Balanced (p > 0.05): No significant difference between samples\")\n",
    "    else:\n",
    "        print(f\"⚠ Imbalanced (p ≤ 0.05): Significant difference detected\")\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POST-SPLIT BALANCE TESTS (Chi-Square)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "p_work_context = test_balance('Work_Context', df_dev, df_holdout)\n",
    "p_ai_adoption = test_balance('AI_Adoption', df_dev, df_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa835e",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistics: Compare Samples\n",
    "\n",
    "Compare key Likert item means to ensure no systematic bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7465ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LIKERT ITEM MEANS: Development vs. Holdout\n",
      "======================================================================\n",
      "\n",
      "Sample of items (first 8):\n",
      "     Development_M  Development_SD  Holdout_M  Holdout_SD   Diff  Abs_Diff\n",
      "PE1          3.660           1.154      3.571       1.122  0.090     0.090\n",
      "PE2          3.340           1.201      3.325       1.217  0.014     0.014\n",
      "EE1          3.809           1.013      3.736       0.987  0.072     0.072\n",
      "EE2          3.654           1.036      3.577       1.024  0.078     0.078\n",
      "SI1          3.136           1.187      3.037       1.201  0.099     0.099\n",
      "SI2          3.383           1.143      3.282       1.130  0.101     0.101\n",
      "FC1          3.272           1.195      3.209       1.214  0.063     0.063\n",
      "FC2          3.444           1.092      3.429       1.018  0.015     0.015\n",
      "\n",
      "Mean absolute difference across all items: 0.081\n",
      "Max absolute difference: 0.186 (TR1)\n",
      "\n",
      "✓ Excellent balance: Mean difference < 0.10 scale points\n"
     ]
    }
   ],
   "source": [
    "# Define construct items\n",
    "construct_items = {\n",
    "    'PE': ['PE1', 'PE2'],\n",
    "    'EE': ['EE1', 'EE2'],\n",
    "    'SI': ['SI1', 'SI2'],\n",
    "    'FC': ['FC1', 'FC2'],\n",
    "    'HM': ['HM1', 'HM2'],\n",
    "    'PV': ['PV1', 'PV2'],\n",
    "    'HB': ['HB1', 'HB2'],\n",
    "    'VO': ['VO1', 'VO2'],\n",
    "    'TR': ['TR1', 'TR2'],\n",
    "    'EX': ['EX1', 'EX2'],\n",
    "    'ER': ['ER1', 'ER2'],\n",
    "    'AX': ['AX1', 'AX2'],\n",
    "    'BI': ['BI1', 'BI2', 'BI3', 'BI4']\n",
    "}\n",
    "\n",
    "all_items = [item for items in construct_items.values() for item in items]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LIKERT ITEM MEANS: Development vs. Holdout\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Development_M': df_dev[all_items].mean(),\n",
    "    'Development_SD': df_dev[all_items].std(),\n",
    "    'Holdout_M': df_holdout[all_items].mean(),\n",
    "    'Holdout_SD': df_holdout[all_items].std(),\n",
    "    'Diff': df_dev[all_items].mean() - df_holdout[all_items].mean()\n",
    "})\n",
    "\n",
    "comparison['Abs_Diff'] = comparison['Diff'].abs()\n",
    "\n",
    "print(\"\\nSample of items (first 8):\")\n",
    "print(comparison.head(8).round(3))\n",
    "\n",
    "print(f\"\\nMean absolute difference across all items: {comparison['Abs_Diff'].mean():.3f}\")\n",
    "print(f\"Max absolute difference: {comparison['Abs_Diff'].max():.3f} ({comparison['Abs_Diff'].idxmax()})\")\n",
    "\n",
    "if comparison['Abs_Diff'].mean() < 0.10:\n",
    "    print(\"\\n✓ Excellent balance: Mean difference < 0.10 scale points\")\n",
    "elif comparison['Abs_Diff'].mean() < 0.20:\n",
    "    print(\"\\n✓ Good balance: Mean difference < 0.20 scale points\")\n",
    "else:\n",
    "    print(\"\\n⚠ Review: Mean difference ≥ 0.20 scale points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d1834",
   "metadata": {},
   "source": [
    "## 6. Save Split Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fc5be722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAMPLES SAVED\n",
      "======================================================================\n",
      "\n",
      "✓ Development sample: data/AIRS_clean_dev.csv\n",
      "  N = 162, Variables = 45\n",
      "\n",
      "✓ Holdout sample: data/AIRS_clean_holdout.csv\n",
      "  N = 163, Variables = 45\n",
      "\n",
      "✓ Stratification key removed from saved files\n",
      "\n",
      "======================================================================\n",
      "\n",
      "SAMPLES SAVED\n",
      "======================================================================\n",
      "\n",
      "✓ Development sample: data/AIRS_clean_dev.csv\n",
      "  N = 162, Variables = 45\n",
      "\n",
      "✓ Holdout sample: data/AIRS_clean_holdout.csv\n",
      "  N = 163, Variables = 45\n",
      "\n",
      "✓ Stratification key removed from saved files\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove temporary stratification key\n",
    "df_dev_clean = df_dev.drop(columns=['stratify_key'])\n",
    "df_holdout_clean = df_holdout.drop(columns=['stratify_key'])\n",
    "\n",
    "# Save to CSV\n",
    "df_dev_clean.to_csv('../data/AIRS_clean_dev.csv', index=False)\n",
    "df_holdout_clean.to_csv('../data/AIRS_clean_holdout.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLES SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Development sample: data/AIRS_clean_dev.csv\")\n",
    "print(f\"  N = {len(df_dev_clean)}, Variables = {len(df_dev_clean.columns)}\")\n",
    "print(f\"\\n✓ Holdout sample: data/AIRS_clean_holdout.csv\")\n",
    "print(f\"  N = {len(df_holdout_clean)}, Variables = {len(df_holdout_clean.columns)}\")\n",
    "print(\"\\n✓ Stratification key removed from saved files\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6b804",
   "metadata": {},
   "source": [
    "## 7. Summary and Quality Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4bcefd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPLIT-SAMPLE VALIDATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Quality Checklist:\n",
      "  ✓ Sample sizes approximately equal\n",
      "  ✓ Development sample ≥ 150 (adequate for EFA)\n",
      "  ✓ Holdout sample ≥ 150 (adequate for CFA)\n",
      "  ✓ Work Context balanced (p > 0.05)\n",
      "  ✓ AI Adoption balanced (p > 0.05)\n",
      "  ✓ Mean item difference < 0.20\n",
      "  ✓ Files saved successfully\n",
      "\n",
      "======================================================================\n",
      "✅ ALL VALIDATION CHECKS PASSED\n",
      "======================================================================\n",
      "\n",
      "Ready for Phase 1: Exploratory Factor Analysis (EFA)\n",
      "Next notebook: 01_EFA_Split_Sample_Development.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SPLIT-SAMPLE VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checks = [\n",
    "    (\"Sample sizes approximately equal\", abs(len(df_dev) - len(df_holdout)) <= 2),\n",
    "    (\"Development sample ≥ 150 (adequate for EFA)\", len(df_dev) >= 150),\n",
    "    (\"Holdout sample ≥ 150 (adequate for CFA)\", len(df_holdout) >= 150),\n",
    "    (\"Work Context balanced (p > 0.05)\", p_work_context > 0.05),\n",
    "    (\"AI Adoption balanced (p > 0.05)\", p_ai_adoption > 0.05),\n",
    "    (\"Mean item difference < 0.20\", comparison['Abs_Diff'].mean() < 0.20),\n",
    "    (\"Files saved successfully\", True)  # If we got here, it succeeded\n",
    "]\n",
    "\n",
    "print(\"\\nQuality Checklist:\")\n",
    "for check, passed in checks:\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {check}\")\n",
    "\n",
    "all_passed = all(passed for _, passed in checks)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ ALL VALIDATION CHECKS PASSED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nReady for Phase 1: Exploratory Factor Analysis (EFA)\")\n",
    "    print(\"Next notebook: 01_EFA_Split_Sample_Development.ipynb\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"⚠ REVIEW REQUIRED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nSome validation checks failed. Review results above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf33d59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "**Stratification Strategy**:\n",
    "- Stratified by Work Context × AI Adoption (6 groups)\n",
    "- Ensures balanced representation of key moderator variables\n",
    "- Critical for moderation analysis (H4)\n",
    "\n",
    "**Sample Size Adequacy**:\n",
    "- N ≈ 159 per sample supports 12-factor CFA (N:q ratio ≈ 6.6:1 for 24 items)\n",
    "- Minimum recommended: 5:1 for stable solutions (Bentler & Chou, 1987)\n",
    "- Both samples adequate for planned analyses\n",
    "\n",
    "**Random Seed**: 42 (reproducible splits for dissertation transparency)\n",
    "\n",
    "**Next Steps**:\n",
    "1. Run EFA on development sample (polychoric correlations)\n",
    "2. Select items based on loadings ≥ 0.50, cross-loadings < 0.30\n",
    "3. Test measurement model with CFA on holdout sample\n",
    "4. Proceed to structural modeling if fit indices acceptable\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
