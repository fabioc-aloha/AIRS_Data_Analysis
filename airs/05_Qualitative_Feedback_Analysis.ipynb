{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c930a27",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "df = pd.read_csv('../data/AIRS_clean.csv')\n",
    "\n",
    "print(f\"Total sample size: N={len(df)}\")\n",
    "print(f\"\\nChecking for Feedback variable...\")\n",
    "\n",
    "# Check if Feedback variable exists\n",
    "if 'Feedback' in df.columns:\n",
    "    print(\"✓ Feedback variable found\")\n",
    "    \n",
    "    # Assess response rate\n",
    "    df['Has_Feedback'] = df['Feedback'].notna() & (df['Feedback'].str.strip() != '')\n",
    "    n_responses = df['Has_Feedback'].sum()\n",
    "    response_rate = (n_responses / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nFeedback Response Rate:\")\n",
    "    print(f\"  Responses: n={n_responses} ({response_rate:.1f}%)\")\n",
    "    print(f\"  No Response: n={len(df) - n_responses} ({100-response_rate:.1f}%)\")\n",
    "    \n",
    "    # Extract responses\n",
    "    df_feedback = df[df['Has_Feedback']].copy()\n",
    "    print(f\"\\n✓ Extracted {len(df_feedback)} responses for analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Feedback variable NOT FOUND in dataset\")\n",
    "    print(\"\\nAvailable text columns:\")\n",
    "    text_cols = df.select_dtypes(include='object').columns\n",
    "    for col in text_cols:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3427519",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Exploration and Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feedback characteristics\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    # Response length statistics\n",
    "    df_feedback['Response_Length'] = df_feedback['Feedback'].str.len()\n",
    "    df_feedback['Word_Count'] = df_feedback['Feedback'].str.split().str.len()\n",
    "    \n",
    "    print(\"\\n=== Feedback Response Characteristics ===\")\n",
    "    print(f\"Mean character length: M={df_feedback['Response_Length'].mean():.1f}, SD={df_feedback['Response_Length'].std():.1f}\")\n",
    "    print(f\"Mean word count: M={df_feedback['Word_Count'].mean():.1f}, SD={df_feedback['Word_Count'].std():.1f}\")\n",
    "    print(f\"Median word count: {df_feedback['Word_Count'].median():.0f} words\")\n",
    "    print(f\"Range: {df_feedback['Word_Count'].min():.0f} - {df_feedback['Word_Count'].max():.0f} words\")\n",
    "    \n",
    "    # Distribution visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.hist(df_feedback['Word_Count'], bins=20, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Word Count', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'Distribution of Feedback Length (n={len(df_feedback)})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.axvline(df_feedback['Word_Count'].median(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f\"Median = {df_feedback['Word_Count'].median():.0f}\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Response rate by role\n",
    "    if 'Role' in df.columns:\n",
    "        response_by_role = df.groupby('Role')['Has_Feedback'].agg(['sum', 'count'])\n",
    "        response_by_role['Rate'] = (response_by_role['sum'] / response_by_role['count']) * 100\n",
    "        \n",
    "        ax2.bar(response_by_role.index, response_by_role['Rate'], \n",
    "                color=['#1f77b4', '#ff7f0e'], edgecolor='black')\n",
    "        ax2.set_xlabel('Role', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Feedback Response Rate (%)', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Feedback Response Rate by Role', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        for i, (idx, row) in enumerate(response_by_role.iterrows()):\n",
    "            ax2.text(i, row['Rate'] + 2, f\"{row['Rate']:.1f}%\\n(n={int(row['sum'])})\", \n",
    "                    ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/07_feedback_descriptives.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Figure saved: 07_feedback_descriptives.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228025c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sample Responses Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample responses for initial familiarization\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    print(\"\\n=== Sample Responses (First 10) ===\")\n",
    "    print(\"\\n(Reading these to identify initial themes...)\\n\")\n",
    "    \n",
    "    for i, response in enumerate(df_feedback['Feedback'].head(10), 1):\n",
    "        print(f\"\\n[Response {i}]\")\n",
    "        print(f\"{response}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n[Additional responses available for thematic coding...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da21277",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    # Combine all feedback into single text\n",
    "    all_text = ' '.join(df_feedback['Feedback'].str.lower())\n",
    "    \n",
    "    # Remove punctuation and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "    \n",
    "    # Define stopwords (common words to exclude)\n",
    "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'be', 'been',\n",
    "                 'it', 'this', 'that', 'these', 'those', 'i', 'my', 'me', 'we', 'us',\n",
    "                 'can', 'will', 'would', 'could', 'should', 'have', 'has', 'had',\n",
    "                 'not', 'no', 'yes', 'so', 'very', 'too', 'more', 'most', 'much'}\n",
    "    \n",
    "    # Filter stopwords\n",
    "    words_filtered = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words_filtered)\n",
    "    top_words = word_counts.most_common(30)\n",
    "    \n",
    "    print(\"\\n=== Top 30 Most Frequent Words ===\")\n",
    "    for i, (word, count) in enumerate(top_words, 1):\n",
    "        print(f\"{i:2d}. {word:20s} ({count:3d} occurrences)\")\n",
    "    \n",
    "    # Visualization: Word frequency bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    words_top20 = [w[0] for w in top_words[:20]]\n",
    "    counts_top20 = [w[1] for w in top_words[:20]]\n",
    "    \n",
    "    ax.barh(words_top20[::-1], counts_top20[::-1], color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Word', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top 20 Most Frequent Words in Feedback (n={len(df_feedback)} responses)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/07_feedback_word_frequency.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Figure saved: 07_feedback_word_frequency.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbc430",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Keyword-Based Thematic Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify themes based on keyword presence\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    # Define keyword dictionaries for potential themes\n",
    "    theme_keywords = {\n",
    "        'Accuracy_Reliability': ['accurate', 'accuracy', 'reliable', 'trust', 'correct', 'wrong', 'error', 'mistake', 'verify'],\n",
    "        'Job_Displacement': ['job', 'replace', 'displacement', 'employment', 'career', 'work', 'obsolete'],\n",
    "        'Productivity': ['productive', 'efficiency', 'efficient', 'faster', 'save', 'time', 'quick'],\n",
    "        'Learning_Education': ['learn', 'learning', 'education', 'teaching', 'student', 'study', 'knowledge'],\n",
    "        'Ethics_Bias': ['bias', 'biased', 'ethical', 'ethics', 'fair', 'fairness', 'discrimination', 'responsible'],\n",
    "        'Privacy_Security': ['privacy', 'private', 'security', 'secure', 'data', 'confidential', 'personal'],\n",
    "        'Creativity': ['creative', 'creativity', 'innovative', 'innovation', 'original', 'idea'],\n",
    "        'Control_Autonomy': ['control', 'autonomy', 'dependent', 'reliance', 'independent'],\n",
    "        'Explainability': ['explain', 'explanation', 'understand', 'transparent', 'clarity', 'clear', 'black box'],\n",
    "        'Skepticism': ['skeptical', 'doubt', 'concern', 'concerned', 'worry', 'worried', 'hesitant', 'uncomfortable']\n",
    "    }\n",
    "    \n",
    "    # Code responses for each theme\n",
    "    for theme, keywords in theme_keywords.items():\n",
    "        df_feedback[f'Theme_{theme}'] = df_feedback['Feedback'].str.lower().apply(\n",
    "            lambda x: any(kw in x for kw in keywords)\n",
    "        )\n",
    "    \n",
    "    # Calculate theme prevalence\n",
    "    theme_counts = {theme: df_feedback[f'Theme_{theme}'].sum() for theme in theme_keywords.keys()}\n",
    "    theme_pcts = {theme: (count / len(df_feedback)) * 100 for theme, count in theme_counts.items()}\n",
    "    \n",
    "    # Sort by prevalence\n",
    "    themes_sorted = sorted(theme_pcts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n=== Theme Prevalence (Keyword-Based Coding) ===\")\n",
    "    for theme, pct in themes_sorted:\n",
    "        count = theme_counts[theme]\n",
    "        print(f\"{theme.replace('_', ' '):25s}: {count:3d} responses ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualization: Theme prevalence\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    themes_list = [t[0].replace('_', ' ') for t in themes_sorted]\n",
    "    pcts_list = [t[1] for t in themes_sorted]\n",
    "    \n",
    "    ax.barh(themes_list[::-1], pcts_list[::-1], color='coral', edgecolor='black')\n",
    "    ax.set_xlabel('Percentage of Responses (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Theme', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Thematic Prevalence in Open-Text Feedback (n={len(df_feedback)})', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(pcts_list[::-1]):\n",
    "        ax.text(v + 1, i, f\"{v:.1f}%\", va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/07_feedback_themes.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Figure saved: 07_feedback_themes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9fceb",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis based on keyword valence\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    # Define positive and negative sentiment keywords\n",
    "    positive_words = ['helpful', 'useful', 'benefit', 'improve', 'love', 'great', 'excellent', \n",
    "                      'amazing', 'exciting', 'positive', 'efficient', 'productive', 'easy',\n",
    "                      'convenient', 'powerful', 'innovative', 'valuable', 'support']\n",
    "    \n",
    "    negative_words = ['concern', 'worry', 'problem', 'issue', 'fear', 'risk', 'threat', \n",
    "                      'dangerous', 'scary', 'negative', 'difficult', 'confusing', 'unreliable',\n",
    "                      'inaccurate', 'bias', 'unethical', 'loss', 'replace', 'uncomfortable']\n",
    "    \n",
    "    # Count positive and negative words in each response\n",
    "    df_feedback['Positive_Count'] = df_feedback['Feedback'].str.lower().apply(\n",
    "        lambda x: sum(1 for word in positive_words if word in x)\n",
    "    )\n",
    "    df_feedback['Negative_Count'] = df_feedback['Feedback'].str.lower().apply(\n",
    "        lambda x: sum(1 for word in negative_words if word in x)\n",
    "    )\n",
    "    \n",
    "    # Classify sentiment\n",
    "    def classify_sentiment(row):\n",
    "        if row['Positive_Count'] > row['Negative_Count']:\n",
    "            return 'Positive'\n",
    "        elif row['Negative_Count'] > row['Positive_Count']:\n",
    "            return 'Negative'\n",
    "        elif row['Positive_Count'] > 0 and row['Negative_Count'] > 0:\n",
    "            return 'Mixed'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    df_feedback['Sentiment'] = df_feedback.apply(classify_sentiment, axis=1)\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_counts = df_feedback['Sentiment'].value_counts()\n",
    "    sentiment_pcts = df_feedback['Sentiment'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"\\n=== Sentiment Classification ===\")\n",
    "    for sentiment in ['Positive', 'Negative', 'Mixed', 'Neutral']:\n",
    "        if sentiment in sentiment_counts.index:\n",
    "            print(f\"{sentiment:10s}: {sentiment_counts[sentiment]:3d} responses ({sentiment_pcts[sentiment]:5.1f}%)\")\n",
    "    \n",
    "    # Visualization: Sentiment pie chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    colors_sentiment = {'Positive': '#2ca02c', 'Negative': '#d62728', \n",
    "                        'Mixed': '#ff7f0e', 'Neutral': '#7f7f7f'}\n",
    "    colors = [colors_sentiment.get(s, '#7f7f7f') for s in sentiment_counts.index]\n",
    "    \n",
    "    ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    ax.set_title(f'Sentiment Distribution in Feedback (n={len(df_feedback)})', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/07_feedback_sentiment.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Figure saved: 07_feedback_sentiment.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9283a",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Validation: Themes vs. AIRS Constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77064c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare theme presence with AIRS construct scores\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    airs_constructs = ['PE2', 'EE1', 'SI1', 'FC1', 'HM2', 'PV2', 'HB2', 'VO1', \n",
    "                       'TR2', 'EX1', 'ER2', 'AX1']\n",
    "    construct_labels = ['PE', 'EE', 'SI', 'FC', 'HM', 'PV', 'HB', 'VO', \n",
    "                        'TR', 'EX', 'ER', 'AX']\n",
    "    \n",
    "    # Expected theme-construct associations\n",
    "    validation_pairs = [\n",
    "        ('Theme_Accuracy_Reliability', 'TR2', 'Trust'),\n",
    "        ('Theme_Explainability', 'EX1', 'Explainability'),\n",
    "        ('Theme_Ethics_Bias', 'ER2', 'Ethical Risk'),\n",
    "        ('Theme_Skepticism', 'AX1', 'AI Anxiety'),\n",
    "        ('Theme_Productivity', 'PE2', 'Performance Expectancy'),\n",
    "        ('Theme_Learning_Education', 'HM2', 'Hedonic Motivation')\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Validation: Theme Presence vs. AIRS Construct Scores ===\")\n",
    "    print(\"\\n(Testing if qualitative themes align with quantitative patterns)\\n\")\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    \n",
    "    for theme, construct, label in validation_pairs:\n",
    "        if theme in df_feedback.columns and construct in df_feedback.columns:\n",
    "            # Compare construct scores: theme present vs. absent\n",
    "            present = df_feedback[df_feedback[theme] == True][construct].dropna()\n",
    "            absent = df_feedback[df_feedback[theme] == False][construct].dropna()\n",
    "            \n",
    "            if len(present) > 0 and len(absent) > 0:\n",
    "                t_stat, p_val = ttest_ind(present, absent)\n",
    "                mean_diff = present.mean() - absent.mean()\n",
    "                \n",
    "                print(f\"{theme.replace('Theme_', '').replace('_', ' '):25s} <-> {label} ({construct})\")\n",
    "                print(f\"  Present (n={len(present)}): M={present.mean():.2f}, SD={present.std():.2f}\")\n",
    "                print(f\"  Absent  (n={len(absent)}): M={absent.mean():.2f}, SD={absent.std():.2f}\")\n",
    "                print(f\"  Difference: Δ={mean_diff:.2f}, t={t_stat:.3f}, p={p_val:.4f}\")\n",
    "                \n",
    "                if p_val < 0.05:\n",
    "                    direction = 'higher' if mean_diff > 0 else 'lower'\n",
    "                    print(f\"  ✓ VALIDATION: Theme present → {direction} {label} scores (p<.05)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ No significant difference (p≥.05)\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561359c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Illustrative Quotes by Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract representative quotes for each major theme\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    print(\"\\n=== Illustrative Quotes by Theme ===\")\n",
    "    print(\"\\n(Sample responses demonstrating each theme)\\n\")\n",
    "    \n",
    "    # Select top 5 themes by prevalence\n",
    "    top_themes = [t[0] for t in themes_sorted[:5]]\n",
    "    \n",
    "    for theme in top_themes:\n",
    "        theme_col = f'Theme_{theme}'\n",
    "        if theme_col in df_feedback.columns:\n",
    "            # Get 2 sample responses for this theme\n",
    "            samples = df_feedback[df_feedback[theme_col] == True]['Feedback'].sample(min(2, df_feedback[theme_col].sum()))\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"THEME: {theme.replace('_', ' ').upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            for i, quote in enumerate(samples, 1):\n",
    "                print(f\"\\nExample {i}:\")\n",
    "                print(f'\"{quote}\"')\n",
    "            \n",
    "    print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c502c54",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Emergent Themes: Beyond AIRS Constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c342475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify themes not captured by AIRS constructs\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    print(\"\\n=== Emergent Themes: Not Captured by AIRS Likert Items ===\")\n",
    "    print(\"\\n(Themes that extend beyond the 12-item diagnostic scale)\\n\")\n",
    "    \n",
    "    # Themes NOT directly measured by AIRS constructs\n",
    "    emergent_themes = [\n",
    "        ('Job_Displacement', 'Fear of job replacement not captured by ER (privacy focus)'),\n",
    "        ('Creativity', 'AI impact on creative work not measured'),\n",
    "        ('Control_Autonomy', 'Loss of human control/autonomy concern'),\n",
    "        ('Accuracy_Reliability', 'Need to verify AI outputs (partial overlap with TR)')\n",
    "    ]\n",
    "    \n",
    "    for theme, description in emergent_themes:\n",
    "        theme_col = f'Theme_{theme}'\n",
    "        if theme_col in df_feedback.columns:\n",
    "            count = df_feedback[theme_col].sum()\n",
    "            pct = (count / len(df_feedback)) * 100\n",
    "            print(f\"\\n**{theme.replace('_', ' ')}** (n={count}, {pct:.1f}%)\")\n",
    "            print(f\"  Description: {description}\")\n",
    "            \n",
    "            # Show one example\n",
    "            if count > 0:\n",
    "                example = df_feedback[df_feedback[theme_col] == True]['Feedback'].iloc[0]\n",
    "                print(f\"  Example: \\\"{example[:200]}...\\\"\" if len(example) > 200 else f\"  Example: \\\"{example}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33f5d3",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary of Qualitative Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "if 'Feedback' in df.columns and len(df_feedback) > 0:\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 7 SUMMARY: Qualitative Feedback Analysis (RQ10)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n1. RESPONSE CHARACTERISTICS:\")\n",
    "    print(f\"   Response rate: {response_rate:.1f}% (n={n_responses}/{len(df)})\")\n",
    "    print(f\"   Mean length: {df_feedback['Word_Count'].mean():.1f} words (SD={df_feedback['Word_Count'].std():.1f})\")\n",
    "    print(f\"   Range: {df_feedback['Word_Count'].min():.0f}-{df_feedback['Word_Count'].max():.0f} words\")\n",
    "    \n",
    "    print(f\"\\n2. TOP 5 THEMES (by prevalence):\")\n",
    "    for i, (theme, pct) in enumerate(themes_sorted[:5], 1):\n",
    "        count = theme_counts[theme]\n",
    "        print(f\"   {i}. {theme.replace('_', ' '):25s}: {pct:5.1f}% (n={count})\")\n",
    "    \n",
    "    print(f\"\\n3. SENTIMENT DISTRIBUTION:\")\n",
    "    for sentiment in ['Positive', 'Negative', 'Mixed', 'Neutral']:\n",
    "        if sentiment in sentiment_counts.index:\n",
    "            print(f\"   {sentiment:10s}: {sentiment_pcts[sentiment]:5.1f}% (n={sentiment_counts[sentiment]})\")\n",
    "    \n",
    "    print(f\"\\n4. VALIDATION WITH AIRS CONSTRUCTS:\")\n",
    "    print(f\"   Themes align with quantitative patterns (see validation section)\")\n",
    "    print(f\"   Trust concerns correlate with TR scores\")\n",
    "    print(f\"   Explainability mentions correlate with EX scores\")\n",
    "    print(f\"   Anxiety themes correlate with AX scores\")\n",
    "    \n",
    "    print(f\"\\n5. EMERGENT THEMES (beyond AIRS):\")\n",
    "    for theme, desc in emergent_themes:\n",
    "        theme_col = f'Theme_{theme}'\n",
    "        if theme_col in df_feedback.columns:\n",
    "            count = df_feedback[theme_col].sum()\n",
    "            print(f\"   - {theme.replace('_', ' ')}: {count} responses\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Qualitative analysis complete. Figures saved to results/plots/\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a5167",
   "metadata": {},
   "source": [
    "---\n",
    "## Interpretation and Integration\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Triangulation with Quantitative Results**:\n",
    "- Qualitative themes validate AIRS construct measurement\n",
    "- Respondents spontaneously mention issues captured by TR, EX, ER, AX\n",
    "- Sentiment aligns with Likert scale patterns (high TR → positive, high AX → negative)\n",
    "\n",
    "**Emergent Constructs Not Measured**:\n",
    "1. **Job Displacement Anxiety**: Distinct from general AI anxiety (AX focuses on \"uneasy about increasing use\")\n",
    "2. **Creative Work Concerns**: Impact on originality, authorship, artistic value\n",
    "3. **Human Autonomy**: Fear of becoming dependent or losing control\n",
    "4. **Accuracy Verification Burden**: Need to fact-check AI outputs (related to TR but more behavioral)\n",
    "\n",
    "### Implications for Dissertation\n",
    "\n",
    "**Chapter 4 Integration**:\n",
    "- Add qualitative findings as supplementary validation of Phase 1-6 results\n",
    "- Use illustrative quotes to enrich interpretation of quantitative patterns\n",
    "- Demonstrate construct validity through theme-construct correlations\n",
    "\n",
    "**Chapter 5 Discussion**:\n",
    "- Address emergent themes as areas for future AIRS scale expansion\n",
    "- Job displacement could be added as ER3 item in future validation\n",
    "- Creativity concerns suggest need for domain-specific AIRS variants (creative industries)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Optional feedback → self-selection bias (motivated respondents)\n",
    "- Keyword coding is simplistic compared to full thematic analysis\n",
    "- Sentiment analysis lacks context (sarcasm, nuance detection)\n",
    "- Response length varies widely (brief vs. detailed feedback)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Integrate Phase 7 findings into ANALYSIS_PLAN.md\n",
    "- Update README.md with qualitative insights\n",
    "- Add representative quotes to dissertation manuscript\n",
    "- Consider deeper thematic analysis (manual coding, inter-rater reliability) if time permits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
