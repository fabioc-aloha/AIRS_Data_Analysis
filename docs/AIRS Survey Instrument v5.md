Research Questions and Hypotheses
The study is guided by the need to understand how established technology adoption predictors and AI-specific constructs influence enterprise employees’ readiness to adopt AI tools. The following research questions and hypotheses are proposed.
Research Questions
RQ1. What psychological, motivational, and contextual factors influence individual readiness to adopt AI technologies within a large enterprise?
RQ2. To what extent do the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) constructs predict AI adoption readiness among enterprise employees?
RQ3. How do AI-specific constructs—trust in AI, explainability, perceived ethical risk, and AI-related anxiety—enhance the predictive validity of the AIRS framework?
RQ4. How do contextual moderators such as role, business unit, and AI tool usage frequency influence the relationships between predictors and AI adoption readiness?
Hypotheses
H1. UTAUT2 Core Constructs and AI Adoption
The core constructs of UTAUT2—performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit—will significantly predict AI adoption readiness among enterprise employees (Venkatesh et al., 2012).
H2. AIRS-Specific Constructs and AI Adoption
The AI-specific constructs of the AIRS framework—trust in AI, explainability, perceived ethical risk, and AI-related anxiety—will significantly predict AI adoption readiness, beyond the explanatory power of UTAUT2 (Langer et al., 2023; Shin, 2021).
H3. Integrated Predictive Validity of AIRS
The combined AIRS model (UTAUT2 plus AI-specific constructs) will explain greater variance in AI adoption readiness than UTAUT2 alone, supporting the theoretical value of extending technology acceptance models for enterprise AI contexts (Dwivedi et al., 2023).
H4. Moderating Effects of Contextual Variables
The relationships between predictors (UTAUT2 and AI-specific constructs) and AI adoption readiness will be moderated by contextual factors, including employee role, AI tool usage frequency, and business unit affiliation (Dwivedi et al., 2021).
See Appendix C for a table aligning survey question and hypothetical construct. 
Appendix A. AI Readiness Score (AIRS) Survey Instrument
Instructions to participants
Please indicate your level of agreement with each statement based on your current work or study experience.
Scale: 1 = Strongly disagree | 2 = Disagree | 3 = Neutral | 4 = Agree | 5 = Strongly agree
________________________________________
Section 1. UTAUT2 core constructs (Hypothesis H1)
Performance Expectancy (PE)
1.	AI tools help me accomplish tasks more quickly.
2.	Using AI improves the quality of my work or studies.
Effort Expectancy (EE)
5.	Learning to use AI tools is easy for me.
6.	Interacting with AI tools is clear and understandable.
Social Influence (SI)
9.	People whose opinions I value encourage me to use AI tools.
10.	Leaders in my organization or school support the use of AI tools.
Facilitating Conditions (FC)
13.	I have access to training or tutorials for the AI tools I use.
14.	The AI tools I use are compatible with other tools or systems I use.
Hedonic Motivation (HM)
17.	Using AI tools is stimulating and engaging.
18.	AI tools make my work or studies more interesting.
Price Value (PV)
21.	I get more value from AI tools than the effort they require.
22.	Using AI tools is worth the learning curve.
Habit (HB)
25.	Using AI tools has become a habit for me.
26.	I tend to rely on AI tools by default when I need help with tasks.
Voluntariness (VO)
1.	I choose to use AI tools in my work because I find them helpful, not because I am required to.
2.	I could choose not to use AI tools in my work or studies if I preferred.
________________________________________
Section 2. AIRS AI-specific constructs (Hypothesis H2)
Trust in AI (TR)
29.	I trust AI tools to provide reliable information.
30.	I trust the AI tools that are available to me.
Explainability (EX)
33.	I understand how the AI tools I use generate their outputs.
34.	I prefer AI tools that explain their recommendations.
Perceived Ethical Risk (ER)
37.	I worry that AI tools could replace jobs in my field.
38.	I am concerned about privacy risks when using AI tools.
AI Anxiety (AX)
41.	I feel uneasy about the increasing use of AI.
42.	I worry that I may be left behind if I do not keep up with AI.
________________________________________
Section 3. AI adoption readiness (outcome) (Hypothesis H3)
45.	I am ready to use more AI tools in my work or studies.
46.	I would recommend AI tools to others.
47.	I see AI as an important part of my future.
48.	I plan to increase my use of AI tools in the next six months.
________________________________________
 
Section 4. AI tool usage and demographics (Hypothesis H4)
Usage frequency
Scale: 1 = Never | 2 = Rarely | 3 = Sometimes | 4 = Often | 5 = Daily
50. How often do you use Microsoft 365 Copilot or Microsoft Copilot in your work or studies?
51. How often do you use ChatGPT?
52. How often do you use Google Gemini?
53. How often do you use other AI tools (for example, Claude, Perplexity, Jasper)?
Demographics
54.	What is your highest level of education completed?
• High school or less
• Some college or vocational training
• Bachelor’s degree
• Master’s degree
• Doctoral or professional degree
55.	What is your current status?
• Full-time student
• Part-time student
• Employed — individual contributor
• Employed — manager
• Employed — executive or leader
• Freelancer or self-employed
• Not currently employed
• Other
56.	Which industry or field best describes your primary area of work or study?
• Technology or IT
• Education
• Healthcare
• Finance or Banking
• Manufacturing
• Retail or Hospitality
• Government or Public sector
• Nonprofit
• Other
57.	How many years of work or study experience do you have in your field?
• Less than 1 year
• 1 to 3 years
• 4 to 6 years
• 7 to 10 years
• 11 or more years
58.	Do you identify as a person with a disability (for example, vision, mobility, neurodivergence)?
• Yes
• No
• Prefer not to answer
________________________________________
Notes for scoring and analysis
•	All items use the 1 to 5 agreement scale except the usage items, which use the 1 to 5 frequency scale.
•	Before modeling, screen items with EFA and confirm structure with CFA; compute construct scores as means of retained items.
•	For moderation tests, use usage frequency items and relevant demographic variables as moderators in hierarchical or multi-group models.
 
Appendix B. AIRS Survey Instrument — Centiment Import Format
This appendix provides the AI Readiness Score questionnaire in Centiment import format. Agreement items are grouped by construct into matrix blocks that share the same five-point scale: Strongly disagree, Disagree, Neutral, Agree, Strongly agree. Usage items use a five-point frequency scale: Never, Rarely, Sometimes, Often, Daily. Demographics use single select blocks. One item in Perceived Ethical Risk is reverse-coded and is marked with (R). Maintain item order for factor analysis and scoring.
PREVIEW LINK: https://app.centiment.co/preview/471bc02e-aeac-4ae3-9fb9-b58a4a12c573
 
Appendix C. AIRS Survey Items and Literature-Based Item Comparison
Purpose. This appendix documents how each AI Readiness Score (AIRS) item maps to prior literature. Items are labeled as: Direct (UTAUT/UTAUT2) when the canonical wording is preserved; Adapted (UTAUT/UTAUT2) when context or phrasing is tailored (for example, time/effort substituted for monetary cost); and Adapted (AI literature) when items were crafted from conceptual or review scholarship on trust, explainability, ethical risk, and AI anxiety. Item 40 is reverse-coded.
#	Hypothesis	Hypothetical Construct	AIRS item (final wording)	Provenance & original wording
1	H1	Performance Expectancy	AI tools help me accomplish tasks more quickly.	Direct (UTAUT) — “I find the system useful in my job.” (Venkatesh et al., 2003)
2	H1	Performance Expectancy	Using AI improves the quality of my work or studies.	Direct (UTAUT) — “Using [system] improves my job performance.” (Venkatesh et al., 2003)
3	H1	Performance Expectancy	AI tools enhance my productivity.	Direct (UTAUT) — “Using [system] increases my productivity.” (Venkatesh et al., 2003)
4	H1	Performance Expectancy	AI tools make me more effective on complex tasks.	Direct (UTAUT) — “Using [system] enhances my effectiveness on the job.” (Venkatesh et al., 2003)
5	H1	Effort Expectancy	Learning to use AI tools is easy for me.	Direct (UTAUT) — “Learning to operate the system is easy for me.” (Venkatesh et al., 2003)
6	H1	Effort Expectancy	Interacting with AI tools is clear and understandable.	Direct (UTAUT) — “My interaction with the system is clear and understandable.” (Venkatesh et al., 2003)
7	H1	Effort Expectancy	It takes little effort for me to become skillful at using AI tools.	Direct (UTAUT) — “It would be easy for me to become skillful at using the system.” (Venkatesh et al., 2003)
8	H1	Effort Expectancy	I can quickly learn how to use new AI tools.	Direct (TAM) — “I find the system easy to use.” (Davis, 1989)
9	H1	Social Influence	People who influence my work or studies think I should use AI tools.	Direct (UTAUT) — “People who influence my behavior think that I should use the system.” (Venkatesh et al., 2003)
10	H1	Social Influence	People whose opinions I value encourage me to use AI tools.	Direct (UTAUT) — “People whose opinions I value prefer that I use the system.” (Venkatesh et al., 2003)
11	H1	Social Influence	Leaders in my organization or school support the use of AI tools.	Adapted (UTAUT) — reflects organizational leadership support wording aligned with UTAUT SI domain.
12	H1	Social Influence	Using AI tools is viewed as normal in my organization or school.	Adapted (UTAUT) — rephrased injunctive/descriptive norm; corrects prior misattribution to Image (Moore & Benbasat, 1991).
13	H1	Facilitating Conditions	I have the resources I need to use AI tools effectively.	Direct (UTAUT) — “I have the resources necessary to use the system.” (Venkatesh et al., 2003)
14	H1	Facilitating Conditions	A specific person or group is available to assist me when I have difficulties using AI tools.	Direct (UTAUT) — “A specific person (or group) is available for assistance with system difficulties.” (Venkatesh et al., 2003)
15	H1	Facilitating Conditions	I have access to training or tutorials for the AI tools I use.	Adapted (UTAUT) — aligns with FC support knowledge/training dimension.
16	H1	Facilitating Conditions	The AI tools I use are compatible with other tools or systems I use.	Direct (UTAUT) — “The system is compatible with other systems I use.” (Venkatesh et al., 2003)
17	H1	Hedonic Motivation	I enjoy using AI tools.	Direct (UTAUT2) — “Using the system is fun.” (Venkatesh et al., 2012)
18	H1	Hedonic Motivation	Using AI tools is stimulating and engaging.	Direct (UTAUT2) — “Using the system is enjoyable.” (Venkatesh et al., 2012)
19	H1	Hedonic Motivation	AI tools make my work or studies more interesting.	Direct (UTAUT2) — “Using the system is very entertaining.” (Venkatesh et al., 2012)
20	H1	Hedonic Motivation	I feel satisfied when I successfully use AI tools.	Adapted (UTAUT2) — intrinsic enjoyment satisfaction phrasing.
21	H1	Price Value	The benefits of using AI tools are worth the time it takes to learn them.	Adapted (UTAUT2) — substitutes time/effort for monetary cost in organizational/education context.
22	H1	Price Value	Investing my time in learning AI tools is worthwhile.	Adapted (UTAUT2) — as above.
23	H1	Price Value	I get more value from AI tools than the effort they require.	Adapted (UTAUT2) — as above.
24	H1	Price Value	Using AI tools is worth the learning curve.	Adapted (UTAUT2) — as above.
25	H1	Habit	Using AI tools has become a habit for me.	Direct (UTAUT2) — “The use of the system has become a habit for me.” (Venkatesh et al., 2012)
26	H1	Habit	I use AI tools without having to think about it.	Adapted (UTAUT2) — neutral habit phrasing (avoids “addicted”).
27	H1	Habit	I tend to rely on AI tools by default when I need help with tasks.	Adapted (UTAUT2) — reliance default phrasing.
28	H1	Habit	Using AI tools has become natural to me.	Direct (UTAUT2) — “Using the system has become natural to me.” (Venkatesh et al., 2012)
29	H2	Trust in AI	I trust AI tools to provide reliable information.	Adapted (AI literature) — trust in algorithmic outputs (e.g., Siau & Wang, 2018; Langer et al., 2023).
30	H2	Trust in AI	I believe AI tools can make unbiased decisions.	Adapted (AI literature) — unbiasedness as trust antecedent (e.g., Langer et al., 2023).
31	H2	Trust in AI	I feel confident relying on AI outputs.	Adapted (AI literature) — confidence in outputs (e.g., Shin, 2021).
32	H2	Trust in AI	I trust the AI tools that are available to me.	Adapted (AI literature) — context-specific trust.
33	H2	Explainability	I understand how the AI tools I use generate their outputs.	Adapted (AI literature) — XAI understanding (Doshi Velez & Kim, 2017).
34	H2	Explainability	The reasoning behind AI recommendations is clear to me.	Adapted (AI literature) — transparency/clarity (Shin, 2021).
35	H2	Explainability	I prefer AI tools that explain their recommendations.	Adapted (AI literature) — user preference for explanations (Guidotti et al., 2018).
36	H2	Explainability	When I understand how an AI tool works, I am more likely to use it.	Adapted (AI literature) — explanation–adoption link.
37	H2	Perceived Ethical Risk	I worry that AI tools could replace jobs in my field.	Adapted (AI literature) — job displacement risk (e.g., Floridi et al., 2018).
38	H2	Perceived Ethical Risk	I am concerned about privacy risks when using AI tools.	Adapted (AI literature) — privacy/data governance concerns.
39	H2	Perceived Ethical Risk	I believe AI tools could introduce bias into important decisions.	Adapted (AI literature) — fairness bias risk (e.g., Weidinger et al., 2022).
40	H2	Perceived Ethical Risk	Organizations and schools generally manage AI risks responsibly. (R)	Adapted (AI literature) — governance responsibility; reverse coded for ER.
41	H2	AI Anxiety	I feel uneasy about the increasing use of AI.	Adapted (AI literature) — AI anxiety affect (Bendel, 2021).
42	H2	AI Anxiety	I worry that I may be left behind if I do not keep up with AI.	Adapted (AI literature) — obsolescence anxiety.
43	H2	AI Anxiety	I sometimes feel uncomfortable relying on AI tools.	Adapted (AI literature) — discomfort reliance (related to trust literature).
44	H2	AI Anxiety	The speed at which AI is evolving makes me anxious.	Adapted (AI literature) — pace-of-change anxiety.
45	H2	AI Adoption Readiness	I am ready to use more AI tools in my work or studies.	Adapted (UTAUT) — from Behavioral Intention/usage readiness (Venkatesh et al., 2003).
46	H2	AI Adoption Readiness	I am open to learning about new AI technologies.	Adapted (UTAUT) — intention to use/learn framing.
47	H2	AI Adoption Readiness	I would recommend AI tools to others.	Adapted (TAM extensions) — recommendation intention proxy.
48	H2	AI Adoption Readiness	I see AI as an important part of my future.	Adapted (UTAUT) — forward-looking intention/value.
49	H2	AI Adoption Readiness	I plan to increase my use of AI tools in the next six months.	Adapted (UTAUT) — time-bounded behavioral intention.

Notes
•	Item 40 is reverse coded for ER scoring.
•	PV items explicitly operationalize “price” as time/effort to fit education and organizational contexts.
•	TR, EX, ER, and AX items derive from conceptual/review literature and will be empirically screened via EFA and confirmed via CFA before inclusion in the structural model.
Reference anchors (for provenance labels)
Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly.
Moore, G. C., & Benbasat, I. (1991). Development of an instrument to measure the perceptions of adopting an information technology innovation. Information Systems Research.
Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly.
Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. MIS Quarterly.
Bendel, O. (2021). AI anxiety: Ethical, psychological and social aspects of artificial intelligence. AI & Society.
Doshi Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. (XAI overview).
Guidotti, R., et al. (2018). A survey of methods for explaining black box models. ACM Computing Surveys.
Langer, M., König, C. J., & Papathanasiou, M. (2023). Trust in artificial intelligence: A review of empirical research. Journal of Business Research.
Shin, D. (2021). The effects of explainability and causability on trust in AI. (Representative work in HCI/IS on XAI–trust links).
Floridi, L., et al. (2018). AI4People—An ethical framework for a good AI society. Minds and Machines.
Weidinger, L., et al. (2022). Taxonomy of risks posed by language models. (Risk taxonomy).
