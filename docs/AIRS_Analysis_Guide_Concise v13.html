<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIRS Analysis Guide - EFA, CFA, SEM (SPSS & AMOS)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 4px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 10px;
        }
        
        h2 {
            color: #2c3e50;
            background: #3498db;
            color: white;
            padding: 10px 15px;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        h3 {
            color: #2c3e50;
            border-left: 4px solid #3498db;
            padding-left: 10px;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        
        h4 {
            color: #34495e;
            margin-top: 15px;
            margin-bottom: 8px;
        }
        
        .subtitle {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 5px;
        }
        
        .author {
            color: #95a5a6;
            font-style: italic;
            margin-bottom: 30px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.95em;
        }
        
        th {
            background: #34495e;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 10px;
            border: 1px solid #ddd;
        }
        
        tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: #e74c3c;
        }
        
        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
            font-size: 0.9em;
        }
        
        .box {
            background: #ecf0f1;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 15px 0;
        }
        
        .box.warning {
            border-left-color: #e74c3c;
            background: #fef5f5;
        }
        
        .box.success {
            border-left-color: #27ae60;
            background: #f0fdf4;
        }
        
        .box strong {
            color: #2c3e50;
        }
        
        ul, ol {
            margin-left: 25px;
            margin-bottom: 15px;
        }
        
        li {
            margin-bottom: 5px;
        }
        
        .timeline {
            background: linear-gradient(to right, #3498db, #2ecc71);
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        
        .timeline strong {
            font-size: 1.1em;
        }
        
        .no-print {
            text-align: center;
            margin: 30px 0;
        }
        
        .btn {
            background: #3498db;
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 5px;
            font-size: 1em;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin: 5px;
        }
        
        .btn:hover {
            background: #2980b9;
        }
        
        .btn.secondary {
            background: #95a5a6;
        }
        
        .btn.secondary:hover {
            background: #7f8c8d;
        }
        
        .references {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #3498db;
        }
        
        .references h2 {
            background: #2c3e50;
        }
        
        .reference-item {
            text-indent: -2em;
            padding-left: 2em;
            margin-bottom: 10px;
        }
        
        @media print {
            .no-print {
                display: none;
            }
            body {
                background: white;
                padding: 0;
            }
            .container {
                box-shadow: none;
                padding: 20px;
            }
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            table {
                font-size: 0.85em;
            }
            th, td {
                padding: 8px;
            }
        }
    </style>

<style id="style-refinement">
  /* Typography reset to avoid global bold */
  body, p, li, td, th {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif;
    font-weight: 400 !important;
    color: #2c3e50;
    line-height: 1.6;
  }
  strong, b {
    font-weight: 600 !important; /* Subtle emphasis */
  }
  h1, h2, h3, h4 {
    font-weight: 700 !important;
    color: #1f2d3d;
    letter-spacing: 0.2px;
  }
  h1 { font-size: 1.9rem; margin-top: 0.2rem; }
  h2 { font-size: 1.4rem; margin-top: 1.6rem; border-bottom: 2px solid #e6eef5; padding-bottom: 6px; }
  h3 { font-size: 1.15rem; margin-top: 1.1rem; color: #2b3a67; }
  .subtitle, .author { font-weight: 500 !important; color: #52667a; }
  /* Container polish */
  .container {
    max-width: 960px;
    margin: 24px auto;
    padding: 28px;
    background: #ffffff;
    border-radius: 14px;
    box-shadow: 0 6px 22px rgba(22,34,51,0.08);
  }
  /* Tables */
  table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    margin: 14px 0 22px;
    font-size: 0.96rem;
  }
  thead th {
    text-align: left;
    background: #f5f9ff;
    color: #2b3a67;
    font-weight: 600 !important;
    border-bottom: 2px solid #e6eef5;
    padding: 10px 12px;
  }
  tbody td {
    border-bottom: 1px solid #eef2f7;
    padding: 10px 12px;
  }
  /* Boxes */
  .box, .box.success, .box.warning {
    border-radius: 12px;
    padding: 14px 16px;
    margin: 14px 0;
    border: 1px solid #e8edf4;
    background: #fbfdff;
  }
  .box.success { background: #f6fffa; border-color: #d5f2e3; }
  .box.warning { background: #fffaf3; border-color: #f5e3c2; }
  /* Code blocks */
  pre, code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Liberation Mono", "Courier New", monospace;
    background: #f8fafc;
    color: #243b53;
    border-radius: 10px;
  }
  pre { padding: 10px 12px; overflow-x: auto; border: 1px solid #edf2f7; }
  code { padding: 2px 5px; }
  /* Lists */
  ul, ol { margin: 0 0 14px 22px; }
  li { margin: 4px 0; }
  /* Links and buttons */
  a { color: #2c7be5; text-decoration: none; }
  a:hover { text-decoration: underline; }
  .btn, .btn.secondary {
    font-weight: 600 !important;
    border-radius: 10px;
  }
  /* Reference items */
  .references h2 {
    background: transparent;
    border-bottom: 2px solid #e6eef5;
    color: #1f2d3d;
    padding-bottom: 6px;
  }
  .reference-item {
    text-indent: 0;
    padding-left: 0;
    margin-bottom: 8px;
  }
  /* Print */
  @media print {
    .container { box-shadow: none; border-radius: 0; }
  }
</style>


<!-- Mermaid support -->
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'loose' });
</script>

</head>
<body>
    <div class="container">
        <h1>AIRS Analysis Guide: EFA, CFA, and SEM</h1>
<p><em>Theorized mechanism.</em> Test the indirect path <strong>Explainability → Trust → AI Readiness</strong> using bias‑corrected bootstrap (5,000 resamples); a 95% confidence interval excluding zero indicates a significant indirect effect excludes zero; report total, direct, and indirect effects.</p>

<p><em>Discriminant validity check.</em> In addition to Fornell–Larcker, report the <strong>Report the Heterotrait–Monotrait (HTMT) ratio alongside Fornell–Larcker. Use < .85 (strict) or < .90 (lenient) thresholds; where feasible, include bootstrap 95% confidence intervals.

<p><em>EFA estimator note.</em> Because AIRS items use 5‑point Likert scales, compute the factor solution from a <strong>polychoric correlation</strong> matrix (PAF or MINRES; oblique rotation). Retention rules: KMO ≥ .60; Bartlett’s p &lt; .05; primary loading ≥ .50 with ≥ .30 cross‑loading separation.</p>

        <div class="subtitle">Evidence-Based Procedures for SPSS & AMOS</div>
        <div class="author">Fabio Correa | Touro University | October 2025</div>
        
        <h2>Introduction: Overview of the AIRS Validation Plan</h2>
<section id="illustrations" style="margin-top:2rem;">
  <h2>Illustrations and Process Maps</h2>
  <p>The following visuals summarize the AIRS workflow and core concepts to orient readers before diving into procedures.</p>

  <div class="box">
    <h3>Overall Research Workflow</h3>
    <p class="caption">From instrument and data collection through analysis and reporting.</p>
    <pre class="mermaid">
flowchart TD
    A["Instrument design & pretest"] --> B["Data collection"]
    B --> C["Data screening: missing data, outliers, factorability"]
    C --> D["EFA (polychoric)"]
    D --> E["CFA fit & validity: CR, AVE, alpha, omega, HTMT"]
    E --> F["Measurement invariance: Configural -> Metric -> Scalar"]
    F --> G["SEM: UTAUT2 baseline (H1)"]
    G --> H["SEM: AIRS extended (H2–H3)"]
    H --> I["Mediation test: EX -> TR -> BI"]
    I --> J["Moderation (H4): multi-group"]
    J --> K["Interpretation & APA reporting"]


    </pre>
  </div>

  <div class="box">
    <h3>Construct Relationship Map</h3>
    <p class="caption">Conceptual (non-SEM) view of how constructs relate to AI Adoption Readiness.</p>
    <pre class="mermaid">
graph LR
  subgraph UTAUT2
    PE[Performance Expectancy]
    EE[Effort Expectancy]
    SI[Social Influence]
    FC[Facilitating Conditions]
    HM[Hedonic Motivation]
    PV[Price Value]
    HB[Habit]
  end

  subgraph AI-specific
    EX[Explainability]
    TR[Trust in AI]
    ER[Ethical Risk]
    AX[Anxiety]
  end

  BI[AI Adoption Readiness]

  %% Positive drivers
  PE --> BI
  EE --> BI
  SI --> BI
  FC --> BI
  HM --> BI
  PV --> BI
  HB --> BI
  TR --> BI
  EX --> TR
  EX ----> BI

  %% Inhibitors (visualized with dotted edges)
  ER -.-> BI
  AX -.-> BI

  %% Moderators as context
  Role((Role)) -. context .- BI
  Usage((AI Usage Freq.)) -. context .- BI
    </pre>
  </div>

  <div class="box">
    <h3>Data Validation and Reliability Workflow</h3>
    <p class="caption">Key diagnostic gates and decisions before moving to SEM.</p>
    <pre class="mermaid">
flowchart TD
    classDef warn fill:#fff3cd,stroke:#f0c36d,color:#7a5b00;

    A["Initial screening"] --> B["KMO >= .60 and Bartlett p < .05?"]
    B -->|Yes| C["EFA (polychoric) with oblique rotation"]
    B -->|No| X["Revise items / sample"]
    class X warn;

    C --> D["Retain items: loading >= .50 and >= .30 gap"]
    D --> E["CFA: CFI/TLI >= .90 (prefer >= .95), RMSEA <= .06 good (<= .08 adequate), SRMR <= .08"]
    E --> F["Convergent validity: AVE >= .50; Reliability: CR >= .70 (>= .60 short scales), omega >= .70"]
    F --> G["Discriminant validity: Fornell–Larcker + HTMT < .85/.90"]
    G --> H["Invariance (optional): Configural -> Metric -> Scalar"]
    H --> I["Proceed to SEM: H1–H4"]


  classDef warn fill:#fff3cd,stroke:#f0c36d,color:#7a5b00;
    </pre>
  </div>
</section>


        <p>This guide provides step-by-step procedures for validating the <strong>Artificial Intelligence Readiness Score (AIRS)</strong> framework—a theoretically grounded extension of UTAUT2 that integrates AI-specific constructs to assess individual readiness for AI adoption in organizational settings.</p>

        <h3>Research Objective</h3>
        <p>The AIRS framework extends the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) by retaining its seven core constructs (Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Conditions, Hedonic Motivation, Price Value, and Habit) while adding four AI-specific constructs that function as enablers (Trust in AI, Perceived Explainability) and inhibitors (Perceived Ethical Risk, AI-Related Anxiety) (Venkatesh et al., 2012).</p>

        
<h3>Research Questions</h3>
<p><strong>RQ1:</strong> What psychological, motivational, and contextual factors influence individual readiness to adopt AI technologies in organizational settings?</p>
<p><strong>RQ2:</strong> To what extent do UTAUT2 constructs (Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Conditions, Hedonic Motivation, Price Value, and Habit) predict AI adoption readiness among students and professionals?</p>
<h3>
<h3>Hypotheses</h3>
<p><strong>H1.</strong> The core constructs of UTAUT2—performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit—will significantly predict AI adoption readiness (Venkatesh et al., 2012).</p>
<p><strong>H2.</strong> The AI-specific constructs of the AIRS framework—trust in AI, explainability, perceived ethical risk, and AI-related anxiety—will significantly predict AI adoption readiness beyond the explanatory power of UTAUT2.</p>
<p><strong>H3.</strong> The combined AIRS model (UTAUT2 plus AI-specific constructs) will explain greater variance in AI adoption readiness than UTAUT2 alone.</p>
<p><strong>H4.</strong> The relationships between predictors (UTAUT2 and AI-specific constructs) and AI adoption readiness will be moderated by contextual factors, including role and AI tool usage frequency.</p>
<h3><h3>Sample & Design</h3>
        <ul>
            <li><strong>Sample:</strong> Approximately 500 U.S. students and professionals recruited through a professional panel provider</li>
            <li><strong>Design:</strong> Quantitative cross-sectional survey with split-sample validation strategy</li>
            <li><strong>Items:</strong> 30 total items (26 predictor items with 2 items per construct, plus 4 outcome items)</li>
            <li><strong>Constructs:</strong> 11 predictor factors (7 UTAUT2 + 4 AI-specific) + 1 outcome (AI Adoption Readiness)</li>
            <li><strong>Scale:</strong> Five-point Likert scale from Strongly Disagree to Strongly Agree for agreement items; Never to Daily for usage frequency items</li>
        </ul>

        <h3>Software Requirements</h3>
        <ul>
            <li><strong>IBM SPSS Statistics:</strong> For data screening and Exploratory Factor Analysis (EFA)</li>
            <li><strong>IBM SPSS AMOS:</strong> For Confirmatory Factor Analysis (CFA) and Structural Equation Modeling (SEM)</li>
        </ul>

        <h3>The 5-Step Validation Process</h3>

        <div class="box">
            <p><strong>Step 1: Data Screening (SPSS)</strong><br>
            Clean data, assess missing values, detect multivariate outliers, verify factorability, and split sample 50/50 for development (N≈250) and validation (N≈250) (Dillman et al., 2014).</p>

            <p><strong>Step 2: Exploratory Factor Analysis (SPSS)</strong><br>
            Use development sample to empirically validate the 11-factor structure. Retain items with primary loadings ≥.50 and minimal cross-loadings (Hinkin, 1998; DeVellis, 2017). Calculate reliability using Cronbach's alpha, accepting α ≥.60 for 2-item constructs.</p>

            <p><strong>Step 3: Confirmatory Factor Analysis (AMOS)</strong><br>
            Use validation sample to confirm the measurement model. Target model fit: CFI/TLI ≥ .90 (acceptable) and preferably ≥ .95 (good), RMSEA ≤ .06 (good) or ≤ .08 (adequate) (Hu & Bentler, 1999). Establish reliability (CR ≥.60 for 2-item constructs), convergent validity (AVE ≥ .50 (convergent validity met)), and discriminant validity (construct correlations <.85) (DeVellis, 2017).</p>

            <p><strong>Step 4: Measurement Invariance (AMOS)</strong><br>
            Test measurement equivalence across groups (students vs. professionals; high vs. low AI usage) using multi-group CFA. Require ΔCFI ≤.010 and ΔRMSEA ≤.015 for metric invariance (Venkatesh et al., 2012).</p>

            <p><strong>Step 5: Structural Equation Modeling (AMOS)</strong><br>
            Test hypotheses using full sample (N=500). Compare UTAUT2 baseline model with AIRS extended model. Evaluate direct effects, incremental R² improvement, nested model comparison (Δχ²), and moderation by role and AI usage frequency (Venkatesh et al., 2012; Dwivedi et al., 2021).</p>
        </div>

        <h3>Expected Outcomes</h3>
        <ul>
            <li>Validated 11-factor measurement model with acceptable-to-excellent fit indices</li>
            <li>Evidence that AI-specific constructs incrementally improve prediction beyond UTAUT2 alone</li>
            <li>Understanding of how Trust, Explainability, Ethical Risk, and Anxiety influence AI adoption readiness</li>
            <li>Identification of moderating effects by role and usage frequency</li>
            <li>Practical readiness instrument for organizations to assess AI adoption barriers and enablers</li>
        </ul>

        <h3>Critical Success Factors</h3>
        <div class="box warning">
            <strong>Key Requirements:</strong>
            <ul>
                <li><strong>Split-sample validation:</strong> Never run EFA and CFA on the same sample to avoid capitalization on chance (Hinkin, 1998)</li>
                <li><strong>Oblique rotation:</strong> Use Promax rotation in EFA—AIRS constructs are theoretically correlated (Comrey & Lee, 1992)</li>
                <li><strong>2-item construct constraints:</strong> Each predictor has exactly 2 items; cannot drop items without losing entire construct</li>
                <li><strong>Adjusted reliability thresholds:</strong> Accept α ≥.60 and CR ≥.60 for 2-item scales (lower than typical .70 threshold due to fewer items)</li>
                <li><strong>Discriminant validity monitoring:</strong> Watch Trust-Explainability correlation (target r = .60-.75, must be <.85) (Shin, 2021; Langer et al., 2023)</li>
                <li><strong>Measurement model first:</strong> Fully validate measurement model before testing structural paths</li>
            </ul>
        </div>

        <div class="timeline">
            <strong>Your 5-Step Analysis Plan</strong>
        </div>

        <table>
            <thead>
                <tr>
                    <th>Step</th>
                    <th>Task</th>
                    <th>Software</th>
                    <th>Sample</th>
                    <th>Key Output</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>1</strong></td>
                    <td>Data screening</td>
                    <td>SPSS</td>
                    <td>500</td>
                    <td>Clean dataset, 50/50 split</td>
                </tr>
                <tr>
                    <td><strong>2</strong></td>
                    <td>EFA</td>
                    <td>SPSS</td>
                    <td>250 (dev)</td>
                    <td>11 factors validated, α calculated</td>
                </tr>
                <tr>
                    <td><strong>3</strong></td>
                    <td>CFA</td>
                    <td>AMOS</td>
                    <td>250 (val)</td>
                    <td>Measurement model confirmed</td>
                </tr>
                <tr>
                    <td><strong>4</strong></td>
                    <td>Invariance</td>
                    <td>AMOS</td>
                    <td>500 (groups)</td>
                    <td>Metric and scalar invariance (if used)</td>
                </tr>
                <tr>
                    <td><strong>5</strong></td>
                    <td>SEM</td>
                    <td>AMOS</td>
                    <td>500 (full)</td>
                    <td>H1–H4 tested, ΔR² reported</td>
                </tr>
            </tbody>
        </table>

        <h2>Section 1: Data Screening (SPSS)</h2>

        <h3>1.1 Import and Verify Data</h3>
        <pre>File → Open → Data → Select your .csv or .xlsx file
Save As → AIRS_Full.sav</pre>

        <p><strong>Initial descriptive check:</strong></p>
        <pre>Analyze → Descriptive Statistics → Frequencies
  Variables: All 30 items (PE1, PE2, EE5, EE6, SI9, SI10, FC13, FC14, HM17, 
             HM18, PV21, PV22, HB25, HB26, TR29, TR30, EX33, EX34, ER37, 
             ER38, AX41, AX42, BI45, BI46, BI47, BI48)
  Statistics: Mean, Std. Deviation, Minimum, Maximum
  OK</pre>

        <div class="box">
            <strong>Verify expected ranges:</strong>
            <ul>
                <li>All values should be 1-5 (five-point Likert scale)</li>
                <li>Mean should be between 2.0-4.5</li>
                <li>Standard Deviation should be > 0.80</li>
                <li>No out-of-range values</li>
            </ul>
        </div>

        <h3>1.2 Missing Data Analysis</h3>
        <pre>Analyze → Missing Value Analysis
  Variables: All 30 items
  Patterns: Check "Tabulated cases"
  OK</pre>

        <div class="box success">
            <strong>Decision rules:</strong>
            <ul>
                <li>If < 5% missing per item: Use listwise deletion (automatic in Factor Analysis)</li>
                <li>If > 5% missing: Consider Full Information Maximum Likelihood (FIML) in AMOS</li>
                <li>Document all missing data patterns in audit log (Dillman et al., 2014)</li>
            </ul>
        </div>

        <h3>1.3 Multivariate Outlier Detection</h3>
        <p><strong>Calculate Mahalanobis distance:</strong></p>
        <pre>Analyze → Regression → Linear
  Dependent: ID (case identifier)
  Independent: All 30 items
  Save: Check "Mahalanobis"
  OK</pre>

        <p>This creates a variable named MAH_1.</p>

        <p><strong>Flag extreme outliers (threshold = 59.7 for 30 variables at p < .001):</strong></p>
        <pre>Transform → Compute Variable
  Target Variable: Outlier
  Numeric Expression: MAH_1 > 59.7
  OK</pre>

        <p><strong>Note:</strong> χ² critical value for 30 df at p < .001 is 59.70. Remove cases where Outlier = 1 only if they represent < 1% of the sample to preserve statistical power.</p>

        <h3>1.4 Assess Factorability</h3>
        <pre>Analyze → Dimension Reduction → Factor
  Variables: All 30 items
  Descriptives: Check "KMO and Bartlett's test of sphericity"
  OK</pre>

        <div class="box">
            <strong>Required thresholds for EFA (Watkins, 2018):</strong>
            <ul>
                <li>KMO (Kaiser-Meyer-Olkin) ≥ .80 (values ≥.90 are excellent)</li>
                <li>Bartlett's Test of Sphericity: p < .001 (rejects hypothesis that correlation matrix is an identity matrix)</li>
                <li>If KMO < .50, sample is not suitable for factor analysis</li>
            </ul>
        </div>

        <h3>1.5 Split Sample for Cross-Validation</h3>
        <pre>Transform → Compute Variable
  Target Variable: RandomSplit
  Numeric Expression: RV.UNIFORM(0,1) < 0.5
  OK

Data → Split File → Organize output by groups
  Groups Based on: RandomSplit
  OK</pre>

        <div class="box success">
            <strong>Save two separate files:</strong>
            <ul>
                <li>Filter: RandomSplit = 1 → Save As → <code>AIRS_Development.sav</code> (N≈250, for EFA)</li>
                <li>Filter: RandomSplit = 0 → Save As → <code>AIRS_Validation.sav</code> (N≈250, for CFA)</li>
            </ul>
            <p><strong>Critical:</strong> This split-sample approach prevents overfitting and ensures independent validation of the factor structure (Hinkin, 1998).</p>
        </div>

        <p><strong>Step 1 Complete.</strong> Proceed to Step 2 using AIRS_Development.sav.</p>

        <h2>Section 2: Exploratory Factor Analysis (SPSS)</h2>

        <p><strong>Use AIRS_Development.sav (N≈250)</strong></p>

        <h3>2.1 Determine Number of Factors</h3>
        <pre>Analyze → Dimension Reduction → Factor
  Variables: All 30 items
  Descriptives: Check "Initial solution" and "KMO and Bartlett's test"
  Extraction: Method = Principal axis factoring
             Check "Scree plot"
  OK</pre>

        <p><strong>Examine scree plot:</strong> Look for "elbow" where eigenvalues level off. Expect 11-12 factors based on theory (11 predictor constructs with 2 items each + 1 outcome construct with 4 items). Compare with Kaiser's criterion (eigenvalues > 1.0).</p>

        <h3>2.2 Run EFA with Promax Rotation</h3>
        <pre>Analyze → Dimension Reduction → Factor
  Variables: All 30 items
  
  Descriptives:
    Check "KMO and Bartlett's test of sphericity"
  
  Extraction:
    Method: Principal axis factoring
    Extract: Fixed number of factors = 12
    Display: Unrotated factor solution
  
  Rotation:
    Method: Promax (oblique rotation with Kappa = 4)
    Display: Rotated solution
    Check "Loading plot(s)"
  
  Options:
    Sorted by size
    Suppress absolute values less than: .30
    Exclude cases listwise
  
  OK</pre>

        <div class="box">
            <strong>Why Promax rotation?</strong>
            <p>AIRS constructs are theoretically correlated (e.g., Trust and Explainability are related enablers). Oblique rotation methods like Promax allow factors to correlate, providing a more realistic representation than orthogonal methods like Varimax (Comrey & Lee, 1992).</p>
        </div>

        <h3>2.3 Evaluate Pattern Matrix</h3>
        <div class="box">
            <strong>Item retention criteria (Hinkin, 1998; DeVellis, 2017):</strong>
            <ul>
                <li>Primary loading ≥ .50 (values ≥.70 are excellent)</li>
                <li>Cross-loadings < .30 (ideally < .20)</li>
                <li>Communality (h²) ≥ .30 (proportion of variance explained)</li>
                <li>Items should load on theoretically expected factors</li>
            </ul>
        </div>

        <div class="box warning">
            <strong>Critical Note for 2-Item Constructs:</strong>
            <p>With only 2 items per construct, you <strong>cannot drop individual items</strong> without losing the entire construct. If an item shows problematic loading:</p>
            <ul>
                <li><strong>Option 1:</strong> Review item wording for ambiguity or check for data entry errors</li>
                <li><strong>Option 2:</strong> Accept slightly lower loading (≥.40 may be necessary given only 2 items)</li>
                <li><strong>Option 3:</strong> Drop entire construct if both items show poor performance (not recommended unless theoretically justified)</li>
            </ul>
            <p><strong>Expected outcome:</strong> All 30 items should load cleanly on their theoretically predicted factors. This is a parsimonious, theory-driven instrument with minimal redundancy.</p>
        </div>

        <h3>2.4 Calculate Reliability</h3>
<p><em>Reliability note.</em> Report <strong>McDonald’s ω</strong> alongside Cronbach’s α for each construct (ω ≥ .70 acceptable, ≥ .80 preferred). Compute via <code>psych::omega</code> or <code>semTools::reliability()</code>.</p>

        <p><strong>For each 2-item predictor construct (run 11 times):</strong></p>
        <pre>Analyze → Scale → Reliability Analysis
  Items: Select 2 items for construct (e.g., PE1, PE2)
  Model: Alpha
  Statistics: Check "Item", "Scale", "Scale if item deleted"
  OK</pre>

        <div class="box warning">
            <p><strong>Adjusted thresholds for 2-item scales:</strong></p>
            <ul>
                <li>Cronbach's α ≥ .60 = Acceptable (2 items inherently yield lower alpha than multi-item scales)</li>
                <li>Cronbach's α ≥ .70 = Good</li>
                <li>Inter-item correlation ≥ .40 = Minimum acceptable</li>
                <li>Note: Traditional α ≥ .70 threshold assumes 4+ items; 2-item scales require adjusted expectations</li>
            </ul>
        </div>

        <p><strong>For 4-item outcome construct (AI Adoption Readiness):</strong></p>
        <pre>Analyze → Scale → Reliability Analysis
  Items: BI45, BI46, BI47, BI48
  Model: Alpha
  Statistics: Check "Item", "Scale", "Scale if item deleted"
  OK</pre>

        <p><strong>Target:</strong> α ≥ .70 for the 4-item outcome construct.</p>

        <p><strong>Documentation:</strong> Record all α values, inter-item correlations, and any items flagged for potential problems. With the 2-item structure, expect all 30 items to be retained.</p>

        <p><strong>Step 2 Complete.</strong> Proceed to Step 3 using AIRS_Validation.sav.</p>

        <h2>Section 3: Confirmatory Factor Analysis (AMOS)</h2>

        <p><strong>Use AIRS_Validation.sav (N≈250)</strong></p>

        <h3>3.1 Setup AMOS</h3>
        <pre>Open AMOS Graphics
File → Data Files → File Name → Browse → Select AIRS_Validation.sav
OK

View → Analysis Properties
  Estimation tab: Maximum likelihood (ML)
  Output tab: Check "Standardized estimates"
              Check "Squared multiple correlations"
              Check "Modification indices" (Threshold = 4)
  OK</pre>

        <h3>3.2 Draw the 12-Factor Measurement Model</h3>
        <p><strong>For each of 11 predictor constructs (2 items each):</strong></p>
        <ol>
            <li>Draw latent variable (oval): Use labels PE, EE, SI, FC, HM, PV, HB, TR, EX, ER, AX</li>
            <li>Draw observed variables (rectangles): Connect 2 items to each latent variable
                <ul>
                    <li>PE: PE1, PE2</li>
                    <li>EE: EE5, EE6</li>
                    <li>SI: SI9, SI10</li>
                    <li>FC: FC13, FC14</li>
                    <li>HM: HM17, HM18</li>
                    <li>PV: PV21, PV22</li>
                    <li>HB: HB25, HB26</li>
                    <li>TR: TR29, TR30</li>
                    <li>EX: EX33, EX34</li>
                    <li>ER: ER37, ER38</li>
                    <li>AX: AX41, AX42</li>
                </ul>
            </li>
            <li>Draw single-headed arrows from latent variable to each observed variable</li>
            <li>Add error terms: Select the "Add a unique variable to an existing variable" tool, then click each rectangle</li>
            <li>Fix the scale: Double-click the latent variable oval → Parameters tab → Variance = 1 → OK</li>
        </ol>

        <p><strong>For outcome construct (4 items):</strong></p>
        <ol>
            <li>Draw latent variable (oval): Label "BI" (Behavioral Intention/AI Adoption Readiness)</li>
            <li>Draw observed variables (rectangles): BI45, BI46, BI47, BI48</li>
            <li>Draw arrows from BI to each of the 4 items</li>
            <li>Add error terms to each rectangle</li>
            <li>Fix scale: Set variance = 1 for BI</li>
        </ol>

        <p><strong>Total model:</strong> 12 latent variables, 30 observed indicators (26 predictor items + 4 outcome items)</p>

        <p><strong>Add covariances between all latent variables:</strong></p>
        <pre>Plugins → Draw Covariances
  Select all 12 latent variables
  OK</pre>

        <p>This creates two-headed arrows between all pairs of latent constructs, allowing them to correlate freely.</p>

        <div class="box warning">
            <strong>Note on 2-Item Factor Identification:</strong>
            <p>Each 2-item construct is "just-identified" (df = 0 when considered alone). This is acceptable and common in parsimonious measurement instruments. Model fit is evaluated holistically across all 12 factors, not for individual 2-item constructs.</p>
        </div>

        <h3>3.3 Run the Model</h3>
        <pre>Analyze → Calculate Estimates

View → Text Output</pre>

        <h3>3.4 Evaluate Model Fit</h3>
        <p><strong>Navigate to:</strong> View → Text Output → Model Fit</p>

        <div class="box">
            <strong>Target fit indices (Hu & Bentler, 1999; Browne & Cudeck, 1992):</strong>
            <ul>
                <li><strong>χ²/df ratio:</strong> < 3.0 acceptable; < 2.0 good</li>
                <li><strong>CFI (Comparative Fit Index):</strong> ≥ .90 acceptable; ≥ .95 excellent</li>
                <li><strong>TLI (Tucker-Lewis Index):</strong> ≥ .90 acceptable; ≥ .95 excellent</li>
                <li><strong>RMSEA (Root Mean Square Error of Approximation):</strong> ≤ .08 acceptable; ≤ .06 good; ≤ .05 excellent</li>
                <li><strong>SRMR (Standardized Root Mean Square Residual):</strong> ≤ .08 acceptable (if available)</li>
            </ul>
        </div>

        <div class="box warning">
            <strong>If model fit is poor (CFI < .90 or RMSEA > .08):</strong>
            <ol>
                <li>Review Modification Indices (MI > 4.0 indicates significant improvement if parameter added)</li>
                <li>Consider theoretically justified modifications:
                    <ul>
                        <li>Correlated errors between items within the same construct (if substantively justified)</li>
                        <li>DO NOT add cross-loadings in CFA—this undermines construct validity</li>
                    </ul>
                </li>
                <li>Make only 1-3 modifications maximum to avoid overfitting</li>
                <li>Re-estimate model after each modification</li>
                <li>Document all modifications with theoretical justification</li>
            </ol>
        </div>

        <h3>3.5 Examine Standardized Factor Loadings</h3>
        <pre>View → Text Output → Estimates → Standardized Regression Weights</pre>

        <div class="box">
            <strong>Criteria for acceptable loadings:</strong>
            <ul>
                <li>All standardized loadings should be ≥ .50 (≥.70 is excellent)</li>
                <li>All loadings should be statistically significant: p < .001 (check "Estimates" section)</li>
                <li>Loadings < .50 indicate weak indicators that may not adequately represent the construct</li>
            </ul>
        </div>

        <h3>3.6 Calculate Reliability and Validity</h3>
        <p><strong>Extract standardized loadings from AMOS output, then calculate manually:</strong></p>

        <h4>Composite Reliability (CR) for each construct:</h4>
        <pre>Formula: CR = (Σλ)² / [(Σλ)² + Σ(1-λ²)]

Where:
  λ = standardized factor loading
  Σλ = sum of standardized loadings
  Σ(1-λ²) = sum of error variances

Example for Performance Expectancy (loadings: .78, .82):
  Σλ = .78 + .82 = 1.60
  (Σλ)² = 2.56
  Σ(1-λ²) = (1-.78²) + (1-.82²) = .39 + .33 = .72
  CR = 2.56 / (2.56 + .72) = 2.56 / 3.28 = .78</pre>

        <div class="box">
            <p><strong>Thresholds for 2-item constructs (adjusted):</strong></p>
            <ul>
                <li>CR ≥ .60 = Acceptable for 2-item scales</li>
                <li>CR ≥ .70 = Good</li>
            </ul>
            <p><strong>Threshold for 4-item outcome:</strong></p>
            <ul>
                <li>CR ≥ .70 = Acceptable</li>
                <li>CR ≥ .80 = Good</li>
            </ul>
        </div>

        <h4>Average Variance Extracted (AVE) for each construct:</h4>
        <pre>Formula: AVE = Σλ² / n

Where:
  λ² = squared standardized loading
  n = number of items

Example for Performance Expectancy:
  AVE = (.78² + .82²) / 2
      = (.6084 + .6724) / 2
      = 1.2808 / 2 = .64</pre>

        <div class="box">
            <strong>Threshold for all constructs:</strong>
            <ul>
                <li>AVE ≥ .50 (convergent validity met) = Adequate convergent validity (at least 50% of variance is due to the construct)</li>
                <li>AVE ≥ .60 = Good convergent validity</li>
            </ul>
        </div>

        <h3>3.7 Assess Discriminant Validity</h3>
        <p><strong>Examine factor correlations:</strong></p>
        <pre>View → Text Output → Estimates → Correlations (among latent variables)</pre>

        <div class="box">
            <strong>Discriminant validity criteria:</strong>
            <ul>
                <li>All construct correlations should be < .85 (constructs are empirically distinct)</li>
                <li>Ideally, correlations should be < .70 for clear discriminant validity</li>
                <li><strong>Fornell-Larcker criterion:</strong> For each construct pair, the square root of each construct's AVE should exceed the correlation between them</li>
            </ul>
        </div>

        <div class="box warning">
            <strong>Critical for AIRS: Trust-Explainability relationship</strong>
            <ul>
                <li>Expected correlation: r = .60-.75 (moderate to strong, but theoretically distinct)</li>
                <li>If r > .85: Potential discriminant validity issue—may need second-order factor or item rewording</li>
                <li>These constructs should be related (both are enablers) but empirically separable (Shin, 2021; Langer et al., 2023)</li>
            </ul>
        </div>

        <p><strong>Step 3 Complete.</strong> Proceed to Step 4 using AIRS_Full.sav with grouping variables.</p>

        <h2>Section 4: Measurement Invariance Testing (AMOS)</h2>
<p><em>Group comparability.</em> Evaluate <strong>metric</strong> and <strong>scalar</strong> invariance after configural fit for key groups (e.g., leader vs. individual contributor; low vs. high AI use). Decision rules: ΔCFI ≤ .010 and ΔRMSEA ≤ .015.</p><p><em>Note:</em> Measurement invariance procedures may still be run as robustness checks but are no longer tied to explicit research questions.</p>


        <p><strong>Use AIRS_Full.sav (N=500) with grouping variables</strong></p>

        <h3>4.1 Setup Multi-Group Model</h3>
        <pre>Open your validated CFA model from Step 3 in AMOS

Analyze → Manage Groups
  New (Group Name: "Students")
  New (Group Name: "Professionals")
  OK

For each group:
  File → Data Files → AIRS_Full.sav
  Grouping Variable: Role (or similar variable name in your dataset)
  Group Value: Select "Student" for Students group
               Select "Professional" for Professionals group
  OK</pre>

        <h3>4.2 Test Configural Invariance</h3>
        <p><strong>Tests whether the same factor structure holds across groups.</strong></p>
        <pre>Ensure both groups use the same model structure (no additional constraints)

Analyze → Calculate Estimates

View → Text Output → Model Fit
Record: CFI, TLI, RMSEA, χ², df for Configural Model</pre>

        <div class="box">
            <strong>Configural invariance is supported if:</strong>
            <ul>
                <li>Model fit is acceptable in both groups: CFI ≥ .90, RMSEA ≤ .06 (good) or ≤ .08 (adequate)</li>
                <li>This establishes that the same 12-factor structure applies to both students and professionals</li>
            </ul>
        </div>

        <h3>4.3 Test Metric Invariance</h3>
        <p><strong>Tests whether factor loadings are equivalent across groups.</strong></p>
        <pre>Analyze → Multiple-Group Analysis
  Check "Measurement weights" (constrains factor loadings to be equal across groups)
  Calculate Estimates

View → Text Output → Model Fit
Record: CFI, TLI, RMSEA, χ², df for Metric Model</pre>

        <p><strong>Compare Metric Model to Configural Model:</strong></p>
        <pre>Calculate:
  ΔCFI = CFI(Metric) - CFI(Configural)
  ΔRMSEA = RMSEA(Metric) - RMSEA(Configural)
  Δχ² = χ²(Metric) - χ²(Configural)
  Δdf = df(Metric) - df(Configural)</pre>

        <div class="box">
            <strong>Metric invariance is supported if (Cheung & Rensvold, 2002):</strong>
            <ul>
                <li>ΔCFI ≤ .010 (change in CFI is minimal)</li>
                <li>ΔRMSEA ≤ .015 (change in RMSEA is minimal)</li>
                <li>If Δχ² is significant but ΔCFI/ΔRMSEA criteria met, proceed (χ² is sample-size sensitive)</li>
            </ul>
            <p><strong>Interpretation:</strong> Metric invariance indicates that items have the same meaning across groups, justifying group comparisons in structural models.</p>
        </div>

        <h3>4.4 Repeat for AI Usage Frequency Groups</h3>
        <pre>Create grouping variable in SPSS:
  Transform → Compute Variable
    Target: UsageGroup
    Expression: IF(Mean(Usage50,Usage51,Usage52,Usage53) >= 3.5) UsageGroup=1
                IF(Mean(Usage50,Usage51,Usage52,Usage53) < 3.5) UsageGroup=0
  (This creates High Usage = 1, Low Usage = 0)
  Save dataset

In AMOS:
  Analyze → Manage Groups
    New: "High AI Usage"
    New: "Low AI Usage"
  Assign grouping variable: UsageGroup
  Test Configural and Metric Invariance as above</pre>

        <p><strong>Step 4 Complete.</strong> If metric invariance is established for both role and usage groups, proceed to Step 5.</p>

        
<div class="box">
  <h3>Post-CFA Measurement Model (Expected)</h3>
  <p class="caption">Latent constructs with two indicators each; validated before SEM.</p>
  <pre class="mermaid">
graph TD
  %% Latent constructs
  subgraph UTAUT2 Constructs
    PE[Performance Expectancy]
    EE[Effort Expectancy]
    SI[Social Influence]
    FC[Facilitating Conditions]
    HM[Hedonic Motivation]
    PV[Price Value]
    HB[Habit]
  end

  subgraph AI-specific Constructs
    TR[Trust in AI]
    EX[Explainability]
    ER[Perceived Ethical Risk]
    AX[AI-related Anxiety]
  end

  subgraph Outcome
    BI[AI Adoption Readiness]
  end

  %% Items per construct (2 each)
  PE --> PE1((PE1))
  PE --> PE2((PE2))

  EE --> EE1((EE1))
  EE --> EE2((EE2))

  SI --> SI1((SI1))
  SI --> SI2((SI2))

  FC --> FC1((FC1))
  FC --> FC2((FC2))

  HM --> HM1((HM1))
  HM --> HM2((HM2))

  PV --> PV1((PV1))
  PV --> PV2((PV2))

  HB --> HB1((HB1))
  HB --> HB2((HB2))

  TR --> TR1((TR1))
  TR --> TR2((TR2))

  EX --> EX1((EX1))
  EX --> EX2((EX2))

  ER --> ER1((ER1))
  ER --> ER2((ER2))

  AX --> AX1((AX1))
  AX --> AX2((AX2))

  BI --> BI1((BI1))
  BI --> BI2((BI2))
  </pre>
</div>

<div class="box">
  <h3>Structural Model A — UTAUT2 Baseline → Readiness</h3>
  <p class="caption">Baseline model testing H1 with correlated predictors.</p>
  <pre class="mermaid">
graph LR
  PE[Performance Expectancy] --> BI[AI Adoption Readiness]
  EE[Effort Expectancy] --> BI
  SI[Social Influence] --> BI
  FC[Facilitating Conditions] --> BI
  HM[Hedonic Motivation] --> BI
  PV[Price Value] --> BI
  HB[Habit] --> BI

  %% Correlated predictors (compact rendering)
  PE --- EE
  EE --- SI
  SI --- FC
  FC --- HM
  HM --- PV
  PV --- HB
  HB --- PE
  </pre>
</div>

<div class="box">
  <h3>Structural Model B — AIRS Extended with Mediation</h3>
  <p class="caption">Extended model testing H2–H3. Indirect effect EX → TR → BI is tested via bootstrap.</p>
  <pre class="mermaid">
graph LR
  %% UTAUT2 predictors
  PE[Performance Expectancy] --> BI[AI Adoption Readiness]
  EE[Effort Expectancy] --> BI
  SI[Social Influence] --> BI
  FC[Facilitating Conditions] --> BI
  HM[Hedonic Motivation] --> BI
  PV[Price Value] --> BI
  HB[Habit] --> BI

  %% AI-specific predictors
  TR[Trust in AI] --> BI
  EX[Explainability] --> TR
  EX ----> BI
  ER[Perceived Ethical Risk] -.->|expected −| BI
  AX[AI-related Anxiety] -.->|expected −| BI

  %% Correlated predictors
  PE --- EE
  EE --- SI
  SI --- FC
  FC --- HM
  HM --- PV
  PV --- HB
  HB --- PE
  TR --- EX
  ER --- AX
  </pre>
</div>

<section id="expected-sem" style="margin-top:2rem;">
  <h2>Expected SEM (H1–H4)</h2>
  <p>This section visualizes the structural models tested after the CFA measurement model is confirmed.</p>

  <div class="box">
    <h3>SEM Model for H1 — UTAUT2 Predictors → Readiness</h3>
    <p class="caption">Tests whether UTAUT2 constructs significantly predict AI Adoption Readiness (H1).</p>
    <pre class="mermaid">
graph LR
  %% UTAUT2 to Readiness
  PE[Performance Expectancy] --> BI[AI Adoption Readiness]
  EE[Effort Expectancy] --> BI
  SI[Social Influence] --> BI
  FC[Facilitating Conditions] --> BI
  HM[Hedonic Motivation] --> BI
  PV[Price Value] --> BI
  HB[Habit] --> BI

  %% Correlated predictors (ellipses)
  PE --- EE
  EE --- SI
  SI --- FC
  FC --- HM
  HM --- PV
  PV --- HB
  HB --- PE
    </pre>
  </div>

  <div class="box">
    <h3>SEM Model for H2–H3 — AIRS Extended with Mediation</h3>
    <p class="caption">Adds AI-specific constructs (Trust, Explainability, Ethical Risk, Anxiety). Tests incremental variance (H3) and includes the EX → TR → BI mechanism.</p>
    <pre class="mermaid">
graph LR
  %% UTAUT2 block
  PE[Performance Expectancy] --> BI[AI Adoption Readiness]
  EE[Effort Expectancy] --> BI
  SI[Social Influence] --> BI
  FC[Facilitating Conditions] --> BI
  HM[Hedonic Motivation] --> BI
  PV[Price Value] --> BI
  HB[Habit] --> BI

  %% AI-specific paths
  EX[Explainability] --> TR[Trust in AI]
  TR --> BI
  EX ----> BI
  ER[Perceived Ethical Risk] -.->|expected −| BI
  AX[AI-related Anxiety] -.->|expected −| BI

  %% Correlated exogenous predictors
  PE --- EE
  EE --- SI
  SI --- FC
  FC --- HM
  HM --- PV
  PV --- HB
  HB --- PE
  TR --- EX
  ER --- AX
    </pre>
    <p class="caption">Indirect effect tested via bias-corrected bootstrap (5,000 resamples): EX → TR → BI.</p>
  </div>

  <div class="box">
    <h3>SEM Perspective for H4 — Moderation by Role and AI Usage</h3>
    <p class="caption">H4 is evaluated via multi-group SEM. The same structural model is estimated in each group and path differences are tested.</p>
    <pre class="mermaid">
flowchart LR
  subgraph Group A: Students
    APE[PE] --> ABI[AI Readiness]
    AEE[EE] --> ABI
    ASI[SI] --> ABI
    AFC[FC] --> ABI
    AHM[HM] --> ABI
    APV[PV] --> ABI
    AHB[HB] --> ABI
    ATR[TR] --> ABI
    AEX[EX] --> ATR
    AEX ----> ABI
    AER[ER] -.-> ABI
    AAX[AX] -.-> ABI
  end

  subgraph Group B: Professionals
    BPE[PE] --> BBI[AI Readiness]
    BEE[EE] --> BBI
    BSI[SI] --> BBI
    BFC[FC] --> BBI
    BHM[HM] --> BBI
    BPV[PV] --> BBI
    BHB[HB] --> BBI
    BTR[TR] --> BBI
    BEX[EX] --> BTR
    BEX ----> BBI
    BER[ER] -.-> BBI
    BAX[AX] -.-> BBI
  end

  %% Indicate comparison of structural weights
  APE -. compare paths .- BPE
  AEE -. compare paths .- BEE
  ASI -. compare paths .- BSI
  AFC -. compare paths .- BFC
  AHM -. compare paths .- BHM
  APV -. compare paths .- BPV
  AHB -. compare paths .- BHB
  ATR -. compare paths .- BTR
  AEX -. compare paths .- BEX
  AER -. compare paths .- BER
  AAX -. compare paths .- BAX
    </pre>
    <p class="caption">Evaluate moderation with chi-square difference tests for constrained vs. unconstrained structural weights, and examine pairwise parameter comparisons.</p>
  </div>
</section>
<h2>Section 5: Structural Equation Modeling (AMOS)</h2>

        <p><strong>Use AIRS_Full.sav (N=500)</strong></p>

        <h3>5.1 Model 1: UTAUT2 Baseline (Test H1)</h3>
        <p><strong>Start from validated CFA measurement model, add structural paths:</strong></p>
        <ol>
            <li>Delete covariances between the 7 UTAUT2 constructs and the BI outcome construct</li>
            <li>Draw single-headed arrows (structural paths) from each UTAUT2 construct to BI:
                <ul>
                    <li>PE → BI</li>
                    <li>EE → BI</li>
                    <li>SI → BI</li>
                    <li>FC → BI</li>
                    <li>HM → BI</li>
                    <li>PV → BI</li>
                    <li>HB → BI</li>
                </ul>
            </li>
            <li>Keep covariances among the 7 UTAUT2 predictors (two-headed arrows)</li>
            <li>Remove or leave separate the 4 AI-specific constructs (TR, EX, ER, AX) for this baseline test</li>
        </ol>

        <pre>Analyze → Calculate Estimates

View → Text Output → Estimates</pre>

        <div class="box">
            <strong>Record for Model 1 (UTAUT2 Baseline):</strong>
            <ul>
                <li>Standardized path coefficients (β) for each predictor → BI</li>
                <li>Significance of each path (p-values, with p < .05 indicating significant effect)</li>
                <li>R² for BI (proportion of variance in AI adoption readiness explained by UTAUT2)</li>
                <li>Model fit: CFI, TLI, RMSEA, χ², df</li>
                <li>AIC (Akaike Information Criterion) for model comparison</li>
            </ul>
        </div>

        <h3>5.2 Model 2: AIRS Extended (Test H2)</h3>
        <p><strong>Add AI-specific constructs to the model:</strong></p>
        <ol>
            <li>Add structural paths from each AI-specific construct to BI:
                <ul>
                    <li>TR → BI (Trust as enabler)</li>
                    <li>EX → BI (Explainability as enabler)</li>
                    <li>ER → BI (Ethical Risk as inhibitor, expect negative β)</li>
                    <li>AX → BI (Anxiety as inhibitor, expect negative β)</li>
                </ul>
            </li>
            <li>Add mediation path: EX → TR (Explainability influences Trust)</li>
            <li>Add covariances among all 11 predictor constructs</li>
            <li>Keep all 7 UTAUT2 → BI paths from Model 1</li>
        </ol>

        <pre>Analyze → Calculate Estimates

View → Text Output → Estimates</pre>

        <div class="box">
            <strong>Record for Model 2 (AIRS Extended):</strong>
            <ul>
                <li>Standardized path coefficients for all 11 predictors → BI</li>
                <li>Path coefficient for EX → TR mediation</li>
                <li>Significance of all paths (p-values)</li>
                <li>R² for BI (should be higher than Model 1 if H2 supported)</li>
                <li>R² for TR (variance explained by Explainability)</li>
                <li>Model fit: CFI, TLI, RMSEA, χ², df</li>
                <li>AIC for model comparison</li>
            </ul>
        </div>

        <h3>5.3 Model Comparison: Test H3 (AIRS vs. UTAUT2)</h3>
        <p><strong>Compare incremental validity:</strong></p>

        <pre>Calculate:
  ΔR² = R²(AIRS) - R²(UTAUT2)
  ΔAIC = AIC(AIRS) - AIC(UTAUT2)  [Lower AIC = better model]
  Δχ² = χ²(UTAUT2) - χ²(AIRS)
  Δdf = df(UTAUT2) - df(AIRS)</pre>

        <div class="box">
            <strong>H3 is supported if:</strong>
            <ul>
                <li>ΔR² > .02 (AIRS explains at least 2% more variance—small effect size)</li>
                <li>ΔR² > .13 would indicate medium effect size (Cohen, 1988)</li>
                <li>Δχ² is statistically significant (p < .05) using chi-square difference test</li>
                <li>ΔAIC < 0 (AIRS has lower AIC, indicating better balance of fit and parsimony)</li>
            </ul>
            <p><strong>Interpretation:</strong> If H3 is supported, the AI-specific constructs (Trust, Explainability, Ethical Risk, Anxiety) add meaningful predictive power beyond the established UTAUT2 framework (Dwivedi et al., 2021).</p>
        </div>

        <h3>5.4 Test Moderation: H4 (Role and AI Usage)</h3>
        <p><strong>Multi-group analysis by Role:</strong></p>
        <pre>Analyze → Manage Groups
  Group 1: Students
  Group 2: Professionals

Model A (Unconstrained):
  Analyze → Calculate Estimates
  Record: χ²(unconstrained), df

Model B (Constrained - All paths equal):
  Analyze → Multiple-Group Analysis
  Check "Structural weights" (constrains all structural paths to equality)
  Calculate Estimates
  Record: χ²(constrained), df

Calculate:
  Δχ² = χ²(constrained) - χ²(unconstrained)
  Δdf = df(constrained) - df(unconstrained)
  
Check significance: Δχ² with Δdf degrees of freedom (p < .05 = moderation present)</pre>

        <div class="box">
            <strong>If Δχ² is significant (moderation detected):</strong>
            <ol>
                <li>Examine Critical Ratios for Differences between Parameters:
                    <pre>View → Text Output → Pairwise Parameter Comparisons</pre>
                </li>
                <li>Critical Ratio (z-score) > |1.96| indicates that specific path differs significantly between groups (p < .05)</li>
                <li>Identify which relationships are moderated by role (e.g., does Social Influence → BI differ for students vs. professionals?)</li>
            </ol>
        </div>

        <p><strong>Repeat multi-group analysis for AI Usage Frequency</strong> (High vs. Low usage groups).</p>

        <div class="box success">
            <p><strong>Expected moderation patterns based on AIRS theory:</strong></p>
            <ul>
                <li><strong>Role:</strong> Professionals may show stronger effects for Facilitating Conditions and Social Influence (organizational context matters more)</li>
                <li><strong>AI Usage Frequency:</strong> High-usage users may show stronger effects for Habit and weaker effects for Anxiety (familiarity reduces anxiety)</li>
            </ul>
        </div>

        <p><strong>Step 5 Complete. Analysis finished.</strong></p>

        <h2>Quick Reference: Target Values and Thresholds</h2>

        <table>
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Acceptable</th>
                    <th>Good/Excellent</th>
                    <th>Source</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td colspan="4" style="background: #34495e; color: white; font-weight: 600;">Exploratory Factor Analysis (EFA)</td>
                </tr>
                <tr>
                    <td>KMO</td>
                    <td>≥ .80</td>
                    <td>≥ .90</td>
                    <td>Watkins, 2018</td>
                </tr>
                <tr>
                    <td>Bartlett's Test</td>
                    <td>p < .001</td>
                    <td>—</td>
                    <td>Watkins, 2018</td>
                </tr>
                <tr>
                    <td>Primary loading</td>
                    <td>≥ .50</td>
                    <td>≥ .70</td>
                    <td>Hinkin, 1998</td>
                </tr>
                <tr>
                    <td>Cross-loading</td>
                    <td>< .30</td>
                    <td>< .20</td>
                    <td>Hinkin, 1998</td>
                </tr>
                <tr>
                    <td>Communality (h²)</td>
                    <td>≥ .30</td>
                    <td>≥ .50</td>
                    <td>Comrey & Lee, 1992</td>
                </tr>
                <tr>
                    <td>Cronbach's α (2-item)</td>
                    <td>≥ .60</td>
                    <td>≥ .70</td>
                    <td>Adjusted for brief scales</td>
                </tr>
                <tr>
                    <td>Cronbach's α (4-item)</td>
                    <td>≥ .70</td>
                    <td>≥ .80</td>
                    <td>DeVellis, 2017</td>
                </tr>
                <tr>
                    <td colspan="4" style="background: #34495e; color: white; font-weight: 600;">Confirmatory Factor Analysis (CFA)</td>
                </tr>
                <tr>
                    <td>CFI</td>
                    <td>≥ .90</td>
                    <td>≥ .95</td>
                    <td>Hu & Bentler, 1999</td>
                </tr>
                <tr>
                    <td>TLI</td>
                    <td>≥ .90</td>
                    <td>≥ .95</td>
                    <td>Hu & Bentler, 1999</td>
                </tr>
                <tr>
                    <td>RMSEA</td>
                    <td>≤ .08</td>
                    <td>≤ .06; ≤ .05 excellent</td>
                    <td>Browne & Cudeck, 1992</td>
                </tr>
                <tr>
                    <td>χ²/df ratio</td>
                    <td>< 3.0</td>
                    <td>< 2.0</td>
                    <td>Kline, 2015</td>
                </tr>
                <tr>
                    <td>Standardized loading</td>
                    <td>≥ .50</td>
                    <td>≥ .70</td>
                    <td>Hair et al., 2010</td>
                </tr>
                <tr>
                    <td>CR (2-item construct)</td>
                    <td>≥ .60</td>
                    <td>≥ .70</td>
                    <td>Adjusted for 2 items</td>
                </tr>
                <tr>
                    <td>CR (4-item outcome)</td>
                    <td>≥ .70</td>
                    <td>≥ .80</td>
                    <td>DeVellis, 2017</td>
                </tr>
                <tr>
                    <td>AVE</td>
                    <td>≥ .50</td>
                    <td>≥ .60</td>
                    <td>Fornell & Larcker, 1981</td>
                </tr>
                <tr>
                    <td>Construct correlation</td>
                    <td>< .85</td>
                    <td>< .70</td>
                    <td>For discriminant validity</td>
                </tr>
                <tr>
                    <td colspan="4" style="background: #34495e; color: white; font-weight: 600;">Measurement Invariance</td>
                </tr>
                <tr>
                    <td>ΔCFI</td>
                    <td>≤ .010</td>
                    <td>—</td>
                    <td>Cheung & Rensvold, 2002</td>
                </tr>
                <tr>
                    <td>ΔRMSEA</td>
                    <td>≤ .015</td>
                    <td>—</td>
                    <td>Chen, 2007</td>
                </tr>
                <tr>
                    <td colspan="4" style="background: #34495e; color: white; font-weight: 600;">Structural Equation Modeling (SEM)</td>
                </tr>
                <tr>
                    <td>Path coefficient significance</td>
                    <td>p < .05</td>
                    <td>p < .01; p < .001</td>
                    <td>Standard practice</td>
                </tr>
                <tr>
                    <td>R² (small effect)</td>
                    <td>> .02</td>
                    <td>—</td>
                    <td>Cohen, 1988</td>
                </tr>
                <tr>
                    <td>R² (medium effect)</td>
                    <td>> .13</td>
                    <td>—</td>
                    <td>Cohen, 1988</td>
                </tr>
                <tr>
                    <td>R² (large effect)</td>
                    <td>> .26</td>
                    <td>—</td>
                    <td>Cohen, 1988</td>
                </tr>
            </tbody>
        </table>

        <h2>Expected Timeline and Deliverables</h2>

        <div class="box success">
            <p><strong>Step 1:</strong> Clean dataset prepared, split into development (N≈250) and validation (N≈250) samples</p>
            <p><strong>Step 2:</strong> 11-factor structure confirmed via EFA, all 30 items retained with acceptable loadings and reliability</p>
            <p><strong>Step 3:</strong> Measurement model validated via CFA with CFI ≥ .90, RMSEA ≤ .06 (good) or ≤ .08 (adequate), all loadings ≥ .50, CR ≥ .60, AVE ≥ .50 (convergent validity met)</p>
            <p><strong>Step 4:</strong> Metric and scalar invariance (if used) across role and usage groups (ΔCFI ≤ .010)</p>
            <p><strong>Step 5:</strong> H1–H4 tested; AIRS demonstrates incremental validity over UTAUT2; moderation effects identified; dissertation-ready results</p>
        </div>

        <div class="timeline">
            <strong>This Guide: Evidence-Based, APA-Documented Analysis Plan</strong>
        </div>

        

<section id="references" class="references" style="margin-top:2rem;">
  <h2>References</h2>
  <style>
    .apa-ref { margin: 0 0 10px 0; padding-left: 1.5em; text-indent: -1.5em; }
  </style>

  <p class="apa-ref">Browne, M. W., & Cudeck, R. (1992). Alternative ways of assessing model fit. <em>Sociological Methods & Research, 21</em>(2), 230–258. https://doi.org/10.1177/0049124192021002005</p>
  <p class="apa-ref">Chen, F. F. (2007). Sensitivity of goodness of fit indexes to lack of measurement invariance. <em>Structural Equation Modeling, 14</em>(3), 464–504. https://doi.org/10.1080/10705510701301834</p>
  <p class="apa-ref">Cheung, G. W., & Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. <em>Structural Equation Modeling, 9</em>(2), 233–255. https://doi.org/10.1207/S15328007SEM0902_5</p>
  <p class="apa-ref">Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed.). Lawrence Erlbaum Associates.</p>
  <p class="apa-ref">Comrey, A. L., & Lee, H. B. (1992). <em>A first course in factor analysis</em> (2nd ed.). Lawrence Erlbaum Associates.</p>
  <p class="apa-ref">DeVellis, R. F. (2017). <em>Scale development: Theory and applications</em> (4th ed.). Sage Publications.</p>
  <p class="apa-ref">Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). <em>Internet, phone, mail, and mixed-mode surveys: The tailored design method</em> (4th ed.). Wiley.</p>
  <p class="apa-ref">Dwivedi, Y. K., Hughes, L., Ismagilova, E., et al. (2021). Artificial Intelligence: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. <em>International Journal of Information Management, 57</em>, 101994. https://doi.org/10.1016/j.ijinfomgt.2019.08.002</p>
  <p class="apa-ref">Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. <em>Journal of Marketing Research, 18</em>(1), 39–50. https://doi.org/10.1177/002224378101800104</p>
  <p class="apa-ref">Hair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). <em>Multivariate data analysis</em> (7th ed.). Pearson.</p>
  <p class="apa-ref">Henseler, J., Ringle, C. M., & Sarstedt, M. (2015). A new criterion for assessing discriminant validity in variance-based structural equation modeling. <em>Journal of the Academy of Marketing Science, 43</em>(1), 115–135. https://doi.org/10.1007/s11747-014-0403-8</p>
  <p class="apa-ref">Hinkin, T. R. (1998). A brief tutorial on the development of measures for use in survey questionnaires. <em>Organizational Research Methods, 1</em>(1), 104–121. https://doi.org/10.1177/109442819800100106</p>
  <p class="apa-ref">Hu, L. T., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. <em>Structural Equation Modeling, 6</em>(1), 1–55. https://doi.org/10.1080/10705519909540118</p>
  <p class="apa-ref">Kline, R. B. (2016). <em>Principles and practice of structural equation modeling</em> (4th ed.). Guilford Press.</p>
  <p class="apa-ref">Langer, M., König, C. J., & Papathanasiou, M. (2023). Trust in artificial intelligence: A review of empirical research. <em>Academy of Management Annals, 17</em>(1), 47–81. https://doi.org/10.5465/annals.2021.0164</p>
  <p class="apa-ref">McDonald, R. P. (1999). <em>Test theory: A unified treatment</em>. Lawrence Erlbaum Associates.</p>
  <p class="apa-ref">Podsakoff, P. M., MacKenzie, S. B., Lee, J. Y., & Podsakoff, N. P. (2003). Common method biases in behavioral research: A critical review of the literature and recommended remedies. <em>Journal of Applied Psychology, 88</em>(5), 879–903. https://doi.org/10.1037/0021-9010.88.5.879</p>
  <p class="apa-ref">Shin, D. (2021). The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. <em>International Journal of Human-Computer Studies, 146</em>, 102551. https://doi.org/10.1016/j.ijhcs.2020.102551</p>
  <p class="apa-ref">Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. <em>MIS Quarterly, 36</em>(1), 157–178. https://doi.org/10.2307/41410412</p>
  <p class="apa-ref">Watkins, M. W. (2018). Exploratory factor analysis: A guide to best practice. <em>Journal of Black Psychology, 44</em>(3), 219–246. https://doi.org/10.1177/0095798418771807</p>
</section>

 
<!-- =============================== -->
<!-- Supplement: High-Value Additions-->
<!-- =============================== -->
</body>
</html>