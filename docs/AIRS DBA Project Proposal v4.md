


Artificial Intelligence Readiness Score:
Extending Model for Enterprise AI Adoption



Fabio Correa
Touro University Worldwide
Doctorate of Business Administration
DBA712 - Doctoral Research Project Proposal

Committee:
Chair: Dr. Karina Kasztelnik
Faculty Member: Dr. Jerome Jones
Faculty Member: Dr. Donna Day

October 5, 2025 
Abstract

The Artificial Intelligence Readiness Score (AIRS) extends UTAUT2 to address trust, explainability, ethical risk, and anxiety—constructs essential for understanding enterprise AI adoption where classic models may underspecify determinants of reliance. As organizations adopt artificial intelligence at an accelerating pace, established acceptance frameworks risk losing explanatory power. This study develops and validates AIRS by retaining UTAUT2’s core constructs and adding four AI-specific determinants: trust in AI, perceived explainability, perceived ethical risk, and AI-related anxiety (Venkatesh et al., 2012; Shin, 2021; Langer et al., 2023; Floridi et al., 2018). A cross-sectional survey of approximately 500 U.S. students and professionals will be administered through a professional panel provider. Dimensionality and reliability will be assessed with exploratory and confirmatory factor analyses, and structural comparisons will test AIRS against a UTAUT2 baseline for incremental validity and mediation through trust. Moderation by role, AI usage frequency, and business unit will also be examined (Dwivedi et al., 2021). Anticipated contributions include a validated readiness instrument and actionable guidance for organizations, linking explainability design and governance safeguards to stronger trust, reduced risk perceptions, and improved adoption outcomes.
Keywords: artificial intelligence adoption, UTAUT2, trust in AI, explainability, ethical risk, AI anxiety, enterprise readiness
 
1. Introduction
Artificial intelligence is moving from isolated pilots to everyday decision support in organizations, which exposes limits in classic technology acceptance theories built for deterministic, transparent systems. Foundational models such as the Theory of Planned Behavior, the Technology Acceptance Model, and Diffusion of Innovations clarified intention formation and social spread, yet they do not directly address probabilistic outputs, opacity, or governance requirements that shape willingness to rely on AI in enterprise contexts (Ajzen, 1991; Davis, 1989; Rogers, 2003). The Unified Theory of Acceptance and Use of Technology and its consumer extension offer a durable baseline through performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit, but these constructs alone leave unresolved how trust, explainability, and perceived ethical exposures alter adoption in high accountability settings (Venkatesh et al., 2003; Venkatesh et al., 2012).
Recent scholarship argues that AI changes the adoption calculus in three ways. First, many systems are partially opaque, which elevates the role of perceived explainability in justifying action. Second, AI introduces distinct ethical exposures around bias, privacy, and accountability that suppress intention when safeguards are unclear. Third, reliance on probabilistic outputs hinges on calibrated trust in both the system and the organizational processes around it (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Shin, 2021; Langer et al., 2023). Reviews and domain studies converge on the need to augment established acceptance frameworks with AI specific enablers and inhibitors to retain predictive validity in organizational settings (Dwivedi et al., 2021; Langer et al., 2023).
This study responds by developing and validating the Artificial Intelligence Readiness Score, a theory grounded extension of UTAUT2 for enterprise contexts. The framework retains the validated UTAUT2 core and adds two enablers, trust in AI and perceived explainability, and two inhibitors, perceived ethical risk and AI related anxiety, which together are expected to explain additional variance in readiness to adopt AI at work. The objective is a practical instrument that supports valid assessment, responsible deployment, and change management by linking explainability design and governance practices to measurable shifts in employee readiness (Venkatesh et al., 2012; Floridi et al., 2018; Langer et al., 2023).
The introduction establishes the study’s motivation and scope. The research focuses on individual readiness among students and professionals in the United States, models contextual moderators such as role and usage frequency, and tests whether the extended framework provides incremental validity over a UTAUT2 baseline. The proposed contribution is both theoretical and applied. It advances acceptance theory by integrating epistemic and ethical determinants alongside expectancy beliefs, and it offers organizations a validated scale for diagnosing readiness and targeting interventions that build calibrated trust, reduce perceived risk, and enable responsible adoption (Shin, 2021; Dwivedi et al., 2021; Langer et al., 2023).
This proposal includes students alongside employed professionals because students are immediate entrants to an AI infused labor market, making them proximal adopters whose expectations and readiness will shape near-term enterprise adoption.

2. Problem Statement and Purpose
Problem statement. Classic acceptance models explain intention and use in many settings, yet they do not fully address how people decide to rely on probabilistic and partially opaque AI systems under organizational accountability. The Unified Theory of Acceptance and Use of Technology and its consumer extension provide a strong baseline, but they do not explicitly model perceived explainability, trust in AI, or ethical exposures that influence willingness to act on AI outputs in enterprise contexts (Venkatesh et al., 2003; Venkatesh et al., 2012; Doshi-Velez & Kim, 2017; Floridi et al., 2018; Dwivedi et al., 2021; Shin, 2021; Langer et al., 2023). There is a need for a validated readiness instrument that retains the UTAUT2 core while adding AI specific enablers and inhibitors that have demonstrated explanatory value in organizational research.
Purpose. The purpose of this study is to develop and validate the Artificial Intelligence Readiness Score for enterprise settings. The study will
1.	define and operationalize four AI specific constructs that extend UTAUT2: trust in AI, perceived explainability, perceived ethical risk, and AI related anxiety
2.	evaluate dimensionality and reliability through exploratory and confirmatory factor analyses
3.	test an extended structural model that compares AIRS to a UTAUT2 baseline to assess incremental validity
4.	examine moderation by role and AI usage frequency to understand contextual effects on readiness (Venkatesh et al., 2012; Floridi et al., 2018; Dwivedi et al., 2021; Shin, 2021; Langer et al., 2023).
The intended contribution is both theoretical and applied. The study refines acceptance theory for socio-technical systems and provides a practical instrument with scoring guidance that organizations can use to diagnose readiness and inform governance and enablement.

3. Research Questions
RQ1. What psychological, motivational, and contextual factors influence individual readiness to adopt AI technologies in organizational settings?
RQ2. To what extent do UTAUT2 constructs predict AI adoption readiness among students and professionals in the United States (Venkatesh et al., 2012)?

4. Literature Review (Critical Synthesis)
Classic acceptance theories explain much of why people intend to use information systems, yet they were not designed for probabilistic and partially opaque systems that must be governed inside organizations. The Theory of Planned Behavior clarifies the role of attitudes, norms, and control, but it is technology agnostic (Ajzen, 1991). The Technology Acceptance Model adds perceived usefulness and ease of use, which travel well across domains but underrepresent ethical and epistemic concerns that shape reliance on artificial intelligence (Davis, 1989). Diffusion of Innovations explains social spread through attributes such as relative advantage and compatibility, yet it is less diagnostic for the individual’s willingness to rely on complex models in accountable settings (Rogers, 2003). The Unified Theory of Acceptance and Use of Technology and UTAUT2 consolidate these lines into performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, habit, and voluntariness of use, providing a durable baseline in organizational and consumer contexts (Venkatesh et al., 2003; Venkatesh et al., 2012).
Artificial intelligence stresses this baseline in three ways. First, many systems are partially opaque, which elevates the role of perceived explainability for justified action. Second, artificial intelligence raises distinctive ethical exposures such as bias, privacy, and accountability that can suppress intention when safeguards are unclear. Third, reliance on probabilistic outputs depends on calibrated trust in the system and in organizational processes around deployment and oversight (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Shin, 2021). Reviews and domain studies converge on a compact set of determinants that repeatedly add explanatory power in enterprise contexts: trust in artificial intelligence and perceived explainability as enablers, and perceived ethical risk and artificial intelligence related anxiety as inhibitors (Dwivedi et al., 2021; Langer et al., 2023).
Trust functions as a gateway condition for consequential use. Evidence indicates that when users perceive the system as competent, fair, predictable, and aligned with organizational goals, intention and reliance increase. Explainability strengthens trust by making reasons clear and actionable at decision time rather than only increasing general transparency (Shin, 2021; Langer et al., 2023). Ethical risk captures anticipated harms from bias, privacy failures, and unclear accountability. Studies show direct negative effects on intention and moderating effects that weaken performance expectancy and social influence when risk is salient (Floridi et al., 2018; Dwivedi et al., 2021). Artificial intelligence related anxiety captures affective responses to autonomy, opacity, and rapid change. Findings suggest a negative effect on intention and possible nonlinear relations with exposure in which moderate, guided use can reduce anxiety while very low or very high exposure can increase it (Tao et al., 2020/2021; Kim et al., 2025).
Two recurring controversies shape measurement choices. First, transparency and explainability are often conflated. The literature supports treating transparency as visibility into data and process, and explainability as reasons for specific outputs that enable justified action. Keeping these constructs distinct improves discriminant validity and clarifies levers for intervention, since transparency supports oversight while explainability supports day to day reliance (Shin, 2021; Langer et al., 2023). Second, privacy appears across models as a component of risk, a predictor of trust, or a moderator that dampens positive drivers. Declaring its role a priori improves comparability across studies and avoids interpretive drift (Dwivedi et al., 2021).
Methodologically, scale development for these constructs follows established guidance. Typical workflows include expert review, cognitive pretesting, exploratory factor analysis with loading and cross loading criteria, confirmatory factor analysis with fit and reliability thresholds, and checks for convergent and discriminant validity. Procedural and statistical remedies for common method variance and measurement invariance across roles and exposure levels are recommended in organizational settings (Hinkin, 1998; DeVellis, 2017; Venkatesh et al., 2012). Studies that compare a UTAUT2 only baseline with extended models commonly report improved fit and additional explained variance for intention and continuance when trust, explainability, ethical risk, and anxiety are included (Gansser & Reich, 2021; Salih et al., 2025; Langer et al., 2023).
Taken together, the evidence motivates a principled extension of UTAUT2 for enterprise artificial intelligence. Retain the validated core of expectancy, social influence, facilitating conditions, enjoyment, perceived value, and habit. Add trust and perceived explainability as enablers, and perceived ethical risk and artificial intelligence related anxiety as inhibitors. This structure aligns with observed effects, maps cleanly to organizational levers such as explanation design, bias and privacy safeguards, auditability, and human in the loop procedures, and provides a measurement blueprint for readiness in accountable settings (Venkatesh et al., 2012; Floridi et al., 2018; Shin, 2021; Langer et al., 2023).

5. Theoretical Orientation and Conceptual Framework
5.1 Orienting theories
This study is grounded in the Unified Theory of Acceptance and Use of Technology and its consumer extension, which synthesize performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, habit, and voluntariness of use as proximal determinants of intention and use (Venkatesh et al., 2003; Venkatesh et al., 2012). Classic models such as the Theory of Planned Behavior, the Technology Acceptance Model, and Diffusion of Innovations inform the background but are not sufficient for probabilistic and partially opaque systems that require governance in organizational settings (Ajzen, 1991; Davis, 1989; Rogers, 2003).
Contemporary work on artificial intelligence adoption highlights four constructs that systematically condition intention to rely on model outputs in enterprises. Trust in AI and perceived explainability function as enablers that increase intention directly and by amplifying expectancy beliefs. Perceived ethical risk and AI related anxiety function as inhibitors that depress intention and can attenuate positive paths from the core framework (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Shin, 2021; Langer et al., 2023; Dwivedi et al., 2021).
5.2 AIRS conceptual framework
AIRS retains the validated UTAUT2 core and integrates four AI specific constructs.
Enablers: Trust in AI: confidence that system outputs are reliable, fair, and aligned with user and organizational goals. Expected to increase intention and to mediate or strengthen the effect of performance expectancy on intention (Shin, 2021; Langer et al., 2023).
Perceived explainability: the perceived clarity and usefulness of reasons attached to model outputs. Expected to increase intention directly and indirectly through trust in AI (Doshi-Velez & Kim, 2017; Shin, 2021).
Inhibitors: Perceived ethical risk: anticipated harms related to bias, privacy, accountability, and job threat. Expected to exert a negative direct effect on intention and to weaken positive paths from performance expectancy and social influence when risk is salient (Floridi et al., 2018; Dwivedi et al., 2021).
AI related anxiety: affective unease about autonomy, opacity, and pace of change. Expected to exert a negative effect on intention and to interact with exposure, with moderate guided use reducing anxiety over time (Tao et al., 2020/2021; Kim et al., 2025).
Contextual moderators: Role, AI usage frequency, and voluntariness are modeled as moderators. Voluntariness of use also functions as a direct predictor, capturing perceived autonomy in adoption decisions, consistent with its treatment in UTAUT2 (Venkatesh et al., 2012). In discretionary contexts, trust and explainability are expected to carry greater weight, whereas in mandated contexts social influence and facilitating conditions are expected to dominate early intention. Greater exposure is expected to strengthen habit and performance expectancy and to reduce the negative effect of anxiety (Venkatesh et al., 2003; Venkatesh et al., 2012; Shin, 2021).
Expanded facilitating conditions: For AI, facilitating conditions include not only infrastructure and training but also governance artifacts such as privacy safeguards, bias and performance monitoring, audit trails, escalation pathways, and human in the loop procedures. These conditions reduce perceived ethical risk and increase the perceived safety of reliance in accountable settings (Floridi et al., 2018; Dwivedi et al., 2021).
5.3 Path model and testable implications
Let BI denote behavioral intention or readiness to adopt AI at work.
1.	Performance expectancy → BI is positive, strengthened when trust in AI is high.
2.	Effort expectancy → BI is positive, with diminishing returns when explainability remains low.
3.	Social influence → BI is positive, stronger under lower voluntariness.
4.	Facilitating conditions → BI is positive and operates partly through reduced perceived ethical risk.
5.	Hedonic motivation, price value, habit, and voluntariness of use → BI are positive, with habit increasing as exposure rises and voluntariness reflecting perceived autonomy in adoption decisions.

Note: Voluntariness of use serves a dual function in this framework. It acts as a direct predictor of behavioral intention and also moderates the strength of social influence and facilitating conditions effects, with these relationships expected to be stronger in mandated versus discretionary adoption contexts. This reintroduction is justified because: (1) in the original UTAUT (2003), VO was a moderator of social influence effects; (2) UTAUT2 (2012) removed VO for consumer contexts where use is typically voluntary; (3) enterprise AI adoption often involves mandatory tools or strong managerial expectations, making VO theoretically relevant again as both predictor and moderator (Venkatesh et al., 2003; Venkatesh et al., 2012).
6.	Perceived explainability → Trust in AI is positive and sizable.
7.	Trust in AI → BI is positive and mediates part of the performance expectancy effect.
8.	Perceived ethical risk → BI is negative and weakens the effects of performance expectancy and social influence.
9.	AI related anxiety → BI is negative and may decline with calibrated exposure.
These implications align with the study’s hypotheses and guide the structural comparisons between a UTAUT2 baseline and the AIRS extended model.
5.4 Measurement model assumptions
Constructs are modeled as first order latent variables with multi item indicators. Convergent validity will be assessed through standardized loadings and average variance extracted at or above recommended thresholds. Discriminant validity will be evaluated using Fornell–Larcker and heterotrait–monotrait checks, with particular attention to the separation of explainability from general transparency. Reliability will be assessed with coefficient alpha, omega, and composite reliability at accepted levels. Measurement invariance across role and usage frequency groups will be tested before comparing structural paths (Hinkin, 1998; DeVellis, 2017; Venkatesh et al., 2012).
5.5 Framework graphic
The framework graphic presents a left to right construct level view of AIRS. On the right, Behavioral Intention or Readiness is the outcome. Three construct groups feed that outcome.
First, the UTAUT2 core appears in the center column: performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, habit, and voluntariness of use. Each core construct is modeled with a positive path to Behavioral Intention or Readiness.
Second, the AI specific enablers sit above the core. Perceived Explainability has a positive path to Trust in AI and a separate positive path to Behavioral Intention or Readiness. Trust in AI has a positive path to Behavioral Intention or Readiness.
Third, the AI specific inhibitors sit below the core. Perceived Ethical Risk and AI Related Anxiety each have a negative path to Behavioral Intention or Readiness.
Contextual moderators are shown on the left as influences on key paths rather than as direct predictors. Role moderates effects associated with perceived ethical risk and with the inhibitor block more generally. AI Usage Frequency moderates the effects of habit and AI related anxiety. Voluntariness moderates the effects of social influence and facilitating conditions. The graphic conveys direction and locus of effects only. It does not include items, indicators, or error terms, and it is intended to anchor the structural tests specified in the Methodology.





 
6. Hypotheses
H1. UTAUT2 Core Constructs and AI Adoption
The core constructs of UTAUT2—performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, habit, and voluntariness of use—will significantly predict AI adoption readiness among enterprise employees (Venkatesh et al., 2012).
H2. AIRS-Specific Constructs and AI Adoption
The AI-specific constructs of the AIRS framework—trust in AI, explainability, perceived ethical risk, and AI-related anxiety—will significantly predict AI adoption readiness, beyond the explanatory power of UTAUT2 (Langer et al., 2023; Shin, 2021).
H3. Integrated Predictive Validity of AIRS
The combined AIRS model (UTAUT2 plus AI-specific constructs) will explain greater variance in AI adoption readiness than UTAUT2 alone, supporting the theoretical value of extending technology acceptance models for enterprise AI contexts (Dwivedi et al., 2021).
H4. Moderating Effects of Contextual Variables
The relationships between predictors (UTAUT2 and AI-specific constructs) and AI adoption readiness will be moderated by contextual factors, including employee role, AI tool usage frequency, and business unit affiliation (Dwivedi et al., 2021).
H5. Mediation Mechanisms
The relationships between AI-specific constructs and behavioral intention will be mediated by trust in AI and AI anxiety, such that:
H5a. Trust in AI will mediate the relationship between explainability and behavioral intention, with greater explainability increasing trust, which in turn increases adoption intention (Shin, 2021; Langer et al., 2023).
H5b. Trust in AI will mediate the relationship between perceived ethical risk and behavioral intention, with higher ethical risk decreasing trust, which in turn decreases adoption intention (Floridi et al., 2018; Dwivedi et al., 2021).
H5c. AI anxiety will mediate the relationship between perceived ethical risk and behavioral intention, with higher ethical risk increasing anxiety, which in turn decreases adoption intention (Tao et al., 2020/2021; Kim et al., 2025).

7. Methodology
7.1 Design and justification
This study uses a quantitative cross sectional survey to develop and validate the Artificial Intelligence Readiness Score for enterprise contexts. The design supports scale development, measurement validation, and structural testing against a baseline model from UTAUT2 (Venkatesh et al., 2012). Psychometric procedures follow established guidance for construct development and validation, including exploratory and confirmatory factor analyses and tests of reliability and validity (Hinkin, 1998; DeVellis, 2017).
7.2 Population, sampling, and sample size
The target population is adults in the United States who are either students or employed professionals. A professional panel provider will recruit respondents and field the instrument. The goal is approximately 500 valid completes to support split sample validation and structural modeling with multiple latent constructs. A balanced composition of students and professionals will enable moderation tests by role and by AI usage frequency. For employed respondents, business unit will be captured for additional moderation. Screening will verify age, residence in the United States, and student or employment status.
Students are included because they are transitioning into an AI infused job market and will soon make adoption decisions inside organizations; their readiness is therefore directly relevant to near-term enterprise contexts.
7.3 Instrument
The survey instrument operationalizes the UTAUT2 core constructs and four AI specific constructs that function as enablers and inhibitors. All agreement items use a five point Likert scale from strongly disagree to strongly agree. AI tool usage frequency items use a five point scale from never to daily. Demographics capture role, employment status, industry or field, business unit for employed respondents, education, and years of experience. The full instrument appears in the appendix. Content validity is grounded in the acceptance literature and contemporary work on trust, explainability, risk, and anxiety in AI adoption (Venkatesh et al., 2012; Floridi et al., 2018; Shin, 2021; Langer et al., 2023).
7.4 Data collection procedures
The panel provider will program and host the questionnaire and recruit U.S. students and professionals until the target N is reached. No separate pilot will be conducted. A brief cognitive pretest will be conducted prior to launch. Instead, real time quality monitoring will be used during fielding, with daily review of in progress data and replacement of low quality cases. Respondents will be presented with an information sheet and consent language before participation.

7.5 Data quality and response integrity
Vendor controls will include bot and duplicate prevention, device and IP checks, speeding and straightlining detection, at least two instructed response items, and open ended validation. Researcher controls will include daily checks for speeders, long string and intra matrix variance, duplicate fingerprints, and cross item logic. Exclusions will be documented in an audit log. Item order will be preserved for factor analyses. Reverse coded items will be handled at analysis time. These steps follow recommended design safeguards for survey research and measurement reliability (Dillman et al., 2014; DeVellis, 2017; Fowler, 2014).
7.6 Operationalization table
Hypothesis	Construct Family	Measure	Scale	Validity and Reliability Checks	Data Source	Planned Analysis
H1	UTAUT2 Core Constructs	AIRS Instrument	Five-point agreement	EFA and CFA for dimensionality; Cronbach’s alpha, McDonald’s omega, composite reliability, AVE, HTMT	U.S. students and professionals	Exploratory and confirmatory factor analyses (EFA → CFA); baseline SEM model
H2	AI-Specific Constructs	AIRS Instrument	Five-point agreement	Same as above	Same	Structural equation modeling (SEM) to test direct effects on AI readiness
H3	Integrated Predictive Validity	AIRS Instrument	Five-point agreement	Same as above	Same	Nested model comparison: UTAUT2 baseline vs. AIRS extended model; incremental R² and fit indices
H4	Contextual Moderators	Role, business unit, and AI usage frequency	Single-select and five-point frequency	Measurement invariance across groups	Same	Multi-group SEM and interaction effects
H5	Mediation Mechanisms	AIRS Instrument (EX, TR, ER, AX, BI)	Five-point agreement	Same as above	Same	Bootstrap mediation analysis testing indirect effects: EX → TR → BI, ER → TR → BI, ER → AX → BI
Model Comparison (H1–H3)	UTAUT2 Baseline vs. AIRS Extended Model	Modeled Constructs	As above	Model fit and reliability at accepted thresholds (CFI/TLI ≥ .90, RMSEA ≤ .08)	Same	Nested model comparison: UTAUT2 baseline vs. AIRS extended model

7.7 Analysis plan
Data screening. Examine missingness, response time distributions, and quality flags before factor work. Exclude cases that fail hard checks and report exclusions.
Exploratory factor analysis. Conduct EFA on a development split to assess dimensionality. Use polychoric correlations with an estimator appropriate for ordered categorical data. Retain items with primary loadings at or above .50 and minimal cross loadings. Adequacy will be assessed with KMO at or above .60 and a significant Bartlett test (Hinkin, 1998; DeVellis, 2017).
Confirmatory factor analysis. Test the measurement model on a holdout split. Target standardized loadings at or above .50, composite reliability at or above .70, and average variance extracted at or above .50. Assess fit with CFI and TLI at or above .90 and RMSEA at or below .08. Evaluate discriminant validity with Fornell–Larcker criteria and HTMT.
Measurement invariance. Test configural, metric, and scalar invariance across role and usage frequency groups before comparing structural paths.
Structural modeling. Compare a UTAUT2 baseline with the AIRS extended model. Estimate direct effects from UTAUT2 constructs and from explainability, trust, ethical risk, and AI related anxiety to readiness. Evaluate incremental validity by changes in explained variance and improvements in fit indices.
Mediation analysis. Test indirect effects using bootstrap procedures with bias-corrected confidence intervals (k=5000 iterations). Specifically test: (H5a) explainability → trust → readiness, (H5b) ethical risk → trust → readiness, and (H5c) ethical risk → anxiety → readiness. Decompose total, direct, and indirect effects for each pathway (Preacher & Hayes, 2008).
Moderation. Test interactions or multi group differences for role, AI usage frequency, and business unit where applicable, consistent with the hypotheses and conceptual framework (Venkatesh et al., 2012; Dwivedi et al., 2021).


7.8 Feasibility
The panel provider can reach both student and professional segments in the United States. The target N of approximately 500 supports a split for EFA and CFA, stable parameter estimation for multi construct SEM, and planned moderation tests. The instrument, quality controls, and analysis plan are aligned with accepted practice for scale development and structural testing in information systems research (Hinkin, 1998; DeVellis, 2017; Venkatesh et al., 2012).
7.9 Protections and data management
Participation is voluntary. Respondents will be informed about the study purpose, data use, and confidentiality. The vendor will de identify data before delivery. No directly identifying information will be included in the analysis dataset. Data will be stored on secure drives with access restricted to the research team. Findings will be reported in aggregate. The study will seek institutional review board approval prior to fielding and will follow ethical guidance for survey research and organizational studies (Dillman et al., 2014; Fowler, 2014).

8. Limitations, Biases, and Significance
8.1 Limitations and potential biases
Cross sectional design. The design captures associations at one time point and does not establish causal direction among constructs (Venkatesh et al., 2012).
Sampling frame. The study recruits U.S. students and professionals through a panel provider. This frame may not represent employees in specific enterprises or regulated sectors, which can limit generalizability to some organizational contexts (Fowler, 2014). Although students may not yet occupy enterprise roles, we include them intentionally as near-term entrants to an AI infused job market and analyze students and professionals separately to preserve interpretability and examine generalization boundaries.
Self report and single source data. Perceptions of usefulness, trust, explainability, ethical risk, and anxiety are self reported and collected in one instrument, which increases the risk of common method variance and social desirability bias (Dillman et al., 2014).
Measurement risks. Constructs that sit near one another conceptually, especially explainability and general transparency, can show cross loading or weak discriminant validity if wording drifts. Anxiety and ethical risk can also overlap if items are not disciplined to affect versus anticipated harm domains (Shin, 2021; Langer et al., 2023).
Model complexity. Adding multiple latent constructs raises the risk of overfitting in small subsamples and places demands on fit and reliability thresholds during confirmatory analysis (Hinkin, 1998; DeVellis, 2017).
Nonresponse and data quality. Panel studies can face nonresponse patterns, speeding, straightlining, and duplicate attempts. Although quality controls are planned, residual noise may remain and can attenuate estimated effects (Dillman et al., 2014; Fowler, 2014).
8.2 Mitigation strategies
Design safeguards. Separate predictor and outcome sections, use consistent anchors within blocks, provide plain term definitions, and include confidentiality language to reduce demand characteristics and satisficing (Dillman et al., 2014).
Statistical checks. Apply marker or latent method factor probes for common method variance. Use polychoric correlations and an estimator suitable for ordered responses. Report alpha, omega, composite reliability, average variance extracted, and heterotrait to monotrait ratios. Enforce loading and cross loading rules in exploratory work and confirmatory fit targets such as CFI and TLI at or above point nine zero and RMSEA at or below point zero eight (Hinkin, 1998; DeVellis, 2017).
Construct clarity. Keep explainability items focused on reasons for specific outputs and handle transparency in governance language within facilitating conditions. Anchor ethical risk to bias, privacy, and accountability. Anchor anxiety to felt unease and pace of change to preserve discriminant validity (Shin, 2021; Langer et al., 2023; Floridi et al., 2018).
Robust validation. Use split samples for exploratory and confirmatory analyses, test configural, metric, and scalar invariance across role and usage groups, and run sensitivity analyses with and without excluded cases to assess stability of results (Venkatesh et al., 2012).
Preregistration and audit trail. Document exclusion criteria, interim quality checks, and any item edits before analysis to improve transparency and replicability.
8.3 Significance and expected contributions
Theoretical contribution. The study extends UTAUT2 by integrating epistemic and ethical determinants that condition reliance on probabilistic systems. It tests whether trust in artificial intelligence and perceived explainability act as enablers and whether perceived ethical risk and artificial intelligence related anxiety act as inhibitors, providing evidence for a compact and enterprise relevant extension to the acceptance canon (Venkatesh et al., 2012; Shin, 2021; Langer et al., 2023; Dwivedi et al., 2021).
Measurement contribution. AIRS offers a validated readiness instrument with scoring guidance, invariance checks across practical subgroups, and documented reliability and validity, enabling cumulative research on enterprise adoption.
Practical contribution. Findings link specific levers to readiness. Explainability design and communication, governance and privacy safeguards, bias and performance monitoring, and enablement programs that build calibrated trust and reduce anxiety are expected to improve adoption outcomes in accountable settings (Floridi et al., 2018; Dwivedi et al., 2021).

9. Conclusion
This proposal advances a practical, theory grounded framework for assessing enterprise readiness to adopt artificial intelligence. The Artificial Intelligence Readiness Score retains the validated UTAUT2 core and extends it with trust in AI, perceived explainability, perceived ethical risk, and AI related anxiety. The design uses a cross sectional survey of approximately 500 U.S. students and professionals and a split sample validation strategy with exploratory and confirmatory factor analyses, followed by structural comparisons of a UTAUT2 baseline and the AIRS model. The expected contributions are a validated readiness instrument, evidence of incremental validity beyond UTAUT2, and actionable guidance that links explainability and governance to adoption outcomes (Venkatesh et al., 2012; Floridi et al., 2018; Shin, 2021; Langer et al., 2023; Dwivedi et al., 2021).
I respectfully request the committee’s approval to proceed. Approval will enable timely fielding and ensure that the resulting evidence can inform research and practice on responsible and effective AI adoption in organizational settings.

 
References
Ajzen, I. (1991). The theory of planned behavior. Organizational Behavior and Human Decision Processes, 50(2), 179–211. https://doi.org/10.1016/0749-5978(91)90020-T
Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319–340. https://doi.org/10.2307/249008
DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). SAGE.
Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). Internet, phone, mail, and mixed-mode surveys: The tailored design method (4th ed.). Wiley.
Doshi-Velez, F., & Kim, B. (2017). Toward a rigorous science of interpretable machine learning. arXiv. https://doi.org/10.48550/arXiv.1702.08608
Dwivedi, Y. K., Hughes, L., Ismagilova, E., Aarts, G., Coombs, C., Crick, T., Duan, Y., Dwivedi, R., Janssen, M., Kar, A. K., Kizgin, H., Kronemann, B., Medaglia, R., Meunier-FitzHugh, K. L., Misra, S. K., Mogaji, E., Sharma, S. K., Singh, J. B., Raghavan, V., … Williams, M. D. (2021). Artificial intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. International Journal of Information Management, 57, 101994. https://doi.org/10.1016/j.ijinfomgt.2019.08.002
Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., Luetge, C., Madelin, R., Pagallo, U., Rossi, F., Schafer, B., Valcke, P., & Vayena, E. (2018). AI4People: An ethical framework for a good AI society—Opportunities, risks, principles, and recommendations. Minds and Machines, 28, 689–707. https://doi.org/10.1007/s11023-018-9482-5
Fowler, F. J., Jr. (2014). Survey research methods (5th ed.). SAGE.
Gansser, O. A., & Reich, C. S. (2021). A new acceptance model for artificial intelligence with extensions to UTAUT2: An empirical study in three segments of application. Technology in Society, 65, 101535. https://doi.org/10.1016/j.techsoc.2021.101535
Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM Computing Surveys, 51(5), Article 93. https://doi.org/10.1145/3236009
Hinkin, T. R. (1998). A brief tutorial on the development of measures for use in survey questionnaires. Organizational Research Methods, 1(1), 104–121. https://doi.org/10.1177/109442819800100106
Kim, J. J. H., Soh, J., Kadkol, S., Solomon, I., Yeh, H., Srivatsa, A. V., … Ajilore, O. (2025). AI anxiety: A comprehensive analysis of psychological factors and interventions. AI and Ethics, 5, 3993–4009. https://doi.org/10.1007/s43681-025-00686-9
Langer, M., König, C. J., Back, C., & Hemsing, V. (2023). Trust in artificial intelligence: Comparing trust processes between human and automated trustees in light of unfair bias. Journal of Business and Psychology, 38, 493–508. https://doi.org/10.1007/s10869-022-09829-9
Li, J., & Huang, J.-S. (2020). Dimensions of artificial intelligence anxiety based on the integrated fear acquisition theory. Technology in Society, 63, 101410. https://doi.org/10.1016/j.techsoc.2020.101410
Moore, G. C., and Benbasat, I. (1991). Development of an instrument to measure the perceptions of adopting an information technology innovation. Information Systems Research, 2(3), 192–222. https://doi.org/10.1287/isre.2.3.192
Rogers, E. M. (2003). Diffusion of innovations (5th ed.). Free Press.
Salih, L., Tarhini, A., & Acikgoz, F. (2025). AI-Enabled Service Continuance: Roles of Trust and Privacy Risk. Journal of Computer Information Systems, 1–16. https://doi.org/10.1080/08874417.2025.2452544
Shin, D. (2021). The effects of explainability and causability on trust and acceptance of AI. International Journal of Human-Computer Studies, 146, 102551. https://doi.org/10.1016/j.ijhcs.2020.102551
Siau, K., & Wang, W. (2018). Building trust in artificial intelligence, machine learning, and robotics. Cutter Business Technology Journal, 31(2), 47–53.
Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 27(3), 425–478. https://doi.org/10.2307/30036540
Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. MIS Quarterly, 36(1), 157–178. https://doi.org/10.2307/41410412
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, A., Balle, B., Kasirzadeh, A., Brown, S., Kenton, Z., Hawkins, W., Steinhardt, J., & Gabriel, I. (2022). Ethical and social risks of harm from language models. arXiv. https://arxiv.org/abs/2112.04359
 
Appendix A. AI Readiness Score (AIRS) Survey Instrument (reduced)
Instructions to participants
Please indicate your level of agreement with each statement based on your current work or study experience.
Scale: 1 = Strongly disagree | 2 = Disagree | 3 = Neutral | 4 = Agree | 5 = Strongly agree
________________________________________
Section 1. UTAUT2 core constructs (Hypothesis H1)
Performance Expectancy (PE)
1.	AI tools help me accomplish tasks more quickly.
2.	Using AI improves the quality of my work or studies.
Effort Expectancy (EE)
5.	Learning to use AI tools is easy for me.
6.	Interacting with AI tools is clear and understandable.
Social Influence (SI)
9.	People whose opinions I value encourage me to use AI tools.
10.	Leaders in my organization or school support the use of AI tools.
Facilitating Conditions (FC)
13.	I have access to training or tutorials for the AI tools I use.
14.	The AI tools I use are compatible with other tools or systems I use.
Hedonic Motivation (HM)
17.	Using AI tools is stimulating and engaging.
18.	AI tools make my work or studies more interesting.
Price Value (PV)
21.	I get more value from AI tools than the effort they require.
22.	Using AI tools is worth the learning curve.
Habit (HB)
25.	Using AI tools has become a habit for me.
26.	I tend to rely on AI tools by default when I need help with tasks.
Voluntariness (VO)
1.	I choose to use AI tools in my work because I find them helpful, not because I am required to.
2.	I could choose not to use AI tools in my work or studies if I preferred.
________________________________________
Section 2. AIRS AI-specific constructs (Hypothesis H2)
Trust in AI (TR)
29.	I trust AI tools to provide reliable information.
30.	I trust the AI tools that are available to me.
Explainability (EX)
33.	I understand how the AI tools I use generate their outputs.
34.	I prefer AI tools that explain their recommendations.
Perceived Ethical Risk (ER)
37.	I worry that AI tools could replace jobs in my field.
38.	I am concerned about privacy risks when using AI tools.
AI Anxiety (AX)
41.	I feel uneasy about the increasing use of AI.
42.	I worry that I may be left behind if I do not keep up with AI.
________________________________________
Section 3. AI adoption readiness (outcome) (Hypothesis H3)
45.	I am ready to use more AI tools in my work or studies.
46.	I would recommend AI tools to others.
47.	I see AI as an important part of my future.
48.	I plan to increase my use of AI tools in the next six months.
________________________________________
 
Section 4. AI tool usage and demographics (Hypothesis H4)
Usage frequency
Scale: 1 = Never | 2 = Rarely | 3 = Sometimes | 4 = Often | 5 = Daily
50. How often do you use Microsoft 365 Copilot or Microsoft Copilot in your work or studies?
51. How often do you use ChatGPT?
52. How often do you use Google Gemini?
53. How often do you use other AI tools (for example, Claude, Perplexity, Jasper)?
Demographics
54.	What is your highest level of education completed?
• High school or less
• Some college or vocational training
• Bachelor’s degree
• Master’s degree
• Doctoral or professional degree
55.	What is your current status?
• Full-time student
• Part-time student
• Employed — individual contributor
• Employed — manager
• Employed — executive or leader
• Freelancer or self-employed
• Not currently employed
• Other
56.	Which industry or field best describes your primary area of work or study?
• Technology or IT
• Education
• Healthcare
• Finance or Banking
• Manufacturing
• Retail or Hospitality
• Government or Public sector
• Nonprofit
• Other
57.	How many years of work or study experience do you have in your field?
• Less than 1 year
• 1 to 3 years
• 4 to 6 years
• 7 to 10 years
• 11 or more years
58.	Do you identify as a person with a disability (for example, vision, mobility, neurodivergence)?
• Yes
• No
• Prefer not to answer
________________________________________
Notes for scoring and analysis
•	All items use the 1 to 5 agreement scale except the usage items, which use the 1 to 5 frequency scale.
•	Before modeling, screen items with EFA and confirm structure with CFA; compute construct scores as means of retained items.
•	For moderation tests, use usage frequency items and relevant demographic variables as moderators in hierarchical or multi-group models.
 
Appendix B. AIRS Survey Instrument — Centiment Import Format
This appendix provides the AI Readiness Score questionnaire in Centiment import format. Agreement items are grouped by construct into matrix blocks that share the same five-point scale: Strongly disagree, Disagree, Neutral, Agree, Strongly agree. Usage items use a five-point frequency scale: Never, Rarely, Sometimes, Often, Daily. Demographics use single select blocks. One item in Perceived Ethical Risk is reverse-coded and is marked with (R). Maintain item order for factor analysis and scoring.
PREVIEW LINK: https://app.centiment.co/preview/471bc02e-aeac-4ae3-9fb9-b58a4a12c573
 
Appendix C. AIRS Survey Items and Literature mapping for the initial pool
Purpose. This appendix documents how each AI Readiness Score (AIRS) item maps to prior literature. Items are labeled as: Direct (UTAUT/UTAUT2) when the canonical wording is preserved; Adapted (UTAUT/UTAUT2) when context or phrasing is tailored (for example, time/effort substituted for monetary cost); and Adapted (AI literature) when items were crafted from conceptual or review scholarship on trust, explainability, ethical risk, and AI anxiety. Item 40 is reverse-coded.
#	Hypothesis	Hypothetical Construct	AIRS item (final wording)	Provenance & original wording
1	H1	Performance Expectancy	AI tools help me accomplish tasks more quickly.	Direct (UTAUT) — “I find the system useful in my job.” (Venkatesh et al., 2003)
2	H1	Performance Expectancy	Using AI improves the quality of my work or studies.	Direct (UTAUT) — “Using [system] improves my job performance.” (Venkatesh et al., 2003)
3	H1	Performance Expectancy	AI tools enhance my productivity.	Direct (UTAUT) — “Using [system] increases my productivity.” (Venkatesh et al., 2003)
4	H1	Performance Expectancy	AI tools make me more effective on complex tasks.	Direct (UTAUT) — “Using [system] enhances my effectiveness on the job.” (Venkatesh et al., 2003)
5	H1	Effort Expectancy	Learning to use AI tools is easy for me.	Direct (UTAUT) — “Learning to operate the system is easy for me.” (Venkatesh et al., 2003)
6	H1	Effort Expectancy	Interacting with AI tools is clear and understandable.	Direct (UTAUT) — “My interaction with the system is clear and understandable.” (Venkatesh et al., 2003)
7	H1	Effort Expectancy	It takes little effort for me to become skillful at using AI tools.	Direct (UTAUT) — “It would be easy for me to become skillful at using the system.” (Venkatesh et al., 2003)
8	H1	Effort Expectancy	I can quickly learn how to use new AI tools.	Direct (TAM) — “I find the system easy to use.” (Davis, 1989)
9	H1	Social Influence	People who influence my work or studies think I should use AI tools.	Direct (UTAUT) — “People who influence my behavior think that I should use the system.” (Venkatesh et al., 2003)
10	H1	Social Influence	People whose opinions I value encourage me to use AI tools.	Direct (UTAUT) — “People whose opinions I value prefer that I use the system.” (Venkatesh et al., 2003)
11	H1	Social Influence	Leaders in my organization or school support the use of AI tools.	Adapted (UTAUT) — reflects organizational leadership support wording aligned with UTAUT SI domain.
12	H1	Social Influence	Using AI tools is viewed as normal in my organization or school.	Adapted (UTAUT) — rephrased injunctive/descriptive norm; corrects prior misattribution to Image (Moore & Benbasat, 1991).
13	H1	Facilitating Conditions	I have the resources I need to use AI tools effectively.	Direct (UTAUT) — “I have the resources necessary to use the system.” (Venkatesh et al., 2003)
14	H1	Facilitating Conditions	A specific person or group is available to assist me when I have difficulties using AI tools.	Direct (UTAUT) — “A specific person (or group) is available for assistance with system difficulties.” (Venkatesh et al., 2003)
15	H1	Facilitating Conditions	I have access to training or tutorials for the AI tools I use.	Adapted (UTAUT) — aligns with FC support knowledge/training dimension.
16	H1	Facilitating Conditions	The AI tools I use are compatible with other tools or systems I use.	Direct (UTAUT) — “The system is compatible with other systems I use.” (Venkatesh et al., 2003)
17	H1	Hedonic Motivation	I enjoy using AI tools.	Direct (UTAUT2) — “Using the system is fun.” (Venkatesh et al., 2012)
18	H1	Hedonic Motivation	Using AI tools is stimulating and engaging.	Direct (UTAUT2) — “Using the system is enjoyable.” (Venkatesh et al., 2012)
19	H1	Hedonic Motivation	AI tools make my work or studies more interesting.	Direct (UTAUT2) — “Using the system is very entertaining.” (Venkatesh et al., 2012)
20	H1	Hedonic Motivation	I feel satisfied when I successfully use AI tools.	Adapted (UTAUT2) — intrinsic enjoyment satisfaction phrasing.
21	H1	Price Value	The benefits of using AI tools are worth the time it takes to learn them.	Adapted (UTAUT2) — substitutes time/effort for monetary cost in organizational/education context.
22	H1	Price Value	Investing my time in learning AI tools is worthwhile.	Adapted (UTAUT2) — as above.
23	H1	Price Value	I get more value from AI tools than the effort they require.	Adapted (UTAUT2) — as above.
24	H1	Price Value	Using AI tools is worth the learning curve.	Adapted (UTAUT2) — as above.
25	H1	Habit	Using AI tools has become a habit for me.	Direct (UTAUT2) — “The use of the system has become a habit for me.” (Venkatesh et al., 2012)
26	H1	Habit	I use AI tools without having to think about it.	Adapted (UTAUT2) — neutral habit phrasing (avoids “addicted”).
27	H1	Habit	I tend to rely on AI tools by default when I need help with tasks.	Adapted (UTAUT2) — reliance default phrasing.
28	H1	Habit	Using AI tools has become natural to me.	Direct (UTAUT2) — “Using the system has become natural to me.” (Venkatesh et al., 2012)
29	H2	Trust in AI	I trust AI tools to provide reliable information.	Adapted (AI literature) — trust in algorithmic outputs (e.g., Siau & Wang, 2018; Langer et al., 2023).
30	H2	Trust in AI	I believe AI tools can make unbiased decisions.	Adapted (AI literature) — unbiasedness as trust antecedent (e.g., Langer et al., 2023).
31	H2	Trust in AI	I feel confident relying on AI outputs.	Adapted (AI literature) — confidence in outputs (e.g., Shin, 2021).
32	H2	Trust in AI	I trust the AI tools that are available to me.	Adapted (AI literature) — context-specific trust.
33	H2	Explainability	I understand how the AI tools I use generate their outputs.	Adapted (AI literature) — XAI understanding (Doshi Velez & Kim, 2017).
34	H2	Explainability	The reasoning behind AI recommendations is clear to me.	Adapted (AI literature) — transparency/clarity (Shin, 2021).
35	H2	Explainability	I prefer AI tools that explain their recommendations.	Adapted (AI literature) — user preference for explanations (Guidotti et al., 2018).
36	H2	Explainability	When I understand how an AI tool works, I am more likely to use it.	Adapted (AI literature) — explanation–adoption link.
37	H2	Perceived Ethical Risk	I worry that AI tools could replace jobs in my field.	Adapted (AI literature) — job displacement risk (e.g., Floridi et al., 2018).
38	H2	Perceived Ethical Risk	I am concerned about privacy risks when using AI tools.	Adapted (AI literature) — privacy/data governance concerns.
39	H2	Perceived Ethical Risk	I believe AI tools could introduce bias into important decisions.	Adapted (AI literature) — fairness bias risk (e.g., Weidinger et al., 2022).
40	H2	Perceived Ethical Risk	Organizations and schools generally manage AI risks responsibly. (R)	Adapted (AI literature) — governance responsibility; reverse coded for ER.
41	H2	AI Anxiety	I feel uneasy about the increasing use of AI.	Adapted (AI literature) — AI anxiety affect (Kim et al., 2025).
42	H2	AI Anxiety	I worry that I may be left behind if I do not keep up with AI.	Adapted (AI literature) — obsolescence anxiety (Kim et al., 2025).
43	H2	AI Anxiety	I sometimes feel uncomfortable relying on AI tools.	Adapted (AI literature) — discomfort reliance (related to trust literature).
44	H2	AI Anxiety	The speed at which AI is evolving makes me anxious.	Adapted (AI literature) — pace-of-change anxiety.
45	H2	AI Adoption Readiness	I am ready to use more AI tools in my work or studies.	Adapted (UTAUT) — from Behavioral Intention/usage readiness (Venkatesh et al., 2003).
46	H2	AI Adoption Readiness	I am open to learning about new AI technologies.	Adapted (UTAUT) — intention to use/learn framing.
47	H2	AI Adoption Readiness	I would recommend AI tools to others.	Adapted (TAM extensions) — recommendation intention proxy.
48	H2	AI Adoption Readiness	I see AI as an important part of my future.	Adapted (UTAUT) — forward-looking intention/value.
49	H2	AI Adoption Readiness	I plan to increase my use of AI tools in the next six months.	Adapted (UTAUT) — time-bounded behavioral intention.

Notes
•	Item 40 is reverse coded for ER scoring.
•	PV items explicitly operationalize “price” as time/effort to fit education and organizational contexts.
•	TR, EX, ER, and AX items derive from conceptual/review literature and will be empirically screened via EFA and confirmed via CFA before inclusion in the structural model.

 
Appendix D: Research Timetable (32 Weeks)
The following timetable outlines the major milestones and sequencing of activities for the proposed study across a 32-week period. The schedule demonstrates feasibility by mapping each phase—from proposal defense to IRB approval, data collection, analysis, and final write-up—onto a structured progression. Activities are staged to allow for committee review, ethical oversight, and data quality procedures while maintaining alignment with TUW program gates. The combination of a detailed table and a Gantt chart provides both granular and visual perspectives on the workflow, supporting transparent planning and accountability.
Table D1 - Timetable
Weeks	Milestone / Activity
1–2	Finalize research questions, abstract, and proposal sections; submit draft to Chair.
3–6	Conduct literature synthesis; refine theoretical framework; prepare framework diagram.
7–8	Develop and pretest survey instrument; finalize operationalization table.
9–10	Committee review and revisions; prepare for proposal defense.
11–12	Proposal defense; revisions as directed by committee.
13–14	Submit IRB application; secure approval (expedited/exempt anticipated).
15–18	Program and launch survey through panel provider; begin data collection.
19–22	Complete data collection; screen dataset; document exclusions.
23–24	Conduct exploratory factor analysis (EFA); refine measurement model.
25–26	Conduct confirmatory factor analysis (CFA); assess validity and reliability.
27–28	Estimate structural models; test mediation and moderation hypotheses.
29–30	Draft results section with APA tables and figures.
31	Draft discussion and conclusion sections; integrate limitations and significance.
32	Finalize manuscript; submit full draft to committee; prepare for defense.



 
Appendix E. Bias and Limitation Matrix
Bias and Limitation Matrix for the AIRS Study
Source of Bias / Limitation	Likelihood	Mitigation Strategy	Residual Risk	Reporting Plan
Cross-sectional design – associations only, not causality	High	Acknowledge temporal limits; frame findings as correlational	Medium	Explicit in Limitations; suggest longitudinal follow-up
Panel provider sampling – U.S. students & professionals may not represent all enterprise employees	Medium	Balance student/professional subgroups; test invariance by role	Medium	Report subgroup results separately; discuss generalizability
Self-report data – risk of social desirability or common method variance	High	Include instructed response items, reverse-coded items, procedural separation	Medium	Report CMV diagnostics; acknowledge in Limitations
Measurement overlap – constructs like explainability vs. transparency; anxiety vs. risk	Medium	Use validated/adapted items; apply discriminant validity checks (HTMT, Fornell-Larcker)	Low	Report reliability/validity statistics; discuss construct clarity
Nonresponse/data quality – speeding, straightlining, duplicate entries	Medium	Vendor + researcher quality checks; daily audit	Low	Document exclusions in audit log; report sample cleaning
Model complexity – multiple latent constructs may risk overfitting in smaller subsamples	Low–Medium	Split-sample validation; confirmatory fit indices; sensitivity analyses	Low	Report model fit indices and subsample stability
IRB/ethical considerations – privacy and consent compliance	Low	De-identify data; use secure storage; consent language	Low	Describe protections in Method; report aggregate findings

Note. Likelihood ratings reflect relative expectations for survey-based organizational research. Residual risks are those anticipated after mitigation steps.
