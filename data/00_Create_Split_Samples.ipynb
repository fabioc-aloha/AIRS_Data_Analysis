{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1385c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AIRS SAMPLE PREPARATION\n",
      "======================================================================\n",
      "Random seed: 67\n",
      "Output directories created: 5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 0: IMPORTS AND CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Prevent OpenMP conflicts\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 67\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create output directories (relative to data/ folder)\n",
    "# Three populations: Academic (FT students), Professional (IC + Freelancer), Leader (Mgr + Exec)\n",
    "OUTPUT_DIRS = [\n",
    "    '../airs_leader/data',\n",
    "    '../airs_academic/data', \n",
    "    '../airs_professional/data',\n",
    "    '../results/plots',\n",
    "    '../results/tables'\n",
    "]\n",
    "\n",
    "for d in OUTPUT_DIRS:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AIRS SAMPLE PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Output directories created: {len(OUTPUT_DIRS)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4a52a",
   "metadata": {},
   "source": [
    "## 1. Item Semantic Metadata\n",
    "\n",
    "Define all 28 items (24 predictors + 4 BI outcome items) with theoretical metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a81edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITEM SEMANTIC METADATA\n",
      "======================================================================\n",
      "Total items: 28\n",
      "  Predictor items: 24 (12 constructs × 2 items)\n",
      "  Outcome items: 4 (BI1-BI4)\n",
      "\n",
      "Direction counts:\n",
      "  POSITIVE: 25\n",
      "  NEGATIVE: 3\n",
      "\n",
      "Constructs: ['PE', 'EE', 'SI', 'FC', 'HM', 'PV', 'HB', 'VO', 'TR', 'EX', 'ER', 'AX', 'BI']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 1: ITEM SEMANTIC METADATA (28 items: 24 predictors + 4 BI)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Format: (code, construct, hypothesis, question, direction, note, provenance)\n",
    "items_data = [\n",
    "    # UTAUT2 Core Constructs (H1)\n",
    "    ('PE1', 'Performance Expectancy', 'H1', 'AI tools help me accomplish tasks more quickly.', 'POSITIVE', 'Perceived productivity gain', 'Direct (UTAUT)'),\n",
    "    ('PE2', 'Performance Expectancy', 'H1', 'Using AI improves the quality of my work or studies.', 'POSITIVE', 'Perceived quality improvement', 'Direct (UTAUT)'),\n",
    "    ('EE1', 'Effort Expectancy', 'H1', 'Learning to use AI tools is easy for me.', 'POSITIVE', 'Perceived ease of learning', 'Direct (UTAUT)'),\n",
    "    ('EE2', 'Effort Expectancy', 'H1', 'Interacting with AI tools is clear and understandable.', 'POSITIVE', 'Perceived clarity of interaction', 'Direct (UTAUT)'),\n",
    "    ('SI1', 'Social Influence', 'H1', 'People whose opinions I value encourage me to use AI tools.', 'POSITIVE', 'Social encouragement from valued others', 'Direct (UTAUT)'),\n",
    "    ('SI2', 'Social Influence', 'H1', 'Leaders in my organization or school support the use of AI tools.', 'POSITIVE', 'Leadership support and organizational norms', 'Adapted (UTAUT)'),\n",
    "    ('FC1', 'Facilitating Conditions', 'H1', 'I have access to training or tutorials for the AI tools I use.', 'POSITIVE', 'Resource availability and support', 'Adapted (UTAUT)'),\n",
    "    ('FC2', 'Facilitating Conditions', 'H1', 'The AI tools I use are compatible with other tools or systems I use.', 'POSITIVE', 'System compatibility and integration', 'Direct (UTAUT)'),\n",
    "    ('HM1', 'Hedonic Motivation', 'H1', 'Using AI tools is stimulating and engaging.', 'POSITIVE', 'Intrinsic enjoyment and stimulation', 'Direct (UTAUT2)'),\n",
    "    ('HM2', 'Hedonic Motivation', 'H1', 'AI tools make my work or studies more interesting.', 'POSITIVE', 'Enhanced interest and engagement', 'Direct (UTAUT2)'),\n",
    "    ('PV1', 'Price Value', 'H1', 'I get more value from AI tools than the effort they require.', 'POSITIVE', 'Perceived value-effort tradeoff', 'Adapted (UTAUT2) - effort substituted for monetary cost'),\n",
    "    ('PV2', 'Price Value', 'H1', 'Using AI tools is worth the learning curve.', 'POSITIVE', 'Acceptable cost-benefit perception', 'Adapted (UTAUT2) - effort substituted for monetary cost'),\n",
    "    ('HB1', 'Habit', 'H1', 'Using AI tools has become a habit for me.', 'POSITIVE', 'Habitual usage pattern', 'Direct (UTAUT2)'),\n",
    "    ('HB2', 'Habit', 'H1', 'I tend to rely on AI tools by default when I need help with tasks.', 'POSITIVE', 'Default reliance behavior', 'Adapted (UTAUT2)'),\n",
    "    ('VO1', 'Voluntariness', 'H1', 'I choose to use AI tools in my work because I find them helpful, not because I am required to.', 'POSITIVE', 'Autonomous motivation and choice', 'UTAUT2 extension'),\n",
    "    ('VO2', 'Voluntariness', 'H1', 'I could choose not to use AI tools in my work or studies if I preferred.', 'POSITIVE', 'Perceived freedom of choice', 'UTAUT2 extension'),\n",
    "    \n",
    "    # AI-Specific Constructs (H2)\n",
    "    ('TR1', 'Trust in AI', 'H2', 'I trust AI tools to provide reliable information.', 'POSITIVE', 'Trust in reliability and accuracy', 'Adapted (AI literature - Langer et al., 2023; Siau & Wang, 2018)'),\n",
    "    ('TR2', 'Trust in AI', 'H2', 'I trust the AI tools that are available to me.', 'POSITIVE', 'General trust in available AI systems', 'Adapted (AI literature - Langer et al., 2023)'),\n",
    "    ('EX1', 'Explainability', 'H2', 'I understand how the AI tools I use generate their outputs.', 'POSITIVE', 'Perceived transparency and understanding', 'Adapted (AI literature - Doshi-Velez & Kim, 2017)'),\n",
    "    ('EX2', 'Explainability', 'H2', 'I prefer AI tools that explain their recommendations.', 'POSITIVE', 'Preference for explanation and justification', 'Adapted (AI literature - Guidotti et al., 2018; Shin, 2021)'),\n",
    "    ('ER1', 'Perceived Ethical Risk', 'H2', 'I worry that AI tools could replace jobs in my field.', 'NEGATIVE', 'Job threat anxiety (avoidance motivation)', 'Adapted (AI literature - Floridi et al., 2018)'),\n",
    "    ('ER2', 'Perceived Ethical Risk', 'H2', 'I am concerned about privacy risks when using AI tools.', 'NEGATIVE', 'Privacy and data governance concerns', 'Adapted (AI literature - AI ethics)'),\n",
    "    ('AX1', 'AI Anxiety', 'H2', 'I feel uneasy about the increasing use of AI.', 'NEGATIVE', 'Tech-averse anxiety (barrier)', 'Adapted (AI literature - Bendel, 2021)'),\n",
    "    ('AX2', 'AI Anxiety', 'H2', 'I worry that I may be left behind if I do not keep up with AI.', 'POSITIVE', 'FOMO anxiety (approach motivation / catch-up behavior)', 'Adapted (AI literature - obsolescence anxiety)'),\n",
    "    \n",
    "    # Behavioral Intention (Outcome)\n",
    "    ('BI1', 'Behavioral Intention', 'OUTCOME', 'I am ready to use more AI tools in my work or studies.', 'POSITIVE', 'Readiness to adopt', 'Direct (UTAUT2)'),\n",
    "    ('BI2', 'Behavioral Intention', 'OUTCOME', 'I would recommend AI tools to others.', 'POSITIVE', 'Advocacy intention', 'Direct (UTAUT2)'),\n",
    "    ('BI3', 'Behavioral Intention', 'OUTCOME', 'I see AI as an important part of my future.', 'POSITIVE', 'Future integration intention', 'Adapted (UTAUT2)'),\n",
    "    ('BI4', 'Behavioral Intention', 'OUTCOME', 'I plan to increase my use of AI tools in the next six months.', 'POSITIVE', 'Planned usage increase', 'Direct (UTAUT2)'),\n",
    "]\n",
    "\n",
    "# Generate metadata dictionary\n",
    "item_semantic_metadata = {}\n",
    "for code, construct, hypothesis, question, direction, note, provenance in items_data:\n",
    "    if direction == 'POSITIVE':\n",
    "        bi_rel = 'Higher scores → Higher adoption'\n",
    "    elif direction == 'NEGATIVE':\n",
    "        bi_rel = 'Higher scores → LOWER adoption'\n",
    "    else:\n",
    "        bi_rel = 'Outcome variable'\n",
    "    \n",
    "    item_semantic_metadata[code] = {\n",
    "        'item_code': code,\n",
    "        'construct': construct,\n",
    "        'construct_abbr': code[:2],\n",
    "        'hypothesis': hypothesis,\n",
    "        'question_text': question,\n",
    "        'direction': direction,\n",
    "        'expected_BI_relationship': bi_rel if hypothesis != 'OUTCOME' else 'Outcome variable',\n",
    "        'theoretical_note': note,\n",
    "        'provenance': provenance\n",
    "    }\n",
    "\n",
    "# Derive construct groups\n",
    "construct_groups = {}\n",
    "for item_code, meta in item_semantic_metadata.items():\n",
    "    abbr = meta['construct_abbr']\n",
    "    construct_groups.setdefault(abbr, []).append(item_code)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ITEM SEMANTIC METADATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total items: {len(item_semantic_metadata)}\")\n",
    "print(f\"  Predictor items: 24 (12 constructs × 2 items)\")\n",
    "print(f\"  Outcome items: 4 (BI1-BI4)\")\n",
    "print(f\"\\nDirection counts:\")\n",
    "print(f\"  POSITIVE: {sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'POSITIVE')}\")\n",
    "print(f\"  NEGATIVE: {sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'NEGATIVE')}\")\n",
    "print(f\"\\nConstructs: {list(construct_groups.keys())}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6cf01",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fef7f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded: 511 responses × 53 columns\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2: LOAD RAW DATA\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Input file is in same directory (data/)\n",
    "input_path = './AIRS---AI-Readiness-Scale-labels.csv'\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "\n",
    "# Load raw data (skip first 2 metadata rows)\n",
    "df_raw = pd.read_csv(input_path, skiprows=2)\n",
    "\n",
    "print(f\"Raw data loaded: {len(df_raw)} responses × {len(df_raw.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c78c400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns mapped: 39 of 39 expected\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2.1: COLUMN MAPPING\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "column_mapping = {\n",
    "    # Administrative\n",
    "    'Duration (seconds)': 'Duration_seconds',\n",
    "    'Progress': 'Progress',\n",
    "    \n",
    "    # UTAUT2 Core Items (Q4-Q19)\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | AI tools help me accomplish tasks more quickly': 'PE1',\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | Using AI improves the quality of my work or studies': 'PE2',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Learning to use AI tools is easy for me': 'EE1',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Interacting with AI tools is clear and understandable': 'EE2',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | People whose opinions I value encourage me to use AI tools': 'SI1',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | Leaders in my organization or school support the use of AI tools': 'SI2',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | I have access to training or tutorials for the AI tools I use': 'FC1',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | The AI tools I use are compatible with other tools or systems I use': 'FC2',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | Using AI tools is stimulating and engaging': 'HM1',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | AI tools make my work or studies more interesting': 'HM2',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | I get more value from AI tools than the effort they require': 'PV1',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | Using AI tools is worth the learning curve': 'PV2',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | Using AI tools has become a habit for me': 'HB1',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | I tend to rely on AI tools by default when I need help with tasks': 'HB2',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I choose to use AI tools in my work because I find them helpful, not because I am required to': 'VO1',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I could choose not to use AI tools in my work or studies if I preferred.': 'VO2',\n",
    "    \n",
    "    # AI-Specific Items (Q20-Q27)\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust AI tools to provide reliable information': 'TR1',\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust the AI tools that are available to me': 'TR2',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I understand how the AI tools I use generate their outputs': 'EX1',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I prefer AI tools that explain their recommendations': 'EX2',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I worry that AI tools could replace jobs in my field': 'ER1',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I am concerned about privacy risks when using AI tools': 'ER2',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I feel uneasy about the increasing use of AI': 'AX1',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I worry that I may be left behind if I do not keep up with AI': 'AX2',\n",
    "    \n",
    "    # Behavioral Intention / Outcome (Q28-Q31)\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I am ready to use more AI tools in my work or studies': 'BI1',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I would recommend AI tools to others': 'BI2',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I see AI as an important part of my future': 'BI3',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I plan to increase my use of AI tools in the next six months': 'BI4',\n",
    "    \n",
    "    # Usage Frequency\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Microsoft 365 Copilot or Microsoft Copilot': 'Usage_MSCopilot',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | ChatGPT': 'Usage_ChatGPT',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Google Gemini': 'Usage_Gemini',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Other AI tools (for example, Claude, Perplexity, Grok)': 'Usage_Other',\n",
    "    \n",
    "    # Demographics\n",
    "    'What is your current status?': 'Role',\n",
    "    'What is your highest level of education completed?': 'Education',\n",
    "    'Which industry or field best describes your primary area of work or study?': 'Industry',\n",
    "    'How many years of work or study experience do you have in your field?': 'Experience',\n",
    "    'Do you identify as a person with a disability (for example, vision, mobility, neurodivergence)?': 'Disability'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df = df_raw.rename(columns=column_mapping)\n",
    "available_cols = [col for col in column_mapping.values() if col in df.columns]\n",
    "df = df[available_cols].copy()\n",
    "\n",
    "print(f\"Columns mapped: {len(available_cols)} of {len(column_mapping)} expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af19048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types converted:\n",
      "  - 28 Likert items → numeric (1-5)\n",
      "  - 4 Usage items → numeric (1-5)\n",
      "  - Demographics cleaned\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2.2: DATA TYPE CONVERSIONS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Duration to minutes\n",
    "df['Duration_minutes'] = df['Duration_seconds'] / 60\n",
    "df = df.drop('Duration_seconds', axis=1)\n",
    "\n",
    "# Likert items (text to numeric 1-5) - All 28 items\n",
    "likert_items = ['PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2', \n",
    "                'HM1', 'HM2', 'PV1', 'PV2', 'HB1', 'HB2', 'VO1', 'VO2',\n",
    "                'TR1', 'TR2', 'EX1', 'EX2', 'ER1', 'ER2', 'AX1', 'AX2',\n",
    "                'BI1', 'BI2', 'BI3', 'BI4']\n",
    "\n",
    "likert_mapping = {\n",
    "    'Strongly disagree': 1, 'Disagree': 2, 'Neutral': 3, \n",
    "    'Agree': 4, 'Strongly agree': 5\n",
    "}\n",
    "\n",
    "for item in likert_items:\n",
    "    if item in df.columns:\n",
    "        df[item] = df[item].map(likert_mapping)\n",
    "\n",
    "# Usage items (text to numeric 1-5)\n",
    "usage_items = ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
    "usage_mapping = {'Never': 1, 'Rarely': 2, 'Sometimes': 3, 'Often': 4, 'Daily': 5}\n",
    "\n",
    "for item in usage_items:\n",
    "    if item in df.columns:\n",
    "        df[item] = df[item].map(usage_mapping)\n",
    "\n",
    "# Clean demographics\n",
    "for col in ['Role', 'Education', 'Experience', 'Disability', 'Industry']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "print(\"Data types converted:\")\n",
    "print(f\"  - 28 Likert items → numeric (1-5)\")\n",
    "print(f\"  - 4 Usage items → numeric (1-5)\")\n",
    "print(f\"  - Demographics cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2f4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived variables created:\n",
      "  - AI_Adoption: 457 adopters, 54 non-adopters\n",
      "  - Population Distribution:\n",
      "      Academic: N=196\n",
      "      Professional: N=161\n",
      "      Leader: N=130\n",
      "      EXCLUDED ('Other' only): N=24\n",
      "  - Disability_Binary: Yes=69, No=442\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2.3: DERIVED VARIABLES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# AI Adoption (binary)\n",
    "df['AI_Adoption'] = ((df['Usage_MSCopilot'] > 1) | \n",
    "                     (df['Usage_ChatGPT'] > 1) | \n",
    "                     (df['Usage_Gemini'] > 1) | \n",
    "                     (df['Usage_Other'] > 1)).astype(int)\n",
    "\n",
    "# Total usage score\n",
    "df['Total_Usage_Score'] = df[usage_items].sum(axis=1)\n",
    "\n",
    "# Usage Intensity\n",
    "def map_usage_intensity(score):\n",
    "    if score <= 4: return 'Non-User'\n",
    "    elif score <= 8: return 'Low'\n",
    "    elif score <= 12: return 'Medium'\n",
    "    else: return 'High'\n",
    "\n",
    "df['Usage_Intensity'] = df['Total_Usage_Score'].apply(map_usage_intensity)\n",
    "\n",
    "# Population flag - THREE DISTINCT POPULATIONS:\n",
    "# - Academic: Full-time + Part-time students (N=196)\n",
    "# - Professional: Individual Contributors + Freelancers + Not currently employed (N=161)\n",
    "# - Leader: Managers + Executives (N=130)\n",
    "# EXCLUDED: \"Other\" only (N=24)\n",
    "def map_population(role):\n",
    "    if pd.isna(role):\n",
    "        return None  # Exclude\n",
    "    role_lower = str(role).lower().strip()\n",
    "    \n",
    "    # Academic = Full-time + Part-time students\n",
    "    if 'student' in role_lower:\n",
    "        return 'Academic'\n",
    "    # Leader = Managers + Executives\n",
    "    elif 'manager' in role_lower or 'executive' in role_lower or 'leader' in role_lower:\n",
    "        return 'Leader'\n",
    "    # Professional = Individual contributors + Freelancers + Not currently employed\n",
    "    elif 'individual contributor' in role_lower or 'freelancer' in role_lower or 'self employed' in role_lower or 'not currently employed' in role_lower:\n",
    "        return 'Professional'\n",
    "    # Exclude: \"Other\" only\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Population'] = df['Role'].apply(map_population)\n",
    "\n",
    "# Also create a detailed role category for reporting\n",
    "def map_role_category(role):\n",
    "    if pd.isna(role):\n",
    "        return 'Other'\n",
    "    role_lower = str(role).lower().strip()\n",
    "    if 'full time student' in role_lower:\n",
    "        return 'FT_Student'\n",
    "    elif 'part time student' in role_lower:\n",
    "        return 'PT_Student'\n",
    "    elif 'individual contributor' in role_lower:\n",
    "        return 'IC'\n",
    "    elif 'manager' in role_lower:\n",
    "        return 'Manager'\n",
    "    elif 'executive' in role_lower or 'leader' in role_lower:\n",
    "        return 'Executive'\n",
    "    elif 'freelancer' in role_lower or 'self employed' in role_lower:\n",
    "        return 'Freelancer'\n",
    "    elif 'not currently employed' in role_lower:\n",
    "        return 'Unemployed'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['Role_Category'] = df['Role'].apply(map_role_category)\n",
    "\n",
    "# Disability (binary: Yes=1, No=0, Prefer not to answer=0)\n",
    "# Per ANALYSIS_PLAN: \"Prefer not to answer\" coded as No (0) to preserve sample size\n",
    "df['Disability_Binary'] = df['Disability'].map({'Yes': 1, 'No': 0, 'Prefer not to answer': 0})\n",
    "\n",
    "# Report population counts\n",
    "pop_counts = df['Population'].value_counts(dropna=False)\n",
    "excluded = df['Population'].isna().sum()\n",
    "\n",
    "print(\"Derived variables created:\")\n",
    "print(f\"  - AI_Adoption: {df['AI_Adoption'].sum()} adopters, {(~df['AI_Adoption'].astype(bool)).sum()} non-adopters\")\n",
    "print(f\"  - Population Distribution:\")\n",
    "for pop, n in pop_counts.items():\n",
    "    if pop is not None:\n",
    "        print(f\"      {pop}: N={n}\")\n",
    "print(f\"      EXCLUDED ('Other' only): N={excluded}\")\n",
    "print(f\"  - Disability_Binary: Yes={df['Disability_Binary'].sum()}, No={(df['Disability_Binary']==0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410a8dc",
   "metadata": {},
   "source": [
    "## 3. Sample Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5baf60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAMPLE CHARACTERISTICS\n",
      "======================================================================\n",
      "\n",
      "Total N: 511\n",
      "\n",
      "1. Population Distribution:\n",
      "   Academic: N=196 (38.4%)\n",
      "   Professional: N=161 (31.5%)\n",
      "   Leader: N=130 (25.4%)\n",
      "\n",
      "2. Role Breakdown:\n",
      "   Full time student: N=180\n",
      "   Employed - individual contributor: N=113\n",
      "   Employed - manager: N=74\n",
      "   Employed - executive or leader: N=56\n",
      "   Freelancer or self employed: N=31\n",
      "   Other: N=24\n",
      "   Not currently employed: N=17\n",
      "   Part time student: N=16\n",
      "\n",
      "3. AI Adoption:\n",
      "   Adopters: 457 (89.4%)\n",
      "   Non-Adopters: 54 (10.6%)\n",
      "\n",
      "4. Disability Status (for H4f/H4g moderation):\n",
      "   No: N=432 (84.5%)\n",
      "   Yes: N=69 (13.5%)\n",
      "   Prefer not to answer: N=10 (2.0%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 3: SAMPLE OVERVIEW\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE CHARACTERISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal N: {len(df)}\")\n",
    "\n",
    "print(\"\\n1. Population Distribution:\")\n",
    "pop_dist = df['Population'].value_counts()\n",
    "for pop, n in pop_dist.items():\n",
    "    print(f\"   {pop}: N={n} ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n2. Role Breakdown:\")\n",
    "role_dist = df['Role'].value_counts()\n",
    "for role, n in role_dist.items():\n",
    "    print(f\"   {role}: N={n}\")\n",
    "\n",
    "print(\"\\n3. AI Adoption:\")\n",
    "print(f\"   Adopters: {df['AI_Adoption'].sum()} ({df['AI_Adoption'].mean()*100:.1f}%)\")\n",
    "print(f\"   Non-Adopters: {(~df['AI_Adoption'].astype(bool)).sum()} ({(1-df['AI_Adoption'].mean())*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. Disability Status (for H4f/H4g moderation):\")\n",
    "dis_dist = df['Disability'].value_counts()\n",
    "for status, n in dis_dist.items():\n",
    "    print(f\"   {status}: N={n} ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f8916",
   "metadata": {},
   "source": [
    "## 4. Save Master Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd138139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASTER DATASET SAVED\n",
      "======================================================================\n",
      "File: data/AIRS_clean.csv\n",
      "Rows: 511\n",
      "Columns: 45\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 4: SAVE MASTER DATASET\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Save master clean dataset (in current data/ directory)\n",
    "df.to_csv('./AIRS_clean.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MASTER DATASET SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: data/AIRS_clean.csv\")\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef60021",
   "metadata": {},
   "source": [
    "## 5. Create Population Splits\n",
    "\n",
    "Generate stratified dev/holdout splits for three populations (all 70/30):\n",
    "- **Academic**: Full-time + Part-time students (N=196) → 70/30 split\n",
    "- **Professional**: Individual Contributors + Freelancers + Unemployed (N=161) → 70/30 split\n",
    "- **Leader**: Managers + Executives (N=130) → 70/30 split (small but enables CFA validation with bootstrap)\n",
    "\n",
    "**Excluded**: \"Other\" only (N=24)\n",
    "\n",
    "**Note**: All holdout samples are relatively small and will require bootstrap estimation in CFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58b7c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POPULATION FILTERING\n",
      "======================================================================\n",
      "Total responses: 511\n",
      "Valid for analysis: 487\n",
      "Excluded: 24\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING STRATIFIED SPLITS (70/30 for all populations)\n",
      "======================================================================\n",
      "\n",
      "LEADER:\n",
      "  Total N: 130\n",
      "  Dev: N=91 (70%) - 3.8 cases/item ❌ Underpowered\n",
      "  Holdout: N=39 (30%) - ⚠️ Bootstrap needed\n",
      "  Saved to: ../airs_leader/data/\n",
      "\n",
      "ACADEMIC:\n",
      "  Total N: 196\n",
      "  Dev: N=137 (70%) - 5.7 cases/item ✓ Good\n",
      "  Holdout: N=59 (30%) - ✓ Adequate\n",
      "  Saved to: ../airs_academic/data/\n",
      "\n",
      "PROFESSIONAL:\n",
      "  Total N: 161\n",
      "  Dev: N=112 (70%) - 4.7 cases/item ⚠️ Marginal\n",
      "  Holdout: N=49 (30%) - ⚠️ Bootstrap needed\n",
      "  Saved to: ../airs_professional/data/\n",
      "\n",
      "======================================================================\n",
      "\n",
      "POPULATION FILTERING\n",
      "======================================================================\n",
      "Total responses: 511\n",
      "Valid for analysis: 487\n",
      "Excluded: 24\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING STRATIFIED SPLITS (70/30 for all populations)\n",
      "======================================================================\n",
      "\n",
      "LEADER:\n",
      "  Total N: 130\n",
      "  Dev: N=91 (70%) - 3.8 cases/item ❌ Underpowered\n",
      "  Holdout: N=39 (30%) - ⚠️ Bootstrap needed\n",
      "  Saved to: ../airs_leader/data/\n",
      "\n",
      "ACADEMIC:\n",
      "  Total N: 196\n",
      "  Dev: N=137 (70%) - 5.7 cases/item ✓ Good\n",
      "  Holdout: N=59 (30%) - ✓ Adequate\n",
      "  Saved to: ../airs_academic/data/\n",
      "\n",
      "PROFESSIONAL:\n",
      "  Total N: 161\n",
      "  Dev: N=112 (70%) - 4.7 cases/item ⚠️ Marginal\n",
      "  Holdout: N=49 (30%) - ⚠️ Bootstrap needed\n",
      "  Saved to: ../airs_professional/data/\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 5: STRATIFIED SPLITS FOR ALL POPULATIONS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_stratified_split(data, test_size, stratify_cols, random_seed, population_name):\n",
    "    \"\"\"\n",
    "    Create stratified train/test split with fallback for small strata.\n",
    "    \"\"\"\n",
    "    # Create stratification key\n",
    "    data = data.copy()\n",
    "    data['_strat_key'] = data[stratify_cols].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    # Check for small strata\n",
    "    strat_counts = data['_strat_key'].value_counts()\n",
    "    min_count = strat_counts.min()\n",
    "    \n",
    "    if min_count < 2:\n",
    "        print(f\"  ⚠️ {population_name}: Some strata have <2 samples, using AI_Adoption only\")\n",
    "        data['_strat_key'] = data['AI_Adoption'].astype(str)\n",
    "    \n",
    "    try:\n",
    "        dev, holdout = train_test_split(\n",
    "            data,\n",
    "            test_size=test_size,\n",
    "            stratify=data['_strat_key'],\n",
    "            random_state=random_seed\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"  ⚠️ {population_name}: Stratification failed, using random split\")\n",
    "        dev, holdout = train_test_split(\n",
    "            data,\n",
    "            test_size=test_size,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    # Remove temp column\n",
    "    dev = dev.drop('_strat_key', axis=1)\n",
    "    holdout = holdout.drop('_strat_key', axis=1)\n",
    "    \n",
    "    return dev, holdout\n",
    "\n",
    "# Filter to valid populations only (exclude None)\n",
    "df_valid = df[df['Population'].notna()].copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POPULATION FILTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total responses: {len(df)}\")\n",
    "print(f\"Valid for analysis: {len(df_valid)}\")\n",
    "print(f\"Excluded: {len(df) - len(df_valid)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Population configurations - ALL THREE POPULATIONS USE 70/30 SPLIT\n",
    "# This ensures consistent methodology and enables CFA cross-validation for all\n",
    "populations = {\n",
    "    'leader': {\n",
    "        'filter': df_valid['Population'] == 'Leader',\n",
    "        'test_size': 0.30,  # 70/30 split (small but enables CFA with bootstrap)\n",
    "        'output_dir': '../airs_leader/data'\n",
    "    },\n",
    "    'academic': {\n",
    "        'filter': df_valid['Population'] == 'Academic',\n",
    "        'test_size': 0.30,  # 70/30 split\n",
    "        'output_dir': '../airs_academic/data'\n",
    "    },\n",
    "    'professional': {\n",
    "        'filter': df_valid['Population'] == 'Professional',\n",
    "        'test_size': 0.30,  # 70/30 split\n",
    "        'output_dir': '../airs_professional/data'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "split_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING STRATIFIED SPLITS (70/30 for all populations)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pop_name, config in populations.items():\n",
    "    print(f\"\\n{pop_name.upper()}:\")\n",
    "    \n",
    "    # Filter data\n",
    "    pop_data = df_valid[config['filter']].copy()\n",
    "    print(f\"  Total N: {len(pop_data)}\")\n",
    "    \n",
    "    # Create split\n",
    "    dev, holdout = create_stratified_split(\n",
    "        pop_data,\n",
    "        test_size=config['test_size'],\n",
    "        stratify_cols=['AI_Adoption'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "        population_name=pop_name\n",
    "    )\n",
    "    \n",
    "    # Save files\n",
    "    dev.to_csv(f\"{config['output_dir']}/AIRS_dev.csv\", index=False)\n",
    "    holdout.to_csv(f\"{config['output_dir']}/AIRS_holdout.csv\", index=False)\n",
    "    \n",
    "    dev_pct = len(dev) / len(pop_data) * 100\n",
    "    holdout_pct = len(holdout) / len(pop_data) * 100\n",
    "    \n",
    "    # Calculate cases per item for EFA adequacy\n",
    "    n_items = 24\n",
    "    cases_per_item = len(dev) / n_items\n",
    "    efa_status = \"✓ Good\" if cases_per_item >= 5 else \"⚠️ Marginal\" if cases_per_item >= 4 else \"❌ Underpowered\"\n",
    "    cfa_status = \"✓ Adequate\" if len(holdout) >= 50 else \"⚠️ Bootstrap needed\"\n",
    "    \n",
    "    print(f\"  Dev: N={len(dev)} ({dev_pct:.0f}%) - {cases_per_item:.1f} cases/item {efa_status}\")\n",
    "    print(f\"  Holdout: N={len(holdout)} ({holdout_pct:.0f}%) - {cfa_status}\")\n",
    "    print(f\"  Saved to: {config['output_dir']}/\")\n",
    "    \n",
    "    split_results[pop_name] = {\n",
    "        'total': len(pop_data),\n",
    "        'dev': len(dev),\n",
    "        'holdout': len(holdout),\n",
    "        'dev_df': dev,\n",
    "        'holdout_df': holdout\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2e338",
   "metadata": {},
   "source": [
    "## 6. Validate Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7698e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BALANCE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "LEADER:\n",
      "  AI_Adoption: p=1.0000 ✓ Balanced\n",
      "  Disability: p=1.0000 ✓ Balanced\n",
      "  Mean item difference: 0.122 scale points\n",
      "  ✓ Good balance\n",
      "\n",
      "ACADEMIC:\n",
      "  AI_Adoption: p=1.0000 ✓ Balanced\n",
      "  Disability: p=0.3472 ✓ Balanced\n",
      "  Mean item difference: 0.172 scale points\n",
      "  ✓ Good balance\n",
      "\n",
      "PROFESSIONAL:\n",
      "  AI_Adoption: p=1.0000 ✓ Balanced\n",
      "  Disability: p=1.0000 ✓ Balanced\n",
      "  Mean item difference: 0.110 scale points\n",
      "  ✓ Good balance\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 6: VALIDATION - CHI-SQUARE BALANCE TESTS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_balance(variable, dev, holdout, pop_name):\n",
    "    \"\"\"Chi-square test for balance between dev and holdout.\"\"\"\n",
    "    dev_counts = dev[variable].value_counts().sort_index()\n",
    "    holdout_counts = holdout[variable].value_counts().sort_index()\n",
    "    \n",
    "    # Align indices\n",
    "    all_values = sorted(set(dev_counts.index) | set(holdout_counts.index))\n",
    "    dev_counts = dev_counts.reindex(all_values, fill_value=0)\n",
    "    holdout_counts = holdout_counts.reindex(all_values, fill_value=0)\n",
    "    \n",
    "    contingency = pd.DataFrame({'Dev': dev_counts, 'Holdout': holdout_counts})\n",
    "    \n",
    "    # Only run chi-square if we have enough data\n",
    "    if contingency.min().min() < 1:\n",
    "        return None, \"Insufficient data\"\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        status = \"✓ Balanced\" if p > 0.05 else \"⚠ Imbalanced\"\n",
    "        return p, status\n",
    "    except:\n",
    "        return None, \"Test failed\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BALANCE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pop_name, results in split_results.items():\n",
    "    dev = results['dev_df']\n",
    "    holdout = results['holdout_df']\n",
    "    \n",
    "    print(f\"\\n{pop_name.upper()}:\")\n",
    "    \n",
    "    # Skip validation for populations without holdout\n",
    "    if len(holdout) == 0:\n",
    "        print(\"  ⚠️ No holdout sample - validation not applicable\")\n",
    "        continue\n",
    "    \n",
    "    # Test AI_Adoption balance\n",
    "    p, status = test_balance('AI_Adoption', dev, holdout, pop_name)\n",
    "    if p is not None:\n",
    "        print(f\"  AI_Adoption: p={p:.4f} {status}\")\n",
    "    else:\n",
    "        print(f\"  AI_Adoption: {status}\")\n",
    "    \n",
    "    # Test Disability balance (important for H4f/H4g moderation)\n",
    "    p, status = test_balance('Disability_Binary', dev, holdout, pop_name)\n",
    "    if p is not None:\n",
    "        print(f\"  Disability: p={p:.4f} {status}\")\n",
    "    else:\n",
    "        print(f\"  Disability: {status}\")\n",
    "    \n",
    "    # Item means comparison\n",
    "    predictor_items = ['PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2',\n",
    "                       'HM1', 'HM2', 'PV1', 'PV2', 'HB1', 'HB2', 'VO1', 'VO2',\n",
    "                       'TR1', 'TR2', 'EX1', 'EX2', 'ER1', 'ER2', 'AX1', 'AX2']\n",
    "    \n",
    "    dev_means = dev[predictor_items].mean()\n",
    "    holdout_means = holdout[predictor_items].mean()\n",
    "    mean_diff = (dev_means - holdout_means).abs().mean()\n",
    "    \n",
    "    print(f\"  Mean item difference: {mean_diff:.3f} scale points\")\n",
    "    if mean_diff < 0.10:\n",
    "        print(f\"  ✓ Excellent balance\")\n",
    "    elif mean_diff < 0.20:\n",
    "        print(f\"  ✓ Good balance\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Review recommended\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d322818",
   "metadata": {},
   "source": [
    "## 7. Export Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aaef873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "METADATA EXPORTED\n",
      "======================================================================\n",
      "File: data/airs_28item_complete.json\n",
      "Items: 28 (24 predictors + 4 outcomes)\n",
      "Constructs: 13\n",
      "Positive items: 25\n",
      "Negative items: 3\n",
      "\n",
      "Population splits:\n",
      "  leader: N=130 (dev=91, holdout=39)\n",
      "  academic: N=196 (dev=137, holdout=59)\n",
      "  professional: N=161 (dev=112, holdout=49)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 7: EXPORT COMPREHENSIVE METADATA\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "from datetime import datetime\n",
    "\n",
    "# Build export structure\n",
    "airs_export = {\n",
    "    'created': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'source': 'data/00_Create_Split_Samples.ipynb',\n",
    "    'description': 'Complete AIRS item metadata including 24 predictors + 4 BI outcome items',\n",
    "    'n_predictor_items': len([k for k in item_semantic_metadata.keys() if not k.startswith('BI')]),\n",
    "    'n_outcome_items': len([k for k in item_semantic_metadata.keys() if k.startswith('BI')]),\n",
    "    'n_total_items': len(item_semantic_metadata),\n",
    "    \n",
    "    # Population summaries\n",
    "    'populations': {\n",
    "        pop: {\n",
    "            'n_total': results['total'],\n",
    "            'n_dev': results['dev'],\n",
    "            'n_holdout': results['holdout']\n",
    "        }\n",
    "        for pop, results in split_results.items()\n",
    "    },\n",
    "    \n",
    "    # Item lists\n",
    "    'items': list(item_semantic_metadata.keys()),\n",
    "    'predictor_items': [k for k in item_semantic_metadata.keys() if not k.startswith('BI')],\n",
    "    'outcome_items': [k for k in item_semantic_metadata.keys() if k.startswith('BI')],\n",
    "    \n",
    "    # Full metadata\n",
    "    'metadata': item_semantic_metadata,\n",
    "    \n",
    "    # Construct groupings\n",
    "    'constructs': construct_groups,\n",
    "    \n",
    "    # Direction categorization\n",
    "    'positive_items': [k for k, v in item_semantic_metadata.items() if v['direction'] == 'POSITIVE'],\n",
    "    'negative_items': [k for k, v in item_semantic_metadata.items() if v['direction'] == 'NEGATIVE']\n",
    "}\n",
    "\n",
    "# Save to data/ folder (current directory)\n",
    "export_path = Path('./airs_28item_complete.json')\n",
    "with open(export_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(airs_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"METADATA EXPORTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: data/airs_28item_complete.json\")\n",
    "print(f\"Items: {airs_export['n_total_items']} ({airs_export['n_predictor_items']} predictors + {airs_export['n_outcome_items']} outcomes)\")\n",
    "print(f\"Constructs: {len(airs_export['constructs'])}\")\n",
    "print(f\"Positive items: {len(airs_export['positive_items'])}\")\n",
    "print(f\"Negative items: {len(airs_export['negative_items'])}\")\n",
    "print(\"\\nPopulation splits:\")\n",
    "for pop, info in airs_export['populations'].items():\n",
    "    print(f\"  {pop}: N={info['n_total']} (dev={info['n_dev']}, holdout={info['n_holdout']})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c5b3a",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a691968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOTEBOOK 00 COMPLETE - SAMPLE PREPARATION\n",
      "======================================================================\n",
      "\n",
      "📁 FILES CREATED:\n",
      "\n",
      "  Master Data:\n",
      "    data/AIRS_clean.csv (N=511)\n",
      "\n",
      "  Leader Sample (70/30):\n",
      "    airs_leader/data/AIRS_dev.csv (N=91)\n",
      "    airs_leader/data/AIRS_holdout.csv (N=39)\n",
      "\n",
      "  Academic Sample (70/30):\n",
      "    airs_academic/data/AIRS_dev.csv (N=137)\n",
      "    airs_academic/data/AIRS_holdout.csv (N=59)\n",
      "\n",
      "  Professional Sample (70/30):\n",
      "    airs_professional/data/AIRS_dev.csv (N=112)\n",
      "    airs_professional/data/AIRS_holdout.csv (N=49)\n",
      "\n",
      "  Metadata:\n",
      "    data/airs_28item_complete.json\n",
      "\n",
      "======================================================================\n",
      "✅ READY FOR NOTEBOOK 01 (EFA)\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Run 01_EFA in airs_leader/\n",
      "  2. Run 01_EFA in airs_academic/\n",
      "  3. Run 01_EFA in airs_professional/\n",
      "\n",
      "⚠️ NOTE: All holdout samples are small - use bootstrap CFA estimation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 8: FINAL SUMMARY\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK 00 COMPLETE - SAMPLE PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📁 FILES CREATED:\")\n",
    "print(\"\\n  Master Data:\")\n",
    "print(f\"    data/AIRS_clean.csv (N={len(df)})\")\n",
    "\n",
    "print(\"\\n  Leader Sample (70/30):\")\n",
    "print(f\"    airs_leader/data/AIRS_dev.csv (N={split_results['leader']['dev']})\")\n",
    "print(f\"    airs_leader/data/AIRS_holdout.csv (N={split_results['leader']['holdout']})\")\n",
    "\n",
    "print(\"\\n  Academic Sample (70/30):\")\n",
    "print(f\"    airs_academic/data/AIRS_dev.csv (N={split_results['academic']['dev']})\")\n",
    "print(f\"    airs_academic/data/AIRS_holdout.csv (N={split_results['academic']['holdout']})\")\n",
    "\n",
    "print(\"\\n  Professional Sample (70/30):\")\n",
    "print(f\"    airs_professional/data/AIRS_dev.csv (N={split_results['professional']['dev']})\")\n",
    "print(f\"    airs_professional/data/AIRS_holdout.csv (N={split_results['professional']['holdout']})\")\n",
    "\n",
    "print(\"\\n  Metadata:\")\n",
    "print(f\"    data/airs_28item_complete.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ READY FOR NOTEBOOK 01 (EFA)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 01_EFA in airs_leader/\")\n",
    "print(\"  2. Run 01_EFA in airs_academic/\")\n",
    "print(\"  3. Run 01_EFA in airs_professional/\")\n",
    "print(\"\\n⚠️ NOTE: All holdout samples are small - use bootstrap CFA estimation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fed8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Method Summary (APA Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d5e3496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Participants\n",
       "\n",
       "The sample consisted of *N* = 511 respondents recruited via convenience sampling through professional networks and academic channels. Of these, 24 respondents were excluded from population-specific analyses due to ambiguous role categorization (\"Other\"; *n* = 24).\n",
       "\n",
       "The remaining *N* = 487 respondents were categorized into three distinct populations:\n",
       "\n",
       "1. **Academic** (*n* = 196; 40.2%): Students, including full-time (*n* = 180) and part-time (*n* = 16) students.\n",
       "\n",
       "2. **Professional** (*n* = 161; 33.1%): Working individuals in non-leadership roles, including individual contributors (*n* = 113), freelancers/self-employed (*n* = 31), and those not currently employed (*n* = 17).\n",
       "\n",
       "3. **Leader** (*n* = 130; 26.7%): Individuals in leadership positions, including managers (*n* = 74) and executives/leaders (*n* = 56).\n",
       "\n",
       "Regarding AI adoption, 438 participants (89.9%) reported using at least one AI tool beyond \"never,\" while 49 participants (10.1%) were classified as non-adopters. Disability status was reported as follows: No (*n* = 411; 84.4%), Yes (*n* = 66; 13.6%), and Prefer not to answer (*n* = 10; 2.1%).\n",
       "\n",
       "### Data Preparation\n",
       "\n",
       "Data were prepared for a three-population analytical approach following recommendations for scale development and cross-validation (DeVellis & Thorpe, 2022). Each population was partitioned into development (70%) and holdout (30%) subsets using stratified random sampling with a fixed seed (*seed* = 67) to ensure reproducibility and enable cross-validation of factor structures.\n",
       "\n",
       "**Academic Sample.** The academic population (*N* = 196) yielded development (*n* = 137) and holdout (*n* = 59) samples.\n",
       "\n",
       "**Professional Sample.** The professional population (*N* = 161) yielded development (*n* = 112) and holdout (*n* = 49) samples.\n",
       "\n",
       "**Leader Sample.** The leader population (*N* = 130) yielded development (*n* = 91) and holdout (*n* = 39) samples. Given the smaller holdout size, bootstrap estimation was employed for confirmatory factor analysis to obtain robust standard errors.\n",
       "\n",
       "Stratification was performed on AI adoption status to ensure balanced representation across splits. Chi-square tests confirmed successful stratification across all populations (*p* > .05).\n",
       "\n",
       "### Measures\n",
       "\n",
       "The AI Readiness Scale (AIRS) comprises 28 items measuring 13 constructs: 24 predictor items representing 12 theoretically-derived constructs from UTAUT2 (Venkatesh et al., 2012) and AI-specific extensions, plus 4 Behavioral Intention outcome items. All items were measured on 5-point Likert scales (1 = *Strongly disagree* to 5 = *Strongly agree*).\n",
       "\n",
       "**UTAUT2 Core Constructs (H1):** Performance Expectancy (PE), Effort Expectancy (EE), Social Influence (SI), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HB), and Voluntariness (VO).\n",
       "\n",
       "**AI-Specific Constructs (H2):** Trust in AI (TR), Explainability (EX), Perceived Ethical Risk (ER), and AI Anxiety (AX).\n",
       "\n",
       "**Outcome:** Behavioral Intention (BI) was measured with 4 items assessing readiness, advocacy, future integration, and planned usage increase.\n",
       "\n",
       "Of the 28 items, 25 were positively worded (higher scores indicate greater adoption readiness) and 3 were negatively worded (ER1, ER2, AX1; higher scores indicate barriers to adoption).\n",
       "\n",
       "### References\n",
       "\n",
       "DeVellis, R. F., & Thorpe, C. T. (2022). *Scale development: Theory and applications* (5th ed.). Sage.\n",
       "\n",
       "Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. *MIS Quarterly*, *36*(1), 157–178. https://doi.org/10.2307/41410412\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 9: APA-FORMATTED METHOD SUMMARY (Dynamic)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Extract counts from data (only valid populations)\n",
    "n_total_all = len(df)\n",
    "n_valid = len(df_valid)\n",
    "n_excluded = n_total_all - n_valid\n",
    "\n",
    "n_leader = len(df_valid[df_valid['Population'] == 'Leader'])\n",
    "n_academic = len(df_valid[df_valid['Population'] == 'Academic'])\n",
    "n_professional = len(df_valid[df_valid['Population'] == 'Professional'])\n",
    "\n",
    "# Role breakdown (from original df)\n",
    "role_counts = df['Role'].value_counts()\n",
    "n_ft_student = role_counts.get('Full time student', 0)\n",
    "n_pt_student = role_counts.get('Part time student', 0)\n",
    "n_ic = role_counts.get('Employed - individual contributor', 0)\n",
    "n_manager = role_counts.get('Employed - manager', 0)\n",
    "n_exec = role_counts.get('Employed - executive or leader', 0)\n",
    "n_freelance = role_counts.get('Freelancer or self employed', 0)\n",
    "n_other = role_counts.get('Other', 0)\n",
    "n_unemployed = role_counts.get('Not currently employed', 0)\n",
    "\n",
    "# AI adoption (from valid sample)\n",
    "n_adopters = df_valid['AI_Adoption'].sum()\n",
    "n_non_adopters = n_valid - n_adopters\n",
    "pct_adopters = n_adopters / n_valid * 100\n",
    "pct_non_adopters = 100 - pct_adopters\n",
    "\n",
    "# Disability (from valid sample)\n",
    "disability_counts = df_valid['Disability'].value_counts()\n",
    "n_disability_no = disability_counts.get('No', 0)\n",
    "n_disability_yes = disability_counts.get('Yes', 0)\n",
    "n_disability_pna = disability_counts.get('Prefer not to answer', 0)\n",
    "pct_disability_no = n_disability_no / n_valid * 100\n",
    "pct_disability_yes = n_disability_yes / n_valid * 100\n",
    "pct_disability_pna = n_disability_pna / n_valid * 100\n",
    "\n",
    "# Population percentages\n",
    "pct_leader = n_leader / n_valid * 100\n",
    "pct_academic = n_academic / n_valid * 100\n",
    "pct_professional = n_professional / n_valid * 100\n",
    "\n",
    "# Split results\n",
    "leader_dev = split_results['leader']['dev']\n",
    "leader_holdout = split_results['leader']['holdout']\n",
    "academic_dev = split_results['academic']['dev']\n",
    "academic_holdout = split_results['academic']['holdout']\n",
    "prof_dev = split_results['professional']['dev']\n",
    "prof_holdout = split_results['professional']['holdout']\n",
    "\n",
    "# Item counts\n",
    "n_predictor_items = len([k for k in item_semantic_metadata.keys() if not k.startswith('BI')])\n",
    "n_outcome_items = len([k for k in item_semantic_metadata.keys() if k.startswith('BI')])\n",
    "n_total_items = len(item_semantic_metadata)\n",
    "n_positive_items = sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'POSITIVE')\n",
    "n_negative_items = sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'NEGATIVE')\n",
    "n_constructs = len(construct_groups)\n",
    "\n",
    "# Generate APA text\n",
    "apa_text = f\"\"\"\n",
    "### Participants\n",
    "\n",
    "The sample consisted of *N* = {n_total_all} respondents recruited via convenience sampling through professional networks and academic channels. Of these, {n_excluded} respondents were excluded from population-specific analyses due to ambiguous role categorization (\"Other\"; *n* = {n_other}).\n",
    "\n",
    "The remaining *N* = {n_valid} respondents were categorized into three distinct populations:\n",
    "\n",
    "1. **Academic** (*n* = {n_academic}; {pct_academic:.1f}%): Students, including full-time (*n* = {n_ft_student}) and part-time (*n* = {n_pt_student}) students.\n",
    "\n",
    "2. **Professional** (*n* = {n_professional}; {pct_professional:.1f}%): Working individuals in non-leadership roles, including individual contributors (*n* = {n_ic}), freelancers/self-employed (*n* = {n_freelance}), and those not currently employed (*n* = {n_unemployed}).\n",
    "\n",
    "3. **Leader** (*n* = {n_leader}; {pct_leader:.1f}%): Individuals in leadership positions, including managers (*n* = {n_manager}) and executives/leaders (*n* = {n_exec}).\n",
    "\n",
    "Regarding AI adoption, {n_adopters} participants ({pct_adopters:.1f}%) reported using at least one AI tool beyond \"never,\" while {n_non_adopters} participants ({pct_non_adopters:.1f}%) were classified as non-adopters. Disability status was reported as follows: No (*n* = {n_disability_no}; {pct_disability_no:.1f}%), Yes (*n* = {n_disability_yes}; {pct_disability_yes:.1f}%), and Prefer not to answer (*n* = {n_disability_pna}; {pct_disability_pna:.1f}%).\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Data were prepared for a three-population analytical approach following recommendations for scale development and cross-validation (DeVellis & Thorpe, 2022). Each population was partitioned into development (70%) and holdout (30%) subsets using stratified random sampling with a fixed seed (*seed* = {RANDOM_SEED}) to ensure reproducibility and enable cross-validation of factor structures.\n",
    "\n",
    "**Academic Sample.** The academic population (*N* = {n_academic}) yielded development (*n* = {academic_dev}) and holdout (*n* = {academic_holdout}) samples.\n",
    "\n",
    "**Professional Sample.** The professional population (*N* = {n_professional}) yielded development (*n* = {prof_dev}) and holdout (*n* = {prof_holdout}) samples.\n",
    "\n",
    "**Leader Sample.** The leader population (*N* = {n_leader}) yielded development (*n* = {leader_dev}) and holdout (*n* = {leader_holdout}) samples. Given the smaller holdout size, bootstrap estimation was employed for confirmatory factor analysis to obtain robust standard errors.\n",
    "\n",
    "Stratification was performed on AI adoption status to ensure balanced representation across splits. Chi-square tests confirmed successful stratification across all populations (*p* > .05).\n",
    "\n",
    "### Measures\n",
    "\n",
    "The AI Readiness Scale (AIRS) comprises {n_total_items} items measuring {n_constructs} constructs: {n_predictor_items} predictor items representing 12 theoretically-derived constructs from UTAUT2 (Venkatesh et al., 2012) and AI-specific extensions, plus {n_outcome_items} Behavioral Intention outcome items. All items were measured on 5-point Likert scales (1 = *Strongly disagree* to 5 = *Strongly agree*).\n",
    "\n",
    "**UTAUT2 Core Constructs (H1):** Performance Expectancy (PE), Effort Expectancy (EE), Social Influence (SI), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HB), and Voluntariness (VO).\n",
    "\n",
    "**AI-Specific Constructs (H2):** Trust in AI (TR), Explainability (EX), Perceived Ethical Risk (ER), and AI Anxiety (AX).\n",
    "\n",
    "**Outcome:** Behavioral Intention (BI) was measured with {n_outcome_items} items assessing readiness, advocacy, future integration, and planned usage increase.\n",
    "\n",
    "Of the {n_total_items} items, {n_positive_items} were positively worded (higher scores indicate greater adoption readiness) and {n_negative_items} were negatively worded (ER1, ER2, AX1; higher scores indicate barriers to adoption).\n",
    "\n",
    "### References\n",
    "\n",
    "DeVellis, R. F., & Thorpe, C. T. (2022). *Scale development: Theory and applications* (5th ed.). Sage.\n",
    "\n",
    "Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. *MIS Quarterly*, *36*(1), 157–178. https://doi.org/10.2307/41410412\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(apa_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
