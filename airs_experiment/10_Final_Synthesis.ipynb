{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fe2e8f",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NOTEBOOK 10: FINAL SYNTHESIS - CHAPTER 4 INTEGRATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Purpose**: Synthesize all 10 phases into Chapter 4 dissertation narrative\n",
    "\n",
    "**Integration Scope**:\n",
    "- Consolidate findings across all phases\n",
    "- Generate publication-ready summary tables\n",
    "- Create user typology profiles\n",
    "- Cross-tabulate typology with qualitative themes\n",
    "- Identify novel contributions\n",
    "\n",
    "**Phase Summary**:\n",
    "\n",
    "| Phase | Notebook | Key Output |\n",
    "|-------|----------|------------|\n",
    "| 0 | Data Preparation | Split samples (N=261 EFA, N=262 CFA) |\n",
    "| 1 | EFA | 8-factor structure, 16 items |\n",
    "| 2 | CFA | Model D validated (CFI=.975) |\n",
    "| 3 | Invariance | Configural + metric supported (2-group SEM) |\n",
    "| 4 | Structural Model | 3/7 UTAUT2 paths + AI Trust significant |\n",
    "| 5 | Mediation | Model comparison analysis |\n",
    "| 6 | Moderation | Experience moderates PEâ†’BI, HMâ†’BI |\n",
    "| 7 | Tool Usage | 3-group patterns (Academic, Professional, Leader) |\n",
    "| 8 | Qualitative | Calibrated trust themes (3-group) |\n",
    "| 9 | Gap Analysis | Demographics robust |\n",
    "| 10 | Synthesis | User typology Ã— themes |\n",
    "\n",
    "**Role Type Classifications**:\n",
    "- **3-Group (Descriptive)**: Academic (N=216), Professional (N=184), Leader (N=123)\n",
    "- **2-Group (Multi-Group SEM)**: Academic (N=216), Professional+Leader (N=307)\n",
    "\n",
    "**Key Empirical Findings**:\n",
    "\n",
    "| Hypothesis | Path | Î² | p | Status |\n",
    "|------------|------|---|---|--------|\n",
    "| H1c | SI â†’ BI | 0.136 | .024 | âœ… Supported |\n",
    "| H1e | HM â†’ BI | 0.217 | .014 | âœ… Supported |\n",
    "| H1f | PV â†’ BI | 0.505 | <.001 | âœ… Supported (Strongest) |\n",
    "| **H2** | **TR â†’ BI** | **0.106** | **.064** | **âš ï¸ AI Trust Marginal** |\n",
    "| H1a | PE â†’ BI | -0.028 | .791 | âŒ Not Supported |\n",
    "| H1b | EE â†’ BI | -0.008 | .875 | âŒ Not Supported |\n",
    "| H1d | FC â†’ BI | 0.059 | .338 | âŒ Not Supported |\n",
    "| H1g | HB â†’ BI | 0.023 | .631 | âŒ Not Supported |\n",
    "\n",
    "**Novel Contributions**:\n",
    "1. **AI Trust as Marginal Predictor** (Î² = 0.106, p = .064) - supports AIRS extension\n",
    "2. **Experience Moderation** - high experience strengthens PEâ†’BI and HMâ†’BI paths\n",
    "3. **User Typology** - 4 distinct profiles via cluster analysis (Enthusiasts, Cautious Adopters, Moderate Users, Anxious Avoiders)\n",
    "4. **Typology Ã— Theme Association** - Enthusiasts report more positive experiences (Ï‡Â²=10.05, p=.018)\n",
    "5. **Price Value Strongest** - PV unexpectedly strongest predictor (Î² = 0.505)\n",
    "\n",
    "**Outputs**:\n",
    "- `data/final_synthesis.json` - All integrated results\n",
    "- User typology profiles (4 clusters)\n",
    "- Cross-tabulation analysis\n",
    "- Chapter 4 narrative structure\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Phases[\"10-Phase Pipeline\"]\n",
    "        P0[\"00: Data Split\"]\n",
    "        P1[\"01: EFA\"]\n",
    "        P2[\"02: CFA\"]\n",
    "        P3[\"03: Invariance\"]\n",
    "        P4[\"04: Structural\"]\n",
    "        P5[\"05: Mediation\"]\n",
    "        P6[\"06: Moderation\"]\n",
    "        P7[\"07: Usage\"]\n",
    "        P8[\"08: Qualitative\"]\n",
    "        P9[\"09: Gap Analysis\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Findings[\"Key Empirical Findings\"]\n",
    "        F1[\"ğŸ’° PVâ†’BI<br/>Î²=.51 (Strongest)\"]\n",
    "        F2[\"ğŸ¤ TRâ†’BI<br/>Î²=.11 (p=.064)\"]\n",
    "        F3[\"ğŸ“ˆ Experience<br/>Moderation\"]\n",
    "        F4[\"ğŸ‘¥ 4 User<br/>Typologies\"]\n",
    "        F5[\"ğŸ“Š Typology Ã—<br/>Themes\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Output[\"Dissertation Output\"]\n",
    "        CH4[\"Chapter 4<br/>Results\"]\n",
    "    end\n",
    "    \n",
    "    P0 --> P1 --> P2 --> P3 --> P4 --> P5 --> P6 --> P7 --> P8 --> P9\n",
    "    P4 --> F1 & F2\n",
    "    P6 --> F3\n",
    "    P9 --> F4\n",
    "    P9 --> F5\n",
    "    F1 & F2 & F3 & F4 & F5 --> CH4\n",
    "    \n",
    "    style F1 fill:#2e7d32,color:#fff\n",
    "    style F2 fill:#1565c0,color:#fff\n",
    "    style CH4 fill:#7b1fa2,color:#fff,stroke-width:3px\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Results Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d5763",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d11376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 10: FINAL SYNTHESIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ“ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917fdf1",
   "metadata": {},
   "source": [
    "## 1.2 Load All Phase Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf86aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each phase\n",
    "results = {}\n",
    "\n",
    "# Phase 1: EFA Results\n",
    "try:\n",
    "    with open('tables/efa_summary.json', 'r') as f:\n",
    "        results['efa'] = json.load(f)\n",
    "    print(\"âœ“ Loaded EFA results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ EFA results not found\")\n",
    "\n",
    "# Phase 2: CFA Results\n",
    "try:\n",
    "    with open('tables/cfa_summary.json', 'r') as f:\n",
    "        results['cfa'] = json.load(f)\n",
    "    print(\"âœ“ Loaded CFA results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ CFA results not found\")\n",
    "\n",
    "# Phase 3: Invariance Results\n",
    "try:\n",
    "    with open('data/invariance_results.json', 'r') as f:\n",
    "        results['invariance'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Invariance results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Invariance results not found\")\n",
    "\n",
    "# Phase 4: Structural Model Results\n",
    "try:\n",
    "    with open('data/structural_model_results.json', 'r') as f:\n",
    "        results['structural'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Structural model results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Structural model results not found\")\n",
    "\n",
    "# Phase 5: Mediation/Alternative Results\n",
    "try:\n",
    "    with open('data/phase5_alternative_analyses.json', 'r') as f:\n",
    "        results['mediation'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Mediation/Alternative results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Mediation results not found\")\n",
    "\n",
    "# Phase 6: Moderation Results (updated with demographic moderation)\n",
    "try:\n",
    "    with open('data/moderation_results_updated.json', 'r') as f:\n",
    "        results['moderation'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Moderation results (updated with demographics)\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        with open('data/moderation_analysis_results.json', 'r') as f:\n",
    "            results['moderation'] = json.load(f)\n",
    "        print(\"âœ“ Loaded Moderation results (original)\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Moderation results not found\")\n",
    "\n",
    "# Phase 7: Tool Usage Results\n",
    "try:\n",
    "    with open('results/phase7_tool_usage_results.json', 'r') as f:\n",
    "        results['tool_usage'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Tool Usage results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Tool Usage results not found\")\n",
    "\n",
    "# Phase 8: Qualitative Results\n",
    "try:\n",
    "    with open('results/phase8_qualitative_results.json', 'r') as f:\n",
    "        results['qualitative'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Qualitative results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Qualitative results not found\")\n",
    "\n",
    "# Phase 9: Comprehensive Review Results\n",
    "try:\n",
    "    with open('results/phase9_comprehensive_review_results.json', 'r') as f:\n",
    "        results['comprehensive'] = json.load(f)\n",
    "    print(\"âœ“ Loaded Comprehensive Review results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Comprehensive Review results not found\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total phases loaded: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05058b9b",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2: Hypothesis Testing Summary\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0a88a",
   "metadata": {},
   "source": [
    "## 2.1 Complete Hypothesis Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe89994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Hypothesis Testing Summary\n",
    "hypothesis_results = [\n",
    "    # H1: UTAUT2 Core Constructs\n",
    "    {'Hypothesis': 'H1a', 'Path': 'PE â†’ BI', 'Î²': -0.028, 'p': '.791', 'Result': 'Not Supported', 'Notes': 'Performance expectancy not significant'},\n",
    "    {'Hypothesis': 'H1b', 'Path': 'EE â†’ BI', 'Î²': -0.008, 'p': '.875', 'Result': 'Not Supported', 'Notes': 'Effort expectancy not significant'},\n",
    "    {'Hypothesis': 'H1c', 'Path': 'SI â†’ BI', 'Î²': 0.136, 'p': '.024', 'Result': 'Supported', 'Notes': 'Social influence positively predicts behavioral intention'},\n",
    "    {'Hypothesis': 'H1d', 'Path': 'FC â†’ BI', 'Î²': 0.059, 'p': '.338', 'Result': 'Not Supported', 'Notes': 'Facilitating conditions not significant'},\n",
    "    {'Hypothesis': 'H1e', 'Path': 'HM â†’ BI', 'Î²': 0.217, 'p': '.014', 'Result': 'Supported', 'Notes': 'Hedonic motivation positively predicts behavioral intention'},\n",
    "    {'Hypothesis': 'H1f', 'Path': 'PV â†’ BI', 'Î²': 0.505, 'p': '<.001', 'Result': 'Supported', 'Notes': 'Price value STRONGEST predictor - cost/benefit drives adoption'},\n",
    "    {'Hypothesis': 'H1g', 'Path': 'HB â†’ BI', 'Î²': 0.023, 'p': '.631', 'Result': 'Not Supported', 'Notes': 'Habit not significant'},\n",
    "    \n",
    "    # H2: AI Trust Extension\n",
    "    {'Hypothesis': 'H2', 'Path': 'TR â†’ BI', 'Î²': 0.106, 'p': '.064', 'Result': 'Marginal', 'Notes': 'AI Trust marginally significant; supports AIRS extension'},\n",
    "    \n",
    "    # H3: Population Differences\n",
    "    {'Hypothesis': 'H3', 'Path': 'Academic â‰  Professional', 'Î²': 'N/A', 'p': 'N/A', 'Result': 'Not Supported', 'Notes': 'No meaningful structural differences; partial scalar invariance achieved'},\n",
    "    \n",
    "    # H4: Moderation Effects\n",
    "    {'Hypothesis': 'H4a', 'Path': 'Population Ã— (PE,EE)', 'Î²': 'N/A', 'p': '>.05', 'Result': 'Not Supported', 'Notes': 'Population does not moderate predictor effects'},\n",
    "    {'Hypothesis': 'H4b', 'Path': 'Role Ã— (SI,FC)', 'Î²': 'N/A', 'p': '>.05', 'Result': 'Not Supported', 'Notes': 'Role type does not moderate predictor effects'},\n",
    "    {'Hypothesis': 'H4c', 'Path': 'Usage Ã— (HBâ†’BI)', 'Î²': -0.30, 'p': '.030', 'Result': 'Not Supported*', 'Notes': '*Significant in OPPOSITE direction; high-frequency users show negative HBâ†’BI'},\n",
    "    {'Hypothesis': 'H4e', 'Path': 'Voluntariness Ã— (SI,FC)', 'Î²': 'N/A', 'p': '>.60', 'Result': 'Not Supported', 'Notes': 'Model instability; caution warranted'},\n",
    "    {'Hypothesis': 'H4f', 'Path': 'Disability Ã— (EE,FC)', 'Î²': 'N/A', 'p': '>.90', 'Result': 'Not Supported', 'Notes': 'Unreliable; small sample (n=69)'},\n",
    "    \n",
    "    # H5: Mediation\n",
    "    {'Hypothesis': 'H5a', 'Path': 'EX â†’ TR â†’ BI', 'Î²': 'N/A', 'p': 'N/A', 'Result': 'Not Testable', 'Notes': 'Explainability construct dropped (Î± < .60)'},\n",
    "    {'Hypothesis': 'H5b', 'Path': 'ER â†’ TR â†’ BI', 'Î²': 'N/A', 'p': 'N/A', 'Result': 'Not Testable', 'Notes': 'Ethical Risk construct dropped (Î± < .60)'},\n",
    "    {'Hypothesis': 'H5c', 'Path': 'ER â†’ AX â†’ BI', 'Î²': 'N/A', 'p': 'N/A', 'Result': 'Not Testable', 'Notes': 'AI Anxiety construct dropped (Î± < .60)'},\n",
    "]\n",
    "\n",
    "df_hypotheses = pd.DataFrame(hypothesis_results)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLE 4.X: HYPOTHESIS TESTING SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(df_hypotheses.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "supported = len([h for h in hypothesis_results if h['Result'] == 'Supported'])\n",
    "marginal = len([h for h in hypothesis_results if h['Result'] == 'Marginal'])\n",
    "not_supported = len([h for h in hypothesis_results if 'Not Supported' in h['Result']])\n",
    "not_testable = len([h for h in hypothesis_results if h['Result'] == 'Not Testable'])\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary: {supported} Supported, {marginal} Marginal, {not_supported} Not Supported, {not_testable} Not Testable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a0f37",
   "metadata": {},
   "source": [
    "## 2.2 Novel Findings Beyond Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc855e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel findings discovered during analysis\n",
    "novel_findings = [\n",
    "    {\n",
    "        'Finding': 'Price Value Strongest Predictor',\n",
    "        'Statistic': 'Î² = 0.505, p < .001',\n",
    "        'Phase': 4,\n",
    "        'Implication': 'Cost-benefit analysis drives AI adoption decisions',\n",
    "        'Significance': 'Unexpected finding - challenges assumption that free tools negate PV'\n",
    "    },\n",
    "    {\n",
    "        'Finding': 'AI Trust Marginally Significant',\n",
    "        'Statistic': 'Î² = 0.106, p = .064',\n",
    "        'Phase': 4,\n",
    "        'Implication': 'Trust approaches significance as AI-specific predictor',\n",
    "        'Significance': 'Supports AIRS extension to UTAUT2'\n",
    "    },\n",
    "    {\n",
    "        'Finding': 'Experience Moderates PE â†’ BI',\n",
    "        'Statistic': 'Î² = 0.148, p = .013',\n",
    "        'Phase': 6,\n",
    "        'Implication': 'Performance expectancy effect stronger for experienced professionals',\n",
    "        'Significance': 'Novel contribution extending UTAUT2 with career development'\n",
    "    },\n",
    "    {\n",
    "        'Finding': 'Experience Moderates HM â†’ BI',\n",
    "        'Statistic': 'Î² = 0.136, p = .009',\n",
    "        'Phase': 6,\n",
    "        'Implication': 'Hedonic motivation becomes more important with experience',\n",
    "        'Significance': 'Novel contribution extending UTAUT2 with career development'\n",
    "    },\n",
    "    {\n",
    "        'Finding': 'Disability â†’ AI Anxiety Association',\n",
    "        'Statistic': 'd = 0.36, p = .006',\n",
    "        'Phase': 9,\n",
    "        'Implication': 'Accessibility concerns may drive AI-related anxiety',\n",
    "        'Significance': 'Future research direction for inclusive AI design'\n",
    "    },\n",
    "    {\n",
    "        'Finding': '4 User Typology Segments',\n",
    "        'Statistic': 'K-means clustering',\n",
    "        'Phase': 9,\n",
    "        'Implication': 'AI Enthusiasts (16%), Cautious Adopters (30%), Moderate Users (37%), Anxious Avoiders (17%)',\n",
    "        'Significance': 'Practical segmentation for intervention design'\n",
    "    },\n",
    "]\n",
    "\n",
    "df_novel = pd.DataFrame(novel_findings)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLE 4.X: NOVEL FINDINGS BEYOND HYPOTHESIZED RELATIONSHIPS\")\n",
    "print(\"=\" * 100)\n",
    "for i, finding in enumerate(novel_findings, 1):\n",
    "    print(f\"\\n{i}. {finding['Finding']}\")\n",
    "    print(f\"   Statistic: {finding['Statistic']}\")\n",
    "    print(f\"   Implication: {finding['Implication']}\")\n",
    "    print(f\"   Significance: {finding['Significance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59db577",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 3: Model Fit Summary\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fit Summary across phases\n",
    "# Note: Multi-Group uses 2-group (Academic vs Professional+Leader) due to sample size requirements for SEM\n",
    "model_fit = {\n",
    "    'Model': ['EFA Model D', 'CFA Holdout', 'CFA Full Sample', 'Structural (Full)', 'MG-Academic', 'MG-Prof+Leader'],\n",
    "    'N': [261, 262, 523, 523, 216, 307],\n",
    "    'Ï‡Â²': ['N/A', 129.9, 'TBD', 'TBD', 'TBD', 'TBD'],\n",
    "    'df': ['N/A', 98, 'TBD', 'TBD', 98, 98],\n",
    "    'CFI': ['.964*', '.975', 'TBD', 'TBD', 'TBD', 'TBD'],\n",
    "    'TLI': ['.958*', '.960', 'TBD', 'TBD', 'TBD', 'TBD'],\n",
    "    'RMSEA': ['.047*', '.065', 'TBD', 'TBD', 'TBD', 'TBD'],\n",
    "    'SRMR': ['.040*', '.046', 'TBD', 'TBD', 'TBD', 'TBD'],\n",
    "    'Status': ['Excellent', 'Good', 'TBD', 'TBD', 'TBD', 'TBD']\n",
    "}\n",
    "\n",
    "df_fit = pd.DataFrame(model_fit)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLE 4.X: MODEL FIT INDICES ACROSS ANALYSIS PHASES\")\n",
    "print(\"=\" * 100)\n",
    "print(df_fit.to_string(index=False))\n",
    "print(\"\\n*Note: EFA values are from final rotated solution; CFA values from semopy\")\n",
    "print(\"Thresholds: CFI/TLI > .90 acceptable, > .95 good; RMSEA < .08 acceptable, < .05 good; SRMR < .08 acceptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfcaa9",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 4: Construct Reliability Summary\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d513e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability summary\n",
    "reliability = {\n",
    "    'Construct': ['Performance Expectancy (PE)', 'Effort Expectancy (EE)', 'Social Influence (SI)', \n",
    "                  'Facilitating Conditions (FC)', 'Hedonic Motivation (HM)', 'Price Value (PV)',\n",
    "                  'Habit (HB)', 'AI Trust (TR)', 'Behavioral Intention (BI)'],\n",
    "    'Items': ['PE1, PE2', 'EE1, EE2', 'SI1, SI2', 'FC1, FC2', 'HM1, HM2', 'PV1, PV2', 'HB1, HB2', 'TR1, TR2', 'BI1-BI4'],\n",
    "    'Î±_EFA': [0.87, 0.87, 0.78, 0.79, 0.89, 0.87, 0.90, 0.91, 0.91],\n",
    "    'Î±_CFA': [0.87, 0.87, 0.78, 0.79, 0.89, 0.87, 0.90, 0.91, 0.91],\n",
    "    'CR': [0.88, 0.87, 0.79, 0.80, 0.89, 0.88, 0.90, 0.91, 0.91],\n",
    "    'AVE': [0.78, 0.77, 0.65, 0.67, 0.81, 0.78, 0.82, 0.84, 0.72],\n",
    "    'Status': ['Excellent', 'Excellent', 'Good', 'Good', 'Excellent', 'Excellent', 'Excellent', 'Excellent', 'Excellent']\n",
    "}\n",
    "\n",
    "df_reliability = pd.DataFrame(reliability)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLE 4.X: CONSTRUCT RELIABILITY AND VALIDITY\")\n",
    "print(\"=\" * 100)\n",
    "print(df_reliability.to_string(index=False))\n",
    "print(\"\\nNote: Î± = Cronbach's alpha; CR = Composite Reliability; AVE = Average Variance Extracted\")\n",
    "print(\"Thresholds: Î± > .70, CR > .70, AVE > .50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528ec7f",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 5: Dropped Constructs Documentation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53198a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped constructs documentation\n",
    "dropped_constructs = {\n",
    "    'Construct': ['Voluntariness (VO)', 'Explainability (EX)', 'Ethical Risk (ER)', 'AI Anxiety (AX)'],\n",
    "    'Original_Items': ['VO1, VO2', 'EX1, EX2', 'ER1, ER2', 'AX1, AX2'],\n",
    "    'Î±_Observed': [0.43, 0.47, 0.31, 0.58],\n",
    "    'Reason_Dropped': [\n",
    "        'VO1 measures personal choice; VO2 measures external permission â†’ conceptually distinct',\n",
    "        'Items measured different facets of XAI (process vs outcome transparency)',\n",
    "        'Items measured unrelated risks (job displacement vs data privacy)',\n",
    "        'Items loaded on general technology anxiety, not AI-specific'\n",
    "    ],\n",
    "    'Recommendation': [\n",
    "        'Redesign with consistent theoretical focus',\n",
    "        'Separate into process and outcome explainability constructs',\n",
    "        'Develop multi-dimensional ethical concerns scale',\n",
    "        'Validate against established AI anxiety scales (e.g., AIAS)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_dropped = pd.DataFrame(dropped_constructs)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLE 4.X: DROPPED CONSTRUCTS AND ITEM DESIGN ISSUES\")\n",
    "print(\"=\" * 100)\n",
    "print(df_dropped.to_string(index=False))\n",
    "print(\"\\nâš ï¸ These constructs require item redesign for future validation studies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443e5d4",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 6: Chapter 4 Narrative Framework\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9f1b1",
   "metadata": {},
   "source": [
    "## 4.1 Measurement Model Results\n",
    "\n",
    "### 4.1.1 Exploratory Factor Analysis\n",
    "\n",
    "Theory-guided EFA on the development sample (*N* = 261) compared six models ranging from the full theoretical model (12 factors, 24 items) to a minimal viable model (4 factors, 8 items). Model selection prioritized both theoretical coherence and psychometric adequacy.\n",
    "\n",
    "**Model D** (8 factors, 16 items) was selected as it retained all constructs meeting the Î± â‰¥ .60 threshold while preserving theoretical meaning. This model explained 64% of the variance with all factor loadings exceeding .70.\n",
    "\n",
    "Four constructs were dropped due to reliability concerns: Voluntariness (Î± = .43), Explainability (Î± = .47), Ethical Risk (Î± = .31), and AI Anxiety (Î± = .58). Post-hoc analysis revealed that these items measured conceptually distinct facets rather than unidimensional constructs.\n",
    "\n",
    "### 4.1.2 Confirmatory Factor Analysis\n",
    "\n",
    "CFA on the holdout sample (*N* = 262) validated the 8-factor structure with excellent fit: Ï‡Â²(98) = 129.9, CFI = .975, TLI = .960, RMSEA = .065, SRMR = .046. All factor loadings exceeded .70, supporting convergent validity. Discriminant validity was established via Fornell-Larcker criterion, with all âˆšAVE values exceeding inter-construct correlations.\n",
    "\n",
    "### 4.1.3 Measurement Invariance\n",
    "\n",
    "Multi-group CFA tested measurement invariance across academic (*n* = 216) and professional+leader (*n* = 307) populations. The 2-group classification was used for multi-group SEM due to sample size requirements (the leader subgroup alone was *n* = 123, insufficient for stable SEM estimation). Results supported configural invariance (same factor structure) and partial scalar invariance (equivalent loadings and most intercepts). One intercept (SI2) was allowed to vary between groups, consistent with social influence operating differently across academic and workplace contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b629b",
   "metadata": {},
   "source": [
    "## 4.2 Structural Model Results\n",
    "\n",
    "### 4.2.1 Hypothesis Testing\n",
    "\n",
    "Structural equation modeling on the full sample (*N* = 523) tested the hypothesized relationships between UTAUT2 constructs, AI Trust, and Behavioral Intention.\n",
    "\n",
    "**H1 (UTAUT2 Core)**: Three of seven UTAUT2 paths were statistically significant:\n",
    "- Social Influence â†’ BI: Î² = .14, *p* = .024 (H1c **supported**)\n",
    "- Hedonic Motivation â†’ BI: Î² = .22, *p* = .014 (H1e **supported**)\n",
    "- Price Value â†’ BI: Î² = .51, *p* < .001 (H1f **supported** - STRONGEST)\n",
    "- Performance Expectancy â†’ BI: Î² = -.03, *p* = .791 (H1a not supported)\n",
    "- Effort Expectancy â†’ BI: Î² = -.01, *p* = .875 (H1b not supported)\n",
    "- Facilitating Conditions â†’ BI: Î² = .06, *p* = .338 (H1d not supported)\n",
    "- Habit â†’ BI: Î² = .02, *p* = .631 (H1g not supported)\n",
    "\n",
    "**H2 (AI Trust Extension)**: The AI Trust path approached significance:\n",
    "- AI Trust â†’ BI: Î² = .11, *p* = .064 (H2 **marginal support**)\n",
    "\n",
    "This finding provides tentative support for the theoretical extension of UTAUT2 with AI-specific trust considerations.\n",
    "\n",
    "### 4.2.2 Multi-Group Comparisons\n",
    "\n",
    "**H3 (Population Differences)**: Constrained multi-group SEM found no significant differences in structural paths between academic and professional populations, *Ï‡Â²diff* = 8.7, *df* = 7, *p* = .27. H3 was not supported.\n",
    "\n",
    "**H4a-b (Population Ã— Role Moderation)**: No significant moderation effects were detected for population or role type on predictor paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb68ce8",
   "metadata": {},
   "source": [
    "## 4.3 Moderation Analysis Results\n",
    "\n",
    "### 4.3.1 Contextual Moderators (Hypothesized)\n",
    "\n",
    "**H4c (Usage Frequency Ã— Habit)**: A significant but *opposite-to-hypothesized* effect emerged. High-frequency users exhibited a *negative* Habit â†’ BI relationship (Î² = -.20, *p* = .025) compared to low-frequency users (Î² = .10, *p* = .336), *z* = -2.17, *p* = .030. This suggests that frequent AI users have moved beyond habit-driven adoption toward more deliberate evaluation.\n",
    "\n",
    "**H4e (Voluntariness)** and **H4f (Disability)**: Model instability due to small subgroup sizes (*n* = 70 and *n* = 69 respectively) precluded reliable inference. Results should be interpreted with caution.\n",
    "\n",
    "### 4.3.2 Demographic Moderators (Exploratory)\n",
    "\n",
    "Post-hoc moderation analyses tested Industry, Education, and Professional Experience as potential moderators.\n",
    "\n",
    "**Industry**: No significant moderation effects (all *p* > .40). The psychological mechanisms driving AI adoption operate equivalently across Tech/Finance and other sectors.\n",
    "\n",
    "**Education**: No significant moderation effects. AI Trust Ã— Education approached significance (*p* = .069), suggesting higher education may marginally strengthen the Trust â†’ BI relationship.\n",
    "\n",
    "**Professional Experience** (Novel Finding): Two significant moderation effects emerged:\n",
    "- PE Ã— Experience: Î² = .148, *p* = .013 â€” Performance expectancy effect is stronger for experienced professionals\n",
    "- HM Ã— Experience: Î² = .136, *p* = .009 â€” Hedonic motivation effect is stronger for experienced professionals\n",
    "\n",
    "These findings suggest that career stage shapes AI adoption motivations, with experienced professionals placing greater weight on both productivity benefits and enjoyment of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64e5c5",
   "metadata": {},
   "source": [
    "## 4.4 Exploratory Findings\n",
    "\n",
    "### 4.4.1 User Typology\n",
    "\n",
    "K-means clustering identified four distinct user segments:\n",
    "1. **AI Enthusiasts** (16%): High adoption, low anxiety - fully embracing AI\n",
    "2. **Cautious Adopters** (30%): High adoption + high anxiety - engaged but concerned\n",
    "3. **Moderate Users** (37%): Average scores across all constructs\n",
    "4. **Anxious Avoiders** (17%): Low adoption, high anxiety - resistant to AI\n",
    "\n",
    "### 4.4.2 Disability and AI Anxiety\n",
    "\n",
    "Participants reporting disabilities showed significantly higher AI Anxiety scores (M = 3.68, SD = 0.99) compared to those without disabilities (M = 3.35, SD = 0.93), *t*(510) = 2.77, *p* = .006, *d* = 0.36. This small-to-medium effect suggests accessibility concerns may contribute to AI-related anxiety, warranting future research on inclusive AI design.\n",
    "\n",
    "### 4.4.3 Tool Usage Patterns\n",
    "\n",
    "ChatGPT was the dominant AI tool (78% usage), followed by Microsoft Copilot (45%) and Google Gemini (32%). Higher ChatGPT familiarity was associated with more positive attitudes toward AI adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26505793",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 7: Export Final Synthesis\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive synthesis\n",
    "synthesis = {\n",
    "    'study_overview': {\n",
    "        'purpose': 'Develop and validate AIRS instrument',\n",
    "        'theoretical_base': 'UTAUT2 + AI Trust extension',\n",
    "        'sample_size': 523,\n",
    "        'role_types': {\n",
    "            'three_group': ['Academic (n=216)', 'Professional (n=184)', 'Leader (n=123)'],\n",
    "            'two_group_sem': ['Academic (n=216)', 'Professional+Leader (n=307)'],\n",
    "            'note': '3-group for descriptive; 2-group for multi-group SEM (sample size)'\n",
    "        },\n",
    "        'validated_model': 'Model D: 8 factors, 16 items'\n",
    "    },\n",
    "    'hypothesis_summary': {\n",
    "        'supported': ['H1c', 'H1e', 'H1f'],\n",
    "        'marginal': ['H2'],\n",
    "        'not_supported': ['H1a', 'H1b', 'H1d', 'H1g', 'H3', 'H4a', 'H4b', 'H4c*', 'H4e', 'H4f'],\n",
    "        'not_testable': ['H5a', 'H5b', 'H5c'],\n",
    "        'note': 'H4c* significant in opposite direction; H1f (PV) strongest predictor'\n",
    "    },\n",
    "    'novel_contributions': [\n",
    "        'Price Value strongest predictor (Î² = .505***)',\n",
    "        'AI Trust marginally significant (Î² = .106, p = .064)',\n",
    "        'Experience moderates PE â†’ BI (Î² = .148, p = .013)',\n",
    "        'Experience moderates HM â†’ BI (Î² = .136, p = .009)',\n",
    "        'Disability associated with AI Anxiety (d = .36)',\n",
    "        '4-segment user typology identified'\n",
    "    ],\n",
    "    'model_fit': {\n",
    "        'cfa_holdout': {'CFI': 0.975, 'TLI': 0.960, 'RMSEA': 0.065, 'SRMR': 0.046},\n",
    "        'cfa_full': 'TBD'\n",
    "    },\n",
    "    'dropped_constructs': {\n",
    "        'voluntariness': {'alpha': 0.43, 'reason': 'Conceptually distinct items'},\n",
    "        'explainability': {'alpha': 0.47, 'reason': 'Process vs outcome facets'},\n",
    "        'ethical_risk': {'alpha': 0.31, 'reason': 'Unrelated risk dimensions'},\n",
    "        'ai_anxiety': {'alpha': 0.58, 'reason': 'General technology anxiety'}\n",
    "    },\n",
    "    'practical_implications': [\n",
    "        'Trust-building interventions are priority',\n",
    "        'Segment-specific adoption strategies recommended',\n",
    "        'Experience-based messaging for PE and HM',\n",
    "        'Accessibility considerations for anxiety reduction'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save synthesis\n",
    "with open('data/final_synthesis.json', 'w') as f:\n",
    "    json.dump(synthesis, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Final synthesis saved to: data/final_synthesis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188db52",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 8: Key Figures for Chapter 4\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9692ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothesis support visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "categories = ['H1 (UTAUT2)', 'H2 (AI Trust)', 'H3 (Differences)', 'H4 (Moderation)', 'H5 (Mediation)']\n",
    "supported = [3, 0, 0, 0, 0]\n",
    "marginal = [0, 1, 0, 0, 0]\n",
    "not_supported = [4, 0, 1, 5, 0]\n",
    "not_testable = [0, 0, 0, 0, 3]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, supported, width, label='Supported', color='#2e7d32')\n",
    "bars2 = ax.bar(x - 0.5*width, marginal, width, label='Marginal', color='#f57c00')\n",
    "bars3 = ax.bar(x + 0.5*width, not_supported, width, label='Not Supported', color='#c62828')\n",
    "bars4 = ax.bar(x + 1.5*width, not_testable, width, label='Not Testable', color='#757575')\n",
    "\n",
    "ax.set_ylabel('Number of Hypotheses')\n",
    "ax.set_title('Hypothesis Testing Summary by Category')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/hypothesis_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved: plots/hypothesis_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structural path coefficients visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "paths = ['Price Value', 'Hedonic Mot.', 'Social Inf.', 'AI Trust', 'Fac. Cond.', 'Habit', 'Effort Exp.', 'Perf. Expect.']\n",
    "betas = [0.505, 0.217, 0.136, 0.106, 0.059, 0.023, -0.008, -0.028]\n",
    "significant = [True, True, True, False, False, False, False, False]  # TR is marginal\n",
    "\n",
    "colors = ['#2e7d32' if sig else '#c62828' for sig in significant]\n",
    "# Mark AI Trust as marginal (orange)\n",
    "colors[3] = '#f57c00'\n",
    "\n",
    "y_pos = np.arange(len(paths))\n",
    "bars = ax.barh(y_pos, betas, color=colors)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(paths)\n",
    "ax.set_xlabel('Standardized Path Coefficient (Î²)')\n",
    "ax.set_title('Structural Path Coefficients: Predictors â†’ Behavioral Intention')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Add significance markers\n",
    "sig_labels = ['***', '*', '*', 'â€ ', 'ns', 'ns', 'ns', 'ns']\n",
    "for i, (bar, label) in enumerate(zip(bars, sig_labels)):\n",
    "    width = bar.get_width()\n",
    "    ax.annotate(f'{width:.3f} {label}',\n",
    "                xy=(max(width, 0.02), bar.get_y() + bar.get_height()/2),\n",
    "                xytext=(5, 0),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='left', va='center', fontsize=9)\n",
    "\n",
    "ax.legend([plt.Rectangle((0,0),1,1, fc='#2e7d32'), plt.Rectangle((0,0),1,1, fc='#f57c00'), \n",
    "          plt.Rectangle((0,0),1,1, fc='#c62828')], \n",
    "          ['Significant (p < .05)', 'Marginal (p < .10)', 'Non-significant'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/structural_paths.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved: plots/structural_paths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b283d13",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 9: Cross-Tabulation - User Typology Ã— Qualitative Themes\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "This analysis examines whether different user typologies (from Phase 9 cluster analysis) express different qualitative themes in their open-ended feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CROSS-TABULATION: USER TYPOLOGY Ã— QUALITATIVE THEMES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-TABULATION: USER TYPOLOGY Ã— QUALITATIVE THEMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load EFA and CFA samples and combine\n",
    "df_efa = pd.read_csv('data/AIRS_experiment.csv')\n",
    "df_cfa = pd.read_csv('data/AIRS_holdout.csv')\n",
    "df_full = pd.concat([df_efa, df_cfa], ignore_index=True)\n",
    "print(f\"\\nâœ“ Loaded combined sample: N={len(df_full)}\")\n",
    "\n",
    "# Create composite construct scores\n",
    "construct_items = {\n",
    "    'PerfExp': ['PE1', 'PE2'],\n",
    "    'EffortExp': ['EE1', 'EE2'],\n",
    "    'SocialInf': ['SI1', 'SI2'],\n",
    "    'FacilCond': ['FC1', 'FC2'],\n",
    "    'HedonicMot': ['HM1', 'HM2'],\n",
    "    'PriceVal': ['PV1', 'PV2'],\n",
    "    'Habit': ['HB1', 'HB2'],\n",
    "    'AITrust': ['TR1', 'TR2']\n",
    "}\n",
    "\n",
    "for construct, items in construct_items.items():\n",
    "    df_full[construct] = df_full[items].mean(axis=1)\n",
    "\n",
    "# Check if anxiety items exist\n",
    "if 'AX1' in df_full.columns and 'AX2' in df_full.columns:\n",
    "    df_full['AI_Anxiety'] = df_full[['AX1', 'AX2']].mean(axis=1)\n",
    "\n",
    "# Define constructs for clustering (same as Phase 9)\n",
    "cluster_vars = ['PerfExp', 'EffortExp', 'SocialInf', 'FacilCond', \n",
    "                'HedonicMot', 'PriceVal', 'Habit', 'AITrust', 'AI_Adoption']\n",
    "\n",
    "# Prepare data for clustering\n",
    "X = df_full[cluster_vars].dropna()\n",
    "print(f\"âœ“ {len(X)} complete cases for clustering\")\n",
    "\n",
    "# Standardize for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform K-means clustering (K=4 as determined in Phase 9)\n",
    "kmeans = KMeans(n_clusters=4, random_state=67, n_init=10)\n",
    "df_full.loc[X.index, 'Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate cluster profiles\n",
    "cluster_profiles = df_full.groupby('Cluster')[cluster_vars].mean()\n",
    "print(\"\\n1. CLUSTER PROFILES:\")\n",
    "print(cluster_profiles.round(2))\n",
    "\n",
    "# Name clusters based on RELATIVE position (rank-based naming)\n",
    "# Sort clusters by mean AI_Adoption to assign names\n",
    "adoption_means = cluster_profiles['AI_Adoption'].sort_values()\n",
    "trust_means = cluster_profiles['AITrust']\n",
    "\n",
    "# Create names based on patterns - using adoption and trust levels\n",
    "cluster_names = {}\n",
    "for c in range(4):\n",
    "    adoption = cluster_profiles.loc[c, 'AI_Adoption']\n",
    "    trust = cluster_profiles.loc[c, 'AITrust']\n",
    "    avg_readiness = cluster_profiles.loc[c, cluster_vars[:-1]].mean()  # Exclude AI_Adoption\n",
    "    \n",
    "    if avg_readiness > 4.0:\n",
    "        cluster_names[c] = 'AI Enthusiasts'\n",
    "    elif avg_readiness > 3.0:\n",
    "        cluster_names[c] = 'Cautious Adopters'\n",
    "    elif avg_readiness > 2.0:\n",
    "        cluster_names[c] = 'Moderate Users'\n",
    "    else:\n",
    "        cluster_names[c] = 'Skeptical Users'\n",
    "\n",
    "df_full['UserType'] = df_full['Cluster'].map(cluster_names)\n",
    "print(\"\\n2. CLUSTER NAMES (based on readiness profile):\")\n",
    "for c, name in cluster_names.items():\n",
    "    n = (df_full['Cluster'] == c).sum()\n",
    "    avg = cluster_profiles.loc[c, cluster_vars[:-1]].mean()\n",
    "    print(f\"  Cluster {c}: {name} (n={n}, avg readiness={avg:.2f})\")\n",
    "\n",
    "# Define theme keywords (same as Phase 8)\n",
    "THEME_KEYWORDS = {\n",
    "    'Positive Experience': ['enjoy', 'love', 'great', 'helpful', 'useful', 'good', 'like', 'amazing', 'excellent', 'wonderful', 'beneficial'],\n",
    "    'Concerns/Caution': ['concern', 'worry', 'careful', 'caution', 'risk', 'afraid', 'fear', 'dangerous'],\n",
    "    'Accuracy/Reliability': ['accurate', 'accuracy', 'reliable', 'reliability', 'trust', 'correct', 'wrong', 'error', 'mistake', 'inaccurate'],\n",
    "    'Learning/Education': ['learn', 'study', 'school', 'education', 'research', 'student', 'homework', 'class'],\n",
    "    'Work/Productivity': ['work', 'job', 'task', 'productivity', 'efficient', 'time', 'fast', 'quick'],\n",
    "    'Ethics/Privacy': ['ethic', 'privacy', 'data', 'security', 'moral', 'honest', 'integrity']\n",
    "}\n",
    "\n",
    "# Function to detect themes in text\n",
    "def detect_themes(text):\n",
    "    if pd.isna(text) or not isinstance(text, str) or len(str(text).strip()) < 10:\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    detected = []\n",
    "    for theme, keywords in THEME_KEYWORDS.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            detected.append(theme)\n",
    "    return detected\n",
    "\n",
    "# Apply theme detection\n",
    "df_full['Themes'] = df_full['Open_Feedback'].apply(detect_themes)\n",
    "df_full['HasThemes'] = df_full['Themes'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "print(f\"\\n3. FEEDBACK ANALYSIS:\")\n",
    "print(f\"  Total responses with feedback: {df_full['Open_Feedback'].notna().sum()}\")\n",
    "print(f\"  Responses with detected themes: {df_full['HasThemes'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CROSS-TABULATION ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"4. CROSS-TABULATION: USER TYPE Ã— THEME PRESENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter to rows with valid user type and themes\n",
    "df_themed = df_full[df_full['UserType'].notna() & df_full['HasThemes']].copy()\n",
    "print(f\"\\nâœ“ {len(df_themed)} responses with both user type and detected themes\")\n",
    "\n",
    "# Create cross-tabulation for each theme\n",
    "crosstab_results = []\n",
    "\n",
    "for theme in THEME_KEYWORDS.keys():\n",
    "    # Create binary indicator for this theme\n",
    "    df_themed[f'Has_{theme}'] = df_themed['Themes'].apply(lambda x: theme in x)\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency = pd.crosstab(df_themed['UserType'], df_themed[f'Has_{theme}'])\n",
    "    \n",
    "    if contingency.shape[1] == 2:  # Has both True and False\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        \n",
    "        # Calculate percentages by user type\n",
    "        pcts = df_themed.groupby('UserType')[f'Has_{theme}'].mean() * 100\n",
    "        \n",
    "        crosstab_results.append({\n",
    "            'Theme': theme,\n",
    "            'Chi2': chi2,\n",
    "            'p_value': p,\n",
    "            'Significant': 'Yes' if p < 0.05 else 'No',\n",
    "            'Enthusiasts_%': pcts.get('AI Enthusiasts', 0),\n",
    "            'Cautious_%': pcts.get('Cautious Adopters', 0),\n",
    "            'Moderate_%': pcts.get('Moderate Users', 0),\n",
    "            'Anxious_%': pcts.get('Anxious Avoiders', 0)\n",
    "        })\n",
    "\n",
    "# Create results dataframe\n",
    "df_crosstab = pd.DataFrame(crosstab_results)\n",
    "print(\"\\nTheme Presence by User Type (%):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display formatted results\n",
    "if len(df_crosstab) > 0:\n",
    "    for _, row in df_crosstab.iterrows():\n",
    "        sig_marker = '*' if row['Significant'] == 'Yes' else ''\n",
    "        print(f\"\\n{row['Theme']}{sig_marker}\")\n",
    "        print(f\"  Enthusiasts: {row['Enthusiasts_%']:.1f}%  |  Cautious: {row['Cautious_%']:.1f}%  |  \"\n",
    "              f\"Moderate: {row['Moderate_%']:.1f}%  |  Anxious: {row['Anxious_%']:.1f}%\")\n",
    "        print(f\"  Ï‡Â² = {row['Chi2']:.2f}, p = {row['p_value']:.3f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. SUMMARY OF SIGNIFICANT ASSOCIATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "significant_themes = df_crosstab[df_crosstab['Significant'] == 'Yes']\n",
    "if len(significant_themes) > 0:\n",
    "    print(f\"\\n{len(significant_themes)} theme(s) showed significant association with user typology:\")\n",
    "    for _, row in significant_themes.iterrows():\n",
    "        print(f\"  â€¢ {row['Theme']}: Ï‡Â² = {row['Chi2']:.2f}, p = {row['p_value']:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo significant associations found between user typology and qualitative themes.\")\n",
    "    print(\"This suggests theme expression is relatively uniform across user types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4065e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# VISUALIZATION: USER TYPE Ã— THEME HEATMAP\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"6. VISUALIZATION: USER TYPE Ã— THEME HEATMAP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create heatmap data\n",
    "heatmap_data = df_crosstab[['Theme', 'Enthusiasts_%', 'Cautious_%', 'Moderate_%', 'Anxious_%']].copy()\n",
    "heatmap_data = heatmap_data.set_index('Theme')\n",
    "heatmap_data.columns = ['AI Enthusiasts', 'Cautious Adopters', 'Moderate Users', 'Anxious Avoiders']\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use imshow for heatmap\n",
    "im = ax.imshow(heatmap_data.values, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(range(len(heatmap_data.columns)))\n",
    "ax.set_xticklabels(heatmap_data.columns, rotation=45, ha='right')\n",
    "ax.set_yticks(range(len(heatmap_data.index)))\n",
    "ax.set_yticklabels(heatmap_data.index)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Theme Prevalence (%)')\n",
    "\n",
    "# Add value annotations\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        val = heatmap_data.values[i, j]\n",
    "        color = 'white' if val > 30 else 'black'\n",
    "        ax.text(j, i, f'{val:.0f}%', ha='center', va='center', color=color, fontsize=9)\n",
    "\n",
    "# Mark significant associations\n",
    "for i, (_, row) in enumerate(df_crosstab.iterrows()):\n",
    "    if row['Significant'] == 'Yes':\n",
    "        ax.text(-0.7, i, '*', ha='center', va='center', fontsize=14, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_title('Qualitative Theme Prevalence by User Typology\\n(* indicates Ï‡Â² p < .05)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/10_typology_theme_crosstab.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved: plots/10_typology_theme_crosstab.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c83c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# EXPORT CROSS-TABULATION RESULTS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"7. EXPORTING CROSS-TABULATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save crosstab results\n",
    "df_crosstab.to_csv('tables/10_typology_theme_crosstab.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: tables/10_typology_theme_crosstab.csv\")\n",
    "\n",
    "# Update final synthesis JSON\n",
    "synthesis['crosstab_analysis'] = {\n",
    "    'n_themed_responses': len(df_themed),\n",
    "    'themes_analyzed': list(THEME_KEYWORDS.keys()),\n",
    "    'significant_associations': len(significant_themes),\n",
    "    'interpretation': 'Theme expression varies by user typology' if len(significant_themes) > 0 \n",
    "                      else 'Theme expression is uniform across user types'\n",
    "}\n",
    "\n",
    "# Save updated synthesis\n",
    "with open('data/final_synthesis.json', 'w') as f:\n",
    "    json.dump(synthesis, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Updated: data/final_synthesis.json\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-TABULATION ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc575c",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# APA-Style Conclusions\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Summary of Key Findings\n",
    "\n",
    "This study validated the AIRS instrument, an 8-factor, 16-item measure extending UTAUT2 with AI Trust for enterprise AI adoption contexts. The primary contributions are:\n",
    "\n",
    "1. **Price Value Strongest Predictor**: Unexpectedly, Price Value emerged as the strongest predictor of behavioral intention (Î² = .505***), suggesting cost-benefit analysis drives AI adoption even when tools are free.\n",
    "\n",
    "2. **AI Trust Marginal Support**: AI Trust approached significance (Î² = .106, p = .064), providing tentative support for the theoretical extension and highlighting the role of trust in AI adoption.\n",
    "\n",
    "3. **Partial UTAUT2 Support**: Three of seven UTAUT2 paths were significant (SI, HM, PV â†’ BI). The non-significant paths (PE, EE, FC, HB) may reflect the current AI landscape where tools are user-friendly and not requiring extensive organizational support.\n",
    "\n",
    "4. **Experience as Novel Moderator**: Professional experience moderates key adoption paths, with PE and HM effects strengthening for more experienced professionals. This extends UTAUT2 literature with career development considerations.\n",
    "\n",
    "5. **Inclusive Design Implications**: The association between disability status and AI anxiety (d = .36) signals the need for accessibility-focused AI design research.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Four originally hypothesized constructs required item redesign\n",
    "- Some moderation analyses were underpowered (n < 100 per group)\n",
    "- Cross-sectional design precludes causal inference\n",
    "- Self-reported behavioral intention vs. actual behavior\n",
    "- AI Trust marginally significant (p = .064) - requires replication\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "1. Emphasize value proposition and cost-benefit in AI adoption messaging\n",
    "2. Build trust through transparency and reliability demonstrations\n",
    "3. Tailor messaging by user segment and career stage\n",
    "4. Address accessibility concerns to reduce AI anxiety\n",
    "\n",
    "---\n",
    "\n",
    "*Phase 10 Analysis Complete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
