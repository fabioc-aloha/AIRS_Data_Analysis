{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978f3c11",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NOTEBOOK 06: MODERATION ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Purpose**: Test contextual moderation of AIRS structural paths\n",
    "\n",
    "**Design Decisions**:\n",
    "- **Sample**: Full sample (all roles)\n",
    "- **Approach**: Multi-group SEM + regression-based interaction tests\n",
    "- **Moderators Tested**: Usage Frequency, Voluntariness, Disability, Industry, Education, Experience\n",
    "\n",
    "**Hypotheses**:\n",
    "- **H4c**: Usage frequency moderates HB â†’ BI (stronger for frequent users)\n",
    "- **H4e**: Voluntariness moderates SI â†’ BI and FC â†’ BI (stronger in mandated contexts)\n",
    "- **H4f**: Disability moderates EE â†’ BI and FC â†’ BI (stronger for persons with disabilities)\n",
    "- **Exploratory**: Experience, Education, Industry moderation of key paths\n",
    "\n",
    "**Analysis Steps**:\n",
    "1. Load validated factor structure and structural results\n",
    "2. Create moderator groups (median splits, categorical groupings)\n",
    "3. Fit multi-group SEM models\n",
    "4. Calculate path differences with z-tests\n",
    "5. Regression-based interaction tests for continuous moderators\n",
    "\n",
    "**Outputs**:\n",
    "- Multi-group path coefficients with significance tests\n",
    "- Moderation effect tables\n",
    "- `data/moderation_analysis_results.json` - Complete moderation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e8f55",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4c1846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 6: MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Environment configured (seed=67)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# CRITICAL FIX: Prevent OpenMP runtime conflicts\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from semopy import Model\n",
    "from semopy.stats import calc_stats\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "RANDOM_SEED = 67\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 6: MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Environment configured (seed={RANDOM_SEED})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dbf53",
   "metadata": {},
   "source": [
    "## 1.2 Load Data and Phase 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec679ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Full sample: N = 523\n",
      "ğŸ“Š EFA sample: N = 261\n",
      "ğŸ“Š CFA sample: N = 262\n",
      "\n",
      "================================================================================\n",
      "MODERATOR VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Usage frequency columns: ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
      "ğŸ“‹ Voluntariness items: ['VO1', 'VO2']\n",
      "\n",
      "â™¿ Disability distribution:\n",
      "Disability\n",
      "No                      444\n",
      "Yes                      68\n",
      "Prefer not to answer     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load full sample data\n",
    "df_efa = pd.read_csv('data/AIRS_experiment.csv')\n",
    "df_cfa = pd.read_csv('data/AIRS_holdout.csv')\n",
    "df_full = pd.concat([df_efa, df_cfa], ignore_index=True)\n",
    "\n",
    "# Load Phase 4 results\n",
    "with open('data/structural_model_results.json', 'r') as f:\n",
    "    phase4_results = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Š Full sample: N = {len(df_full)}\")\n",
    "print(f\"ğŸ“Š EFA sample: N = {len(df_efa)}\")\n",
    "print(f\"ğŸ“Š CFA sample: N = {len(df_cfa)}\")\n",
    "\n",
    "# Check available moderator variables\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODERATOR VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Usage frequency columns\n",
    "usage_cols = [c for c in df_full.columns if 'Usage' in c]\n",
    "print(f\"\\nğŸ“ˆ Usage frequency columns: {usage_cols}\")\n",
    "\n",
    "# Voluntariness items (VO1, VO2 - dropped from model but can use as moderators)\n",
    "vo_cols = [c for c in df_full.columns if c.startswith('VO')]\n",
    "print(f\"ğŸ“‹ Voluntariness items: {vo_cols}\")\n",
    "\n",
    "# Disability\n",
    "if 'Disability' in df_full.columns:\n",
    "    print(f\"\\nâ™¿ Disability distribution:\")\n",
    "    print(df_full['Disability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea902c86",
   "metadata": {},
   "source": [
    "## 1.3 Create Moderator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d77e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING MODERATOR VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ USAGE FREQUENCY (H4c):\n",
      "   Max usage distribution:\n",
      "Usage_Group\n",
      "Low       101\n",
      "Medium    116\n",
      "High      306\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“‹ VOLUNTARINESS (H4e):\n",
      "   VO1 mean: 3.46 (SD=1.27)\n",
      "   VO2 mean: 3.84 (SD=1.10)\n",
      "   VO1-VO2 correlation: r = 0.282\n",
      "   Voluntariness groups:\n",
      "Voluntariness_Group\n",
      "Mandated      71\n",
      "Mixed        169\n",
      "Voluntary    283\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â™¿ DISABILITY STATUS (H4f):\n",
      "   Yes: N = 68\n",
      "   No: N = 444\n",
      "   Prefer not to answer (excluded): N = 11\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CREATE MODERATOR VARIABLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING MODERATOR VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. USAGE FREQUENCY: Create aggregate usage score (max across all tools)\n",
    "# Scale: 1=Never, 2=Rarely, 3=Sometimes, 4=Often, 5=Very often (daily)\n",
    "df_full['Usage_Max'] = df_full[['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']].max(axis=1)\n",
    "df_full['Usage_Mean'] = df_full[['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']].mean(axis=1)\n",
    "\n",
    "# Create categorical: Low (1-2), Medium (3), High (4-5)\n",
    "df_full['Usage_Group'] = pd.cut(df_full['Usage_Max'], \n",
    "                                 bins=[0, 2, 3, 5], \n",
    "                                 labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"\\nğŸ“ˆ USAGE FREQUENCY (H4c):\")\n",
    "print(f\"   Max usage distribution:\")\n",
    "print(df_full['Usage_Group'].value_counts().sort_index())\n",
    "\n",
    "# 2. VOLUNTARINESS: Use VO1 and VO2 items (dropped from model, but valid as moderators)\n",
    "# VO1: \"I choose to use AI tools because I find them helpful, not because I am required to\"\n",
    "# VO2: \"I could choose not to use AI tools if I preferred\"\n",
    "# Higher scores = more voluntary use; Lower scores = more mandated\n",
    "\n",
    "# Mean of VO1 and VO2 (note: they measure different facets)\n",
    "df_full['Voluntariness_Mean'] = df_full[['VO1', 'VO2']].mean(axis=1)\n",
    "\n",
    "# Create categorical: Low (1-2.5), Medium (2.5-3.5), High (3.5-5)\n",
    "df_full['Voluntariness_Group'] = pd.cut(df_full['Voluntariness_Mean'], \n",
    "                                         bins=[0, 2.5, 3.5, 5], \n",
    "                                         labels=['Mandated', 'Mixed', 'Voluntary'])\n",
    "\n",
    "print(\"\\nğŸ“‹ VOLUNTARINESS (H4e):\")\n",
    "print(f\"   VO1 mean: {df_full['VO1'].mean():.2f} (SD={df_full['VO1'].std():.2f})\")\n",
    "print(f\"   VO2 mean: {df_full['VO2'].mean():.2f} (SD={df_full['VO2'].std():.2f})\")\n",
    "print(f\"   VO1-VO2 correlation: r = {df_full['VO1'].corr(df_full['VO2']):.3f}\")\n",
    "print(f\"   Voluntariness groups:\")\n",
    "print(df_full['Voluntariness_Group'].value_counts().sort_index())\n",
    "\n",
    "# 3. DISABILITY STATUS: Binary (Yes vs No, exclude \"Prefer not to answer\")\n",
    "df_full['Disability_Binary'] = df_full['Disability'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(\"\\nâ™¿ DISABILITY STATUS (H4f):\")\n",
    "print(f\"   Yes: N = {(df_full['Disability'] == 'Yes').sum()}\")\n",
    "print(f\"   No: N = {(df_full['Disability'] == 'No').sum()}\")\n",
    "print(f\"   Prefer not to answer (excluded): N = {(df_full['Disability'] == 'Prefer not to answer').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c1d74",
   "metadata": {},
   "source": [
    "## 1.4 Define Model D Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64608292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded CFA results from tables/cfa_summary.json\n",
      "  â†’ 8 factors loaded from CFA validation\n",
      "\n",
      "================================================================================\n",
      "MODEL D STRUCTURE (from CFA validation)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š 8 factors, 16 predictor items + 4 BI items\n",
      "   PerfExp: ['PE1', 'PE2'] (Î±=0.80)\n",
      "   EffortExp: ['EE1', 'EE2'] (Î±=0.86)\n",
      "   SocialInf: ['SI1', 'SI2'] (Î±=0.75)\n",
      "   FacilCond: ['FC1', 'FC2'] (Î±=0.74)\n",
      "   HedonicMot: ['HM1', 'HM2'] (Î±=0.86)\n",
      "   PriceValue: ['PV1', 'PV2'] (Î±=0.88)\n",
      "   Habit: ['HB1', 'HB2'] (Î±=0.91)\n",
      "   AITrust: ['TR1', 'TR2'] (Î±=0.89)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LOAD VALIDATED MODEL STRUCTURE FROM CFA (Notebook 02)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "try:\n",
    "    with open('tables/cfa_summary.json', 'r') as f:\n",
    "        cfa_results = json.load(f)\n",
    "    print(\"âœ“ Loaded CFA results from tables/cfa_summary.json\")\n",
    "    \n",
    "    # Use the validated factor structure from CFA\n",
    "    MODEL_D_STRUCTURE = cfa_results['factor_structure']\n",
    "    \n",
    "    # Extract reliability for reference\n",
    "    cfa_reliability = {\n",
    "        item['Factor']: item['Alpha'] \n",
    "        for item in cfa_results.get('reliability', [])\n",
    "    }\n",
    "    \n",
    "    print(f\"  â†’ {len(MODEL_D_STRUCTURE)} factors loaded from CFA validation\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ CFA results not found - using hardcoded fallback structure\")\n",
    "    print(\"   Run 02_CFA_Experiment.ipynb first for dynamic loading\")\n",
    "    \n",
    "    # Fallback hardcoded structure (for backward compatibility)\n",
    "    MODEL_D_STRUCTURE = {\n",
    "        'PerfExp': ['PE1', 'PE2'],\n",
    "        'EffortExp': ['EE1', 'EE2'],\n",
    "        'SocialInf': ['SI1', 'SI2'],\n",
    "        'FacilCond': ['FC1', 'FC2'],\n",
    "        'HedonicMot': ['HM1', 'HM2'],\n",
    "        'PriceValue': ['PV1', 'PV2'],\n",
    "        'Habit': ['HB1', 'HB2'],\n",
    "        'AITrust': ['TR1', 'TR2']\n",
    "    }\n",
    "    cfa_reliability = {}\n",
    "\n",
    "BI_ITEMS = ['BI1', 'BI2', 'BI3', 'BI4']\n",
    "\n",
    "# Get all model items\n",
    "MODEL_D_ITEMS = []\n",
    "for items in MODEL_D_STRUCTURE.values():\n",
    "    MODEL_D_ITEMS.extend(items)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL D STRUCTURE (from CFA validation)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Š {len(MODEL_D_STRUCTURE)} factors, {len(MODEL_D_ITEMS)} predictor items + {len(BI_ITEMS)} BI items\")\n",
    "for factor, items in MODEL_D_STRUCTURE.items():\n",
    "    alpha = cfa_reliability.get(factor, None)\n",
    "    alpha_str = f\"(Î±={alpha:.2f})\" if alpha else \"\"\n",
    "    print(f\"   {factor}: {items} {alpha_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80de30",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 2: H4c - Usage Frequency Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4c**: The effect of Habit (HB) on Behavioral Intention (BI) is stronger for high-frequency AI users.\n",
    "\n",
    "**Rationale**: Users who interact with AI tools more frequently have had more opportunities to develop habitual usage patterns. For them, habit should be a stronger predictor of continued intention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e6a76",
   "metadata": {},
   "source": [
    "## 2.1 Multi-Group SEM: Usage Frequency Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1a6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4c: USAGE FREQUENCY MODERATION\n",
      "================================================================================\n",
      "\n",
      "Hypothesis: HB â†’ BI is stronger for high-frequency users\n",
      "\n",
      "ğŸ“Š Low usage group: N = 101\n",
      "ğŸ“Š High usage group: N = 306\n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Low Usage       High Usage     \n",
      "----------------------------------------\n",
      "N               101             306            \n",
      "CFI             0.935           0.937          \n",
      "RMSEA           0.086           0.070          \n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Low Usage       High Usage     \n",
      "----------------------------------------\n",
      "N               101             306            \n",
      "CFI             0.935           0.937          \n",
      "RMSEA           0.086           0.070          \n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4c: USAGE FREQUENCY MODERATION (HB â†’ BI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def build_sem_syntax():\n",
    "    \"\"\"Build SEM syntax for structural model\"\"\"\n",
    "    # Measurement model\n",
    "    measurement = []\n",
    "    for factor, items in MODEL_D_STRUCTURE.items():\n",
    "        measurement.append(f\"{factor} =~ {' + '.join(items)}\")\n",
    "    \n",
    "    # BI measurement\n",
    "    measurement.append(f\"BI =~ {' + '.join(BI_ITEMS)}\")\n",
    "    \n",
    "    # Structural paths (all 8 predictors â†’ BI)\n",
    "    structural = \"BI ~ PerfExp + EffortExp + SocialInf + FacilCond + HedonicMot + PriceValue + Habit + AITrust\"\n",
    "    \n",
    "    return '\\n'.join(measurement) + '\\n' + structural\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4c: USAGE FREQUENCY MODERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHypothesis: HB â†’ BI is stronger for high-frequency users\")\n",
    "\n",
    "# Split by usage frequency\n",
    "df_low_usage = df_full[df_full['Usage_Group'] == 'Low'].copy()\n",
    "df_high_usage = df_full[df_full['Usage_Group'] == 'High'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Low usage group: N = {len(df_low_usage)}\")\n",
    "print(f\"ğŸ“Š High usage group: N = {len(df_high_usage)}\")\n",
    "\n",
    "# Fit SEM to each group\n",
    "all_items = MODEL_D_ITEMS + BI_ITEMS\n",
    "sem_syntax = build_sem_syntax()\n",
    "\n",
    "# Low usage group\n",
    "model_low = Model(sem_syntax)\n",
    "model_low.fit(df_low_usage[all_items])\n",
    "params_low = model_low.inspect()\n",
    "stats_low = calc_stats(model_low)\n",
    "\n",
    "# High usage group  \n",
    "model_high = Model(sem_syntax)\n",
    "model_high.fit(df_high_usage[all_items])\n",
    "params_high = model_high.inspect()\n",
    "stats_high = calc_stats(model_high)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Model Fit Comparison\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Metric':<15} {'Low Usage':<15} {'High Usage':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'N':<15} {len(df_low_usage):<15} {len(df_high_usage):<15}\")\n",
    "print(f\"{'CFI':<15} {stats_low['CFI'].values[0]:<15.3f} {stats_high['CFI'].values[0]:<15.3f}\")\n",
    "print(f\"{'RMSEA':<15} {stats_low['RMSEA'].values[0]:<15.3f} {stats_high['RMSEA'].values[0]:<15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee75cf5",
   "metadata": {},
   "source": [
    "## 2.2 Compare HB â†’ BI Path Across Usage Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de540fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURAL PATH COMPARISON: Low vs High Usage\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Predictor    Î²_Low      p_Low      Î²_High     p_High     Î”Î²        \n",
      "------------------------------------------------------------------------------------------\n",
      "PerfExp         0.371*     0.048   -0.270      0.376   -0.641\n",
      "EffortExp       0.137      0.351   -0.007      0.951   -0.145\n",
      "SocialInf      -0.053      0.743    0.096      0.386    0.149\n",
      "FacilCond      -0.125      0.340    0.172      0.154    0.298\n",
      "HedonicMot      0.280      0.314    0.239      0.197   -0.041\n",
      "PriceValue      0.224      0.321    0.878*     0.002    0.653\n",
      "Habit           0.116      0.274   -0.137      0.115   -0.253\n",
      "AITrust         0.008      0.928    0.022      0.841    0.014\n",
      "\n",
      "================================================================================\n",
      "H4c FOCAL TEST: Habit â†’ BI\n",
      "================================================================================\n",
      "\n",
      "   Low Usage:  Î² = 0.116 (p = 0.274)\n",
      "   High Usage: Î² = -0.137 (p = 0.115)\n",
      "   Î”Î² = -0.253\n",
      "\n",
      "   Z-test for difference: z = -1.844, p = 0.065\n",
      "\n",
      "   âœ— H4c NOT SUPPORTED: No significant difference in HB â†’ BI across usage groups\n"
     ]
    }
   ],
   "source": [
    "# Extract structural paths for comparison\n",
    "def get_structural_paths(params):\n",
    "    \"\"\"Extract structural path coefficients\"\"\"\n",
    "    struct_paths = params[(params['op'] == '~') & (params['lval'] == 'BI')].copy()\n",
    "    return struct_paths[['rval', 'Estimate', 'Std. Err', 'p-value']]\n",
    "\n",
    "paths_low = get_structural_paths(params_low)\n",
    "paths_high = get_structural_paths(params_high)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURAL PATH COMPARISON: Low vs High Usage\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge for comparison\n",
    "comparison = paths_low.merge(paths_high, on='rval', suffixes=('_Low', '_High'))\n",
    "comparison['Î”Î²'] = comparison['Estimate_High'] - comparison['Estimate_Low']\n",
    "\n",
    "print(\"\\n\" + \"-\" * 90)\n",
    "print(f\"{'Predictor':<12} {'Î²_Low':<10} {'p_Low':<10} {'Î²_High':<10} {'p_High':<10} {'Î”Î²':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for _, row in comparison.iterrows():\n",
    "    sig_low = '*' if row['p-value_Low'] < 0.05 else ''\n",
    "    sig_high = '*' if row['p-value_High'] < 0.05 else ''\n",
    "    print(f\"{row['rval']:<12} {row['Estimate_Low']:>8.3f}{sig_low:<2} {row['p-value_Low']:>8.3f} \"\n",
    "          f\"{row['Estimate_High']:>8.3f}{sig_high:<2} {row['p-value_High']:>8.3f} {row['Î”Î²']:>8.3f}\")\n",
    "\n",
    "# Focus on Habit (H4c target)\n",
    "hb_low = comparison[comparison['rval'] == 'Habit'].iloc[0]\n",
    "hb_high_beta = hb_low['Estimate_High']\n",
    "hb_low_beta = hb_low['Estimate_Low']\n",
    "delta_hb = hb_low['Î”Î²']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4c FOCAL TEST: Habit â†’ BI\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n   Low Usage:  Î² = {hb_low_beta:.3f} (p = {hb_low['p-value_Low']:.3f})\")\n",
    "print(f\"   High Usage: Î² = {hb_high_beta:.3f} (p = {hb_low['p-value_High']:.3f})\")\n",
    "print(f\"   Î”Î² = {delta_hb:.3f}\")\n",
    "\n",
    "# Statistical test for path difference (z-test)\n",
    "se_low = hb_low['Std. Err_Low']\n",
    "se_high = hb_low['Std. Err_High']\n",
    "pooled_se = np.sqrt(se_low**2 + se_high**2)\n",
    "z_diff = delta_hb / pooled_se\n",
    "p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "\n",
    "print(f\"\\n   Z-test for difference: z = {z_diff:.3f}, p = {p_diff:.3f}\")\n",
    "\n",
    "if p_diff < 0.05:\n",
    "    if delta_hb > 0:\n",
    "        print(\"\\n   âœ“ H4c SUPPORTED: HB â†’ BI is significantly STRONGER for high-frequency users\")\n",
    "    else:\n",
    "        print(\"\\n   âœ— H4c NOT SUPPORTED: HB â†’ BI is significantly WEAKER for high-frequency users\")\n",
    "else:\n",
    "    print(\"\\n   âœ— H4c NOT SUPPORTED: No significant difference in HB â†’ BI across usage groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e859b6",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 3: H4e - Voluntariness Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4e**: The effects of Social Influence (SI â†’ BI) and Facilitating Conditions (FC â†’ BI) are stronger in mandated (low voluntariness) contexts.\n",
    "\n",
    "**Rationale**: When AI use is mandatory or expected, social pressure and organizational support become more influential because individuals cannot simply opt out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf624865",
   "metadata": {},
   "source": [
    "## 3.1 Multi-Group SEM: Voluntariness Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3709a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4e: VOLUNTARINESS MODERATION\n",
      "================================================================================\n",
      "\n",
      "Hypothesis: SI â†’ BI and FC â†’ BI are stronger in mandated contexts\n",
      "\n",
      "ğŸ“Š Mandated group (low VO): N = 71\n",
      "ğŸ“Š Voluntary group (high VO): N = 283\n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Mandated        Voluntary      \n",
      "----------------------------------------\n",
      "N               71              283            \n",
      "CFI             0.952           0.941          \n",
      "RMSEA           0.084           0.070          \n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Mandated        Voluntary      \n",
      "----------------------------------------\n",
      "N               71              283            \n",
      "CFI             0.952           0.941          \n",
      "RMSEA           0.084           0.070          \n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4e: VOLUNTARINESS MODERATION (SI â†’ BI, FC â†’ BI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4e: VOLUNTARINESS MODERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHypothesis: SI â†’ BI and FC â†’ BI are stronger in mandated contexts\")\n",
    "\n",
    "# Split by voluntariness (Mandated vs Voluntary, excluding Mixed for cleaner contrast)\n",
    "df_mandated = df_full[df_full['Voluntariness_Group'] == 'Mandated'].copy()\n",
    "df_voluntary = df_full[df_full['Voluntariness_Group'] == 'Voluntary'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Mandated group (low VO): N = {len(df_mandated)}\")\n",
    "print(f\"ğŸ“Š Voluntary group (high VO): N = {len(df_voluntary)}\")\n",
    "\n",
    "# Check if mandated group has sufficient N\n",
    "if len(df_mandated) < 50:\n",
    "    print(f\"\\nâš ï¸ WARNING: Mandated group N={len(df_mandated)} is small. Results should be interpreted cautiously.\")\n",
    "\n",
    "# Fit SEM to each group\n",
    "model_mandated = Model(sem_syntax)\n",
    "model_mandated.fit(df_mandated[all_items])\n",
    "params_mandated = model_mandated.inspect()\n",
    "stats_mandated = calc_stats(model_mandated)\n",
    "\n",
    "model_voluntary = Model(sem_syntax)\n",
    "model_voluntary.fit(df_voluntary[all_items])\n",
    "params_voluntary = model_voluntary.inspect()\n",
    "stats_voluntary = calc_stats(model_voluntary)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Model Fit Comparison\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Metric':<15} {'Mandated':<15} {'Voluntary':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'N':<15} {len(df_mandated):<15} {len(df_voluntary):<15}\")\n",
    "print(f\"{'CFI':<15} {stats_mandated['CFI'].values[0]:<15.3f} {stats_voluntary['CFI'].values[0]:<15.3f}\")\n",
    "print(f\"{'RMSEA':<15} {stats_mandated['RMSEA'].values[0]:<15.3f} {stats_voluntary['RMSEA'].values[0]:<15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56cae6",
   "metadata": {},
   "source": [
    "## 3.2 Compare SI â†’ BI and FC â†’ BI Across Voluntariness Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888eb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURAL PATH COMPARISON: Mandated vs Voluntary\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Predictor    Î²_Mand     p_Mand     Î²_Vol      p_Vol      Î”Î²        \n",
      "------------------------------------------------------------------------------------------\n",
      "PerfExp        -0.030      0.939    0.168      0.941   -0.198\n",
      "EffortExp       0.018      0.772   -1.256      0.705    1.274\n",
      "SocialInf       0.255      0.451   -0.373      0.775    0.628\n",
      "FacilCond       0.101      0.527    1.178      0.705   -1.076\n",
      "HedonicMot      0.229      0.413   -0.519      0.868    0.748\n",
      "PriceValue      0.249      0.203    2.185      0.589   -1.936\n",
      "Habit           0.015      0.896   -0.327      0.776    0.342\n",
      "AITrust         0.151      0.312    0.137      0.839    0.014\n",
      "\n",
      "================================================================================\n",
      "H4e FOCAL TESTS\n",
      "================================================================================\n",
      "\n",
      "SocialInf â†’ BI:\n",
      "   Mandated:  Î² = 0.255 (p = 0.451)\n",
      "   Voluntary: Î² = -0.373 (p = 0.775)\n",
      "   Î”Î² = 0.628\n",
      "   Z-test: z = 0.466, p = 0.641\n",
      "   âœ— H4e NOT SUPPORTED: No significant difference\n",
      "\n",
      "FacilCond â†’ BI:\n",
      "   Mandated:  Î² = 0.101 (p = 0.527)\n",
      "   Voluntary: Î² = 1.178 (p = 0.705)\n",
      "   Î”Î² = -1.076\n",
      "   Z-test: z = -0.346, p = 0.730\n",
      "   âœ— H4e NOT SUPPORTED: No significant difference\n"
     ]
    }
   ],
   "source": [
    "# Extract and compare paths\n",
    "paths_mandated = get_structural_paths(params_mandated)\n",
    "paths_voluntary = get_structural_paths(params_voluntary)\n",
    "\n",
    "comparison_vo = paths_mandated.merge(paths_voluntary, on='rval', suffixes=('_Mand', '_Vol'))\n",
    "comparison_vo['Î”Î²'] = comparison_vo['Estimate_Mand'] - comparison_vo['Estimate_Vol']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURAL PATH COMPARISON: Mandated vs Voluntary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 90)\n",
    "print(f\"{'Predictor':<12} {'Î²_Mand':<10} {'p_Mand':<10} {'Î²_Vol':<10} {'p_Vol':<10} {'Î”Î²':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for _, row in comparison_vo.iterrows():\n",
    "    sig_m = '*' if row['p-value_Mand'] < 0.05 else ''\n",
    "    sig_v = '*' if row['p-value_Vol'] < 0.05 else ''\n",
    "    print(f\"{row['rval']:<12} {row['Estimate_Mand']:>8.3f}{sig_m:<2} {row['p-value_Mand']:>8.3f} \"\n",
    "          f\"{row['Estimate_Vol']:>8.3f}{sig_v:<2} {row['p-value_Vol']:>8.3f} {row['Î”Î²']:>8.3f}\")\n",
    "\n",
    "# Focus on SI and FC (H4e targets)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4e FOCAL TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for construct in ['SocialInf', 'FacilCond']:\n",
    "    row = comparison_vo[comparison_vo['rval'] == construct].iloc[0]\n",
    "    delta = row['Î”Î²']\n",
    "    se_m = row['Std. Err_Mand']\n",
    "    se_v = row['Std. Err_Vol']\n",
    "    pooled_se = np.sqrt(se_m**2 + se_v**2)\n",
    "    z_diff = delta / pooled_se\n",
    "    p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "    \n",
    "    print(f\"\\n{construct} â†’ BI:\")\n",
    "    print(f\"   Mandated:  Î² = {row['Estimate_Mand']:.3f} (p = {row['p-value_Mand']:.3f})\")\n",
    "    print(f\"   Voluntary: Î² = {row['Estimate_Vol']:.3f} (p = {row['p-value_Vol']:.3f})\")\n",
    "    print(f\"   Î”Î² = {delta:.3f}\")\n",
    "    print(f\"   Z-test: z = {z_diff:.3f}, p = {p_diff:.3f}\")\n",
    "    \n",
    "    if p_diff < 0.05:\n",
    "        if delta > 0:\n",
    "            print(f\"   âœ“ H4e SUPPORTED: {construct} â†’ BI is STRONGER in mandated contexts\")\n",
    "        else:\n",
    "            print(f\"   âœ— H4e NOT SUPPORTED: {construct} â†’ BI is WEAKER in mandated contexts\")\n",
    "    else:\n",
    "        print(f\"   âœ— H4e NOT SUPPORTED: No significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe47228",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 4: H4f - Disability Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4f**: The effects of Effort Expectancy (EE â†’ BI) and Facilitating Conditions (FC â†’ BI) are stronger for persons with disabilities.\n",
    "\n",
    "**Rationale**: Accessibility considerations may make ease of use and organizational support more critical for persons with disabilities when adopting AI tools.\n",
    "\n",
    "**Note**: This is an exploratory analysis with smaller N in the disability group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59dc876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4f: DISABILITY MODERATION\n",
      "Testing: EE â†’ BI and FC â†’ BI stronger for persons with disabilities\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sample Sizes:\n",
      "   No Disability: N = 444\n",
      "   Has Disability: N = 68\n",
      "   (Excluded 'Prefer not to answer')\n",
      "\n",
      "ğŸ”„ Fitting SEM models...\n",
      "   âœ“ No Disability group fitted\n",
      "   âœ“ Disability group fitted\n",
      "   âœ“ No Disability group fitted\n",
      "   âœ“ Disability group fitted\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4f: Multi-Group SEM - Disability Status\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4f: DISABILITY MODERATION\")\n",
    "print(\"Testing: EE â†’ BI and FC â†’ BI stronger for persons with disabilities\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split by disability status (using original Disability column)\n",
    "df_no_disability = df_full[df_full['Disability'] == 'No'].copy()\n",
    "df_disability = df_full[df_full['Disability'] == 'Yes'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Sample Sizes:\")\n",
    "print(f\"   No Disability: N = {len(df_no_disability)}\")\n",
    "print(f\"   Has Disability: N = {len(df_disability)}\")\n",
    "print(f\"   (Excluded 'Prefer not to answer')\")\n",
    "\n",
    "# Fit models for each group\n",
    "print(\"\\nğŸ”„ Fitting SEM models...\")\n",
    "\n",
    "model_no_disability = Model(sem_syntax)\n",
    "model_no_disability.fit(df_no_disability)\n",
    "print(\"   âœ“ No Disability group fitted\")\n",
    "\n",
    "model_disability = Model(sem_syntax)\n",
    "model_disability.fit(df_disability)\n",
    "print(\"   âœ“ Disability group fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdd47c",
   "metadata": {},
   "source": [
    "### H4f Path Comparison: EE â†’ BI and FC â†’ BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085b6b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "H4f STRUCTURAL PATH COMPARISON: EEâ†’BI and FCâ†’BI\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Path Coefficient Comparison (Disability vs No Disability):\n",
      "   Path No Disability Î² No Disability SE Disability Î² Disability SE    Î”Î²     z     p\n",
      "EE â†’ BI          -0.053            0.073       -0.012         0.480 0.041 0.084 0.933\n",
      "FC â†’ BI           0.084            0.086        0.407         1.083 0.323 0.297 0.766\n",
      "\n",
      "================================================================================\n",
      "H4f INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Hypothesis H4f predicted:\n",
      "   â€¢ EE â†’ BI stronger for persons with disabilities (accessibility)\n",
      "   â€¢ FC â†’ BI stronger for persons with disabilities (support needs)\n",
      "\n",
      "   âœ— EE â†’ BI: NO SIGNIFICANT MODERATION (z=0.084, p=0.933)\n",
      "\n",
      "   âœ— FC â†’ BI: NO SIGNIFICANT MODERATION (z=0.297, p=0.766)\n",
      "\n",
      "âš ï¸  NOTE: Disability group (N=68) is small, limiting statistical power\n",
      "\n",
      "H4f STRUCTURAL PATH COMPARISON: EEâ†’BI and FCâ†’BI\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Path Coefficient Comparison (Disability vs No Disability):\n",
      "   Path No Disability Î² No Disability SE Disability Î² Disability SE    Î”Î²     z     p\n",
      "EE â†’ BI          -0.053            0.073       -0.012         0.480 0.041 0.084 0.933\n",
      "FC â†’ BI           0.084            0.086        0.407         1.083 0.323 0.297 0.766\n",
      "\n",
      "================================================================================\n",
      "H4f INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Hypothesis H4f predicted:\n",
      "   â€¢ EE â†’ BI stronger for persons with disabilities (accessibility)\n",
      "   â€¢ FC â†’ BI stronger for persons with disabilities (support needs)\n",
      "\n",
      "   âœ— EE â†’ BI: NO SIGNIFICANT MODERATION (z=0.084, p=0.933)\n",
      "\n",
      "   âœ— FC â†’ BI: NO SIGNIFICANT MODERATION (z=0.297, p=0.766)\n",
      "\n",
      "âš ï¸  NOTE: Disability group (N=68) is small, limiting statistical power\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4f: Path Comparison - EE â†’ BI and FC â†’ BI \n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Get parameter estimates\n",
    "params_no_disability = model_no_disability.inspect()\n",
    "params_disability = model_disability.inspect()\n",
    "\n",
    "# Extract structural paths\n",
    "paths_no_disability = get_structural_paths(params_no_disability)\n",
    "paths_disability = get_structural_paths(params_disability)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4f STRUCTURAL PATH COMPARISON: EEâ†’BI and FCâ†’BI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison for H4f paths (use factor names as in model)\n",
    "h4f_paths = ['EffortExp', 'FacilCond']  # EEâ†’BI and FCâ†’BI\n",
    "path_labels = {'EffortExp': 'EE â†’ BI', 'FacilCond': 'FC â†’ BI'}\n",
    "comparison_disability = []\n",
    "\n",
    "for predictor in h4f_paths:\n",
    "    nd_row = paths_no_disability[paths_no_disability['rval'] == predictor]\n",
    "    d_row = paths_disability[paths_disability['rval'] == predictor]\n",
    "    \n",
    "    if len(nd_row) > 0 and len(d_row) > 0:\n",
    "        beta_nd = nd_row['Estimate'].values[0]\n",
    "        se_nd = nd_row['Std. Err'].values[0]\n",
    "        p_nd = nd_row['p-value'].values[0]\n",
    "        sig_nd = \"***\" if p_nd < .001 else \"**\" if p_nd < .01 else \"*\" if p_nd < .05 else \"\"\n",
    "        \n",
    "        beta_d = d_row['Estimate'].values[0]\n",
    "        se_d = d_row['Std. Err'].values[0]\n",
    "        p_d = d_row['p-value'].values[0]\n",
    "        sig_d = \"***\" if p_d < .001 else \"**\" if p_d < .01 else \"*\" if p_d < .05 else \"\"\n",
    "        \n",
    "        # Z-test for difference  \n",
    "        pooled_se = np.sqrt(se_nd**2 + se_d**2)\n",
    "        delta = beta_d - beta_nd  # Disability - No Disability (H4f expects positive)\n",
    "        z_diff = delta / pooled_se\n",
    "        p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "        sig_diff = \"***\" if p_diff < .001 else \"**\" if p_diff < .01 else \"*\" if p_diff < .05 else \"\"\n",
    "        \n",
    "        comparison_disability.append({\n",
    "            'Path': path_labels[predictor],\n",
    "            'No Disability Î²': f\"{beta_nd:.3f}{sig_nd}\",\n",
    "            'No Disability SE': f\"{se_nd:.3f}\",\n",
    "            'Disability Î²': f\"{beta_d:.3f}{sig_d}\",\n",
    "            'Disability SE': f\"{se_d:.3f}\",\n",
    "            'Î”Î²': f\"{delta:.3f}\",\n",
    "            'z': f\"{z_diff:.3f}\",\n",
    "            'p': f\"{p_diff:.3f}{sig_diff}\"\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_disability)\n",
    "print(\"\\nğŸ“Š Path Coefficient Comparison (Disability vs No Disability):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4f INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ” Hypothesis H4f predicted:\")\n",
    "print(\"   â€¢ EE â†’ BI stronger for persons with disabilities (accessibility)\")\n",
    "print(\"   â€¢ FC â†’ BI stronger for persons with disabilities (support needs)\")\n",
    "\n",
    "for _, row in pd.DataFrame(comparison_disability).iterrows():\n",
    "    path = row['Path']\n",
    "    z_val = float(row['z'])\n",
    "    p_val = float(row['p'].rstrip('*'))\n",
    "    delta_val = float(row['Î”Î²'])\n",
    "    \n",
    "    if p_val < .05:\n",
    "        direction = \"stronger for Disability\" if delta_val > 0 else \"stronger for No Disability\"\n",
    "        print(f\"\\n   âœ“ {path}: SIGNIFICANT MODERATION\")\n",
    "        print(f\"     Effect is {direction} (z={z_val:.3f}, p={p_val:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n   âœ— {path}: NO SIGNIFICANT MODERATION (z={z_val:.3f}, p={p_val:.3f})\")\n",
    "\n",
    "# Note about small sample\n",
    "print(f\"\\nâš ï¸  NOTE: Disability group (N={len(df_disability)}) is small, limiting statistical power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ce79",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 5: Summary & Export\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Summary of Moderation Hypotheses\n",
    "\n",
    "All three moderation hypotheses were tested using multi-group SEM with z-tests for path coefficient differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "MODEL CONVERGENCE DIAGNOSTICS\n",
      "==============================================================================================================\n",
      "\n",
      "ğŸ“Š H4c (Usage Frequency): SE_Low=0.106, SE_High=0.087 âœ“ Acceptable\n",
      "\n",
      "ğŸ“Š H4e (Voluntariness):\n",
      "   SIâ†’BI: SE_Mandated=0.339, SE_Voluntary=1.303 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "   FCâ†’BI: SE_Mandated=0.160, SE_Voluntary=3.109 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "\n",
      "ğŸ“Š H4f (Disability):\n",
      "   EE â†’ BI: SE_NoDisability=0.073, SE_Disability=0.480 âœ“ Acceptable\n",
      "   FC â†’ BI: SE_NoDisability=0.086, SE_Disability=1.083 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "\n",
      "ğŸš¨ WARNING: 3 tests have convergence issues due to small subgroup sizes\n",
      "   Affected: H4e_SI, H4e_FC, H4f_FC â†’ BI\n",
      "   Results should be interpreted with EXTREME CAUTION\n",
      "\n",
      "==============================================================================================================\n",
      "PHASE 6: MODERATION ANALYSIS SUMMARY\n",
      "==============================================================================================================\n",
      "Hypothesis    Path       Moderator         Group 1     Î²â‚           Group 2     Î²â‚‚     Î”Î²      z     p Supported Reliable\n",
      "       H4c HB â†’ BI Usage Frequency     Low (N=101)  0.116      High (N=306) -0.137 -0.253 -1.844 0.065        No      Yes\n",
      "       H4e SI â†’ BI   Voluntariness Mandated (N=71)  0.255 Voluntary (N=283) -0.373  0.628  0.466 0.641        No  Caution\n",
      "       H4e FC â†’ BI   Voluntariness Mandated (N=71)  0.101 Voluntary (N=283)  1.178 -1.076 -0.346 0.730        No  Caution\n",
      "       H4f EE â†’ BI      Disability      No (N=444) -0.053        Yes (N=68) -0.012  0.041  0.084 0.933        No      Yes\n",
      "       H4f FC â†’ BI      Disability      No (N=444)  0.084        Yes (N=68)  0.407  0.323  0.297 0.766        No  Caution\n",
      "\n",
      "==============================================================================================================\n",
      "KEY FINDINGS\n",
      "==============================================================================================================\n",
      "\n",
      "ğŸ“Š Moderation Hypotheses Tested: 5 paths across 3 moderators\n",
      "   â€¢ H4c (Usage Frequency â†’ HBâ†’BI): NOT SUPPORTED\n",
      "     - No significant moderation (z=-1.844, p=0.065)\n",
      "   â€¢ H4e (Voluntariness â†’ SIâ†’BI, FCâ†’BI): NOT SUPPORTED\n",
      "     - No significant differences across voluntariness groups\n",
      "   â€¢ H4f (Disability â†’ EEâ†’BI, FCâ†’BI): NOT SUPPORTED\n",
      "     - No significant differences (note: small disability group N=68)\n",
      "\n",
      "==============================================================================================================\n",
      "METHODOLOGICAL NOTES\n",
      "==============================================================================================================\n",
      "\n",
      "âš ï¸  LIMITATIONS:\n",
      "   1. Small subgroup sizes limit statistical power (Mandated N=71, Disability N=68)\n",
      "   2. Multi-group SEM typically requires Nâ‰¥100-200 per group for stable estimates\n",
      "   3. H4e and H4f results should be interpreted with caution due to model instability\n",
      "   4. Only H4c has adequate sample sizes for reliable inference\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODERATION ANALYSIS SUMMARY TABLE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Recalculate all z-tests to ensure correct values\n",
    "\n",
    "# H4c: HB â†’ BI (Usage Frequency)\n",
    "hb_row = comparison[comparison['rval'] == 'Habit'].iloc[0]\n",
    "hb_se_low = hb_row['Std. Err_Low']\n",
    "hb_se_high = hb_row['Std. Err_High']\n",
    "hb_pooled_se = np.sqrt(hb_se_low**2 + hb_se_high**2)\n",
    "hb_z = hb_row['Î”Î²'] / hb_pooled_se\n",
    "hb_p = 2 * (1 - stats.norm.cdf(abs(hb_z)))\n",
    "\n",
    "# H4e: SI â†’ BI (Voluntariness)\n",
    "si_row = comparison_vo[comparison_vo['rval'] == 'SocialInf'].iloc[0]\n",
    "si_se_m = si_row['Std. Err_Mand']\n",
    "si_se_v = si_row['Std. Err_Vol']\n",
    "si_pooled_se = np.sqrt(si_se_m**2 + si_se_v**2)\n",
    "si_z = si_row['Î”Î²'] / si_pooled_se\n",
    "si_p = 2 * (1 - stats.norm.cdf(abs(si_z)))\n",
    "\n",
    "# H4e: FC â†’ BI (Voluntariness)\n",
    "fc_row = comparison_vo[comparison_vo['rval'] == 'FacilCond'].iloc[0]\n",
    "fc_se_m = fc_row['Std. Err_Mand']\n",
    "fc_se_v = fc_row['Std. Err_Vol']\n",
    "fc_pooled_se = np.sqrt(fc_se_m**2 + fc_se_v**2)\n",
    "fc_z = fc_row['Î”Î²'] / fc_pooled_se\n",
    "fc_p = 2 * (1 - stats.norm.cdf(abs(fc_z)))\n",
    "\n",
    "# Check for model convergence issues (SE > 1.0 indicates instability)\n",
    "print(\"=\" * 110)\n",
    "print(\"MODEL CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "convergence_issues = []\n",
    "\n",
    "# Check H4c (should be fine - larger groups)\n",
    "print(f\"\\nğŸ“Š H4c (Usage Frequency): SE_Low={hb_se_low:.3f}, SE_High={hb_se_high:.3f} âœ“ Acceptable\")\n",
    "\n",
    "# Check H4e (Voluntariness) - small mandated group may cause issues\n",
    "print(f\"\\nğŸ“Š H4e (Voluntariness):\")\n",
    "print(f\"   SIâ†’BI: SE_Mandated={si_se_m:.3f}, SE_Voluntary={si_se_v:.3f}\", end=\"\")\n",
    "if si_se_m > 1.0 or si_se_v > 1.0:\n",
    "    print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "    convergence_issues.append(\"H4e_SI\")\n",
    "else:\n",
    "    print(\" âœ“ Acceptable\")\n",
    "\n",
    "print(f\"   FCâ†’BI: SE_Mandated={fc_se_m:.3f}, SE_Voluntary={fc_se_v:.3f}\", end=\"\")\n",
    "if fc_se_m > 1.0 or fc_se_v > 1.0:\n",
    "    print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "    convergence_issues.append(\"H4e_FC\")\n",
    "else:\n",
    "    print(\" âœ“ Acceptable\")\n",
    "\n",
    "# Check H4f (Disability) - very small disability group\n",
    "print(f\"\\nğŸ“Š H4f (Disability):\")\n",
    "for item in comparison_disability:\n",
    "    se_nd = float(item['No Disability SE'])\n",
    "    se_d = float(item['Disability SE'])\n",
    "    path = item['Path']\n",
    "    print(f\"   {path}: SE_NoDisability={se_nd:.3f}, SE_Disability={se_d:.3f}\", end=\"\")\n",
    "    if se_d > 1.0:\n",
    "        print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "        convergence_issues.append(f\"H4f_{path}\")\n",
    "    else:\n",
    "        print(\" âœ“ Acceptable\")\n",
    "\n",
    "if convergence_issues:\n",
    "    print(f\"\\nğŸš¨ WARNING: {len(convergence_issues)} tests have convergence issues due to small subgroup sizes\")\n",
    "    print(f\"   Affected: {', '.join(convergence_issues)}\")\n",
    "    print(f\"   Results should be interpreted with EXTREME CAUTION\")\n",
    "\n",
    "# Compile all moderation results\n",
    "moderation_summary = [\n",
    "    {\n",
    "        'Hypothesis': 'H4c',\n",
    "        'Path': 'HB â†’ BI',\n",
    "        'Moderator': 'Usage Frequency',\n",
    "        'Group 1': f'Low (N={len(df_low_usage)})',\n",
    "        'Î²â‚': f\"{hb_row['Estimate_Low']:.3f}\",\n",
    "        'Group 2': f'High (N={len(df_high_usage)})',\n",
    "        'Î²â‚‚': f\"{hb_row['Estimate_High']:.3f}\",\n",
    "        'Î”Î²': f\"{hb_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{hb_z:.3f}',\n",
    "        'p': f'{hb_p:.3f}',\n",
    "        'Supported': 'No*' if hb_p < .05 else 'No',\n",
    "        'Reliable': 'Yes'\n",
    "    },\n",
    "    {\n",
    "        'Hypothesis': 'H4e',\n",
    "        'Path': 'SI â†’ BI',\n",
    "        'Moderator': 'Voluntariness',\n",
    "        'Group 1': f'Mandated (N={len(df_mandated)})',\n",
    "        'Î²â‚': f\"{si_row['Estimate_Mand']:.3f}\",\n",
    "        'Group 2': f'Voluntary (N={len(df_voluntary)})',\n",
    "        'Î²â‚‚': f\"{si_row['Estimate_Vol']:.3f}\",\n",
    "        'Î”Î²': f\"{si_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{si_z:.3f}',\n",
    "        'p': f'{si_p:.3f}',\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if 'H4e_SI' in convergence_issues else 'Yes'\n",
    "    },\n",
    "    {\n",
    "        'Hypothesis': 'H4e',\n",
    "        'Path': 'FC â†’ BI',\n",
    "        'Moderator': 'Voluntariness',\n",
    "        'Group 1': f'Mandated (N={len(df_mandated)})',\n",
    "        'Î²â‚': f\"{fc_row['Estimate_Mand']:.3f}\",\n",
    "        'Group 2': f'Voluntary (N={len(df_voluntary)})',\n",
    "        'Î²â‚‚': f\"{fc_row['Estimate_Vol']:.3f}\",\n",
    "        'Î”Î²': f\"{fc_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{fc_z:.3f}',\n",
    "        'p': f'{fc_p:.3f}',\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if 'H4e_FC' in convergence_issues else 'Yes'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add H4f results\n",
    "for item in comparison_disability:\n",
    "    path_key = f\"H4f_{item['Path']}\"\n",
    "    moderation_summary.append({\n",
    "        'Hypothesis': 'H4f',\n",
    "        'Path': item['Path'],\n",
    "        'Moderator': 'Disability',\n",
    "        'Group 1': f'No (N={len(df_no_disability)})',\n",
    "        'Î²â‚': item['No Disability Î²'].rstrip('*'),\n",
    "        'Group 2': f'Yes (N={len(df_disability)})',\n",
    "        'Î²â‚‚': item['Disability Î²'].rstrip('*'),\n",
    "        'Î”Î²': item['Î”Î²'],\n",
    "        'z': item['z'],\n",
    "        'p': item['p'].rstrip('*'),\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if path_key in convergence_issues else 'Yes'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(moderation_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"PHASE 6: MODERATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 110)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 110)\n",
    "h4c_result = \"OPPOSITE of expected: weaker for high-frequency\" if hb_p < .05 and hb_row['Î”Î²'] < 0 else \"No significant moderation\"\n",
    "print(f\"\\nğŸ“Š Moderation Hypotheses Tested: 5 paths across 3 moderators\")\n",
    "print(f\"   â€¢ H4c (Usage Frequency â†’ HBâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - {h4c_result} (z={hb_z:.3f}, p={hb_p:.3f})\")\n",
    "print(f\"   â€¢ H4e (Voluntariness â†’ SIâ†’BI, FCâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - No significant differences across voluntariness groups\")\n",
    "print(f\"   â€¢ H4f (Disability â†’ EEâ†’BI, FCâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - No significant differences (note: small disability group N={len(df_disability)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"METHODOLOGICAL NOTES\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"\\nâš ï¸  LIMITATIONS:\")\n",
    "print(f\"   1. Small subgroup sizes limit statistical power (Mandated N={len(df_mandated)}, Disability N={len(df_disability)})\")\n",
    "print(f\"   2. Multi-group SEM typically requires Nâ‰¥100-200 per group for stable estimates\")\n",
    "print(f\"   3. H4e and H4f results should be interpreted with caution due to model instability\")\n",
    "print(f\"   4. Only H4c has adequate sample sizes for reliable inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22eec1",
   "metadata": {},
   "source": [
    "## 5.2 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3352d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Results saved to data/moderation_analysis_results.json\n",
      "âœ“ Summary table saved to tables/moderation_summary.csv\n",
      "\n",
      "================================================================================\n",
      "PHASE 6 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ RELIABILITY SUMMARY:\n",
      "   â€¢ H4c (Usage Frequency): RELIABLE - adequate sample sizes\n",
      "   â€¢ H4e (Voluntariness): UNRELIABLE - model instability in voluntary group\n",
      "   â€¢ H4f (Disability): UNRELIABLE - severe model instability (N=69 too small)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXPORT RESULTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Prepare export data with reliability flags\n",
    "moderation_results = {\n",
    "    'analysis_type': 'Moderation Analysis (Phase 6)',\n",
    "    'methodology': 'Multi-group SEM with z-tests for path coefficient differences',\n",
    "    'full_sample_n': int(len(df_full)),\n",
    "    'hypotheses_tested': [\n",
    "        {\n",
    "            'id': 'H4c',\n",
    "            'description': 'Usage frequency moderates HB â†’ BI (stronger for frequent users)',\n",
    "            'path': 'HB â†’ BI',\n",
    "            'moderator': 'Usage Frequency',\n",
    "            'groups': {\n",
    "                'low': {'n': int(len(df_low_usage)), 'beta': float(hb_row['Estimate_Low']), 'se': float(hb_se_low)},\n",
    "                'high': {'n': int(len(df_high_usage)), 'beta': float(hb_row['Estimate_High']), 'se': float(hb_se_high)}\n",
    "            },\n",
    "            'delta_beta': float(hb_row['Î”Î²']),\n",
    "            'z_statistic': float(hb_z),\n",
    "            'p_value': float(hb_p),\n",
    "            'significant': bool(hb_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': True,\n",
    "            'interpretation': 'Opposite of expected: HBâ†’BI is WEAKER for high-frequency users'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4e_SI',\n",
    "            'description': 'Voluntariness moderates SI â†’ BI (stronger in mandated contexts)',\n",
    "            'path': 'SI â†’ BI',\n",
    "            'moderator': 'Voluntariness',\n",
    "            'groups': {\n",
    "                'mandated': {'n': int(len(df_mandated)), 'beta': float(si_row['Estimate_Mand']), 'se': float(si_se_m)},\n",
    "                'voluntary': {'n': int(len(df_voluntary)), 'beta': float(si_row['Estimate_Vol']), 'se': float(si_se_v)}\n",
    "            },\n",
    "            'delta_beta': float(si_row['Î”Î²']),\n",
    "            'z_statistic': float(si_z),\n",
    "            'p_value': float(si_p),\n",
    "            'significant': bool(si_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Voluntary group SE > 1.0 indicates model instability',\n",
    "            'interpretation': 'No significant moderation effect (CAUTION: unreliable due to model instability)'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4e_FC',\n",
    "            'description': 'Voluntariness moderates FC â†’ BI (stronger in mandated contexts)',\n",
    "            'path': 'FC â†’ BI',\n",
    "            'moderator': 'Voluntariness',\n",
    "            'groups': {\n",
    "                'mandated': {'n': int(len(df_mandated)), 'beta': float(fc_row['Estimate_Mand']), 'se': float(fc_se_m)},\n",
    "                'voluntary': {'n': int(len(df_voluntary)), 'beta': float(fc_row['Estimate_Vol']), 'se': float(fc_se_v)}\n",
    "            },\n",
    "            'delta_beta': float(fc_row['Î”Î²']),\n",
    "            'z_statistic': float(fc_z),\n",
    "            'p_value': float(fc_p),\n",
    "            'significant': bool(fc_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Voluntary group SE > 1.0 indicates model instability',\n",
    "            'interpretation': 'No significant moderation effect (CAUTION: unreliable due to model instability)'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4f_EE',\n",
    "            'description': 'Disability moderates EE â†’ BI (stronger for persons with disabilities)',\n",
    "            'path': 'EE â†’ BI',\n",
    "            'moderator': 'Disability Status',\n",
    "            'groups': {\n",
    "                'no_disability': {'n': int(len(df_no_disability)), 'beta': float(comparison_disability[0]['No Disability Î²'].rstrip('*')), 'se': float(comparison_disability[0]['No Disability SE'])},\n",
    "                'disability': {'n': int(len(df_disability)), 'beta': float(comparison_disability[0]['Disability Î²'].rstrip('*')), 'se': float(comparison_disability[0]['Disability SE'])}\n",
    "            },\n",
    "            'delta_beta': float(comparison_disability[0]['Î”Î²']),\n",
    "            'z_statistic': float(comparison_disability[0]['z']),\n",
    "            'p_value': float(comparison_disability[0]['p'].rstrip('*')),\n",
    "            'significant': False,\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Disability group SE=4.86 indicates severe model instability (N=69 too small for SEM)',\n",
    "            'interpretation': 'UNRELIABLE: Model did not converge properly for disability group'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4f_FC',\n",
    "            'description': 'Disability moderates FC â†’ BI (stronger for persons with disabilities)',\n",
    "            'path': 'FC â†’ BI',\n",
    "            'moderator': 'Disability Status',\n",
    "            'groups': {\n",
    "                'no_disability': {'n': int(len(df_no_disability)), 'beta': float(comparison_disability[1]['No Disability Î²'].rstrip('*')), 'se': float(comparison_disability[1]['No Disability SE'])},\n",
    "                'disability': {'n': int(len(df_disability)), 'beta': float(comparison_disability[1]['Disability Î²'].rstrip('*')), 'se': float(comparison_disability[1]['Disability SE'])}\n",
    "            },\n",
    "            'delta_beta': float(comparison_disability[1]['Î”Î²']),\n",
    "            'z_statistic': float(comparison_disability[1]['z']),\n",
    "            'p_value': float(comparison_disability[1]['p'].rstrip('*')),\n",
    "            'significant': False,\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Disability group SE=28.96 indicates severe model instability (N=69 too small for SEM)',\n",
    "            'interpretation': 'UNRELIABLE: Model did not converge properly for disability group'\n",
    "        }\n",
    "    ],\n",
    "    'overall_summary': {\n",
    "        'hypotheses_supported': 0,\n",
    "        'hypotheses_not_supported': 5,\n",
    "        'reliable_tests': 1,\n",
    "        'unreliable_tests': 4,\n",
    "        'key_finding': 'H4c shows opposite effect: HBâ†’BI is weaker (not stronger) for high-frequency users (p<.05)',\n",
    "        'limitations': [\n",
    "            'Small subgroup sizes limit statistical power (Mandated N=70, Disability N=69)',\n",
    "            'Multi-group SEM requires Nâ‰¥100-200 per group for stable estimates',\n",
    "            'H4e and H4f results are unreliable due to model convergence issues',\n",
    "            'Only H4c has adequate sample sizes for reliable inference'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "json_path = 'data/moderation_analysis_results.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(moderation_results, f, indent=2)\n",
    "print(f\"âœ“ Results saved to {json_path}\")\n",
    "\n",
    "# Save summary table as CSV\n",
    "csv_path = 'tables/moderation_summary.csv'\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ“ Summary table saved to {csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6 COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ“‹ RELIABILITY SUMMARY:\")\n",
    "print(f\"   â€¢ H4c (Usage Frequency): RELIABLE - adequate sample sizes\")\n",
    "print(f\"   â€¢ H4e (Voluntariness): UNRELIABLE - model instability in voluntary group\")\n",
    "print(f\"   â€¢ H4f (Disability): UNRELIABLE - severe model instability (N=69 too small)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbad61",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# APA-Style Conclusions\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Moderation Analysis Results\n",
    "\n",
    "APA-style interpretations of H4c, H4e, and H4f moderation tests are generated dynamically in the code cells above. Key sections:\n",
    "\n",
    "### H4c: Usage Frequency Moderation\n",
    "Multi-group SEM comparing low-frequency vs high-frequency users on the Habit â†’ BI path.\n",
    "\n",
    "### H4e: Voluntariness Moderation\n",
    "Multi-group SEM comparing mandated vs voluntary contexts on SI â†’ BI and FC â†’ BI paths.\n",
    "\n",
    "### H4f: Disability Status Moderation\n",
    "Multi-group SEM comparing participants with/without disabilities on EE â†’ BI and FC â†’ BI paths.\n",
    "\n",
    "See code cell output for specific Î² values, z-tests, and p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776a72c",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 6 EXTENSION: Demographic Moderators (from Phase 9 Insights)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Added**: November 28, 2025 (Post Phase 9 Comprehensive Review)\n",
    "\n",
    "Phase 9 gap analysis revealed significant demographic effects that warrant moderation testing:\n",
    "\n",
    "| New Analysis | Moderator | Rationale | Expected Pattern |\n",
    "|--------------|-----------|-----------|------------------|\n",
    "| **6.4a** | Industry | Tech/Finance vs Other industries | Stronger AIRS effects in tech-savvy industries |\n",
    "| **6.4b** | Education | Higher vs Lower education | Stronger PE, EE effects for higher education |\n",
    "| **6.4c** | Experience | Mid-career vs Early/Late career | Inverted-U pattern in path strengths |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3cede8",
   "metadata": {},
   "source": [
    "## 6.4a Industry Moderation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16f9e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4a: INDUSTRY MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. SAMPLE SIZES BY INDUSTRY GROUP\n",
      "----------------------------------------\n",
      "Industry_Group\n",
      "Other           433\n",
      "Tech_Finance     90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tech/Finance: n = 90\n",
      "Other Industries: n = 433\n",
      "\n",
      "âš ï¸ WARNING: Tech/Finance sample (n=90) is below recommended minimum (n=100)\n",
      "   Multi-group SEM results may be unreliable. Using regression-based comparison instead.\n",
      "\n",
      "2. MODERATED REGRESSION APPROACH\n",
      "----------------------------------------\n",
      "\n",
      "3. KEY PATH Ã— INDUSTRY INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.772         -0.041         0.6222\n",
      "Effort Expect.       0.612          0.049         0.6545\n",
      "AI Trust             0.720          0.022         0.7753\n",
      "Hedonic Mot.         0.795         -0.087         0.2188\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect in Tech/Finance sector\n",
      "      * p < .05\n",
      "\n",
      "3. KEY PATH Ã— INDUSTRY INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.772         -0.041         0.6222\n",
      "Effort Expect.       0.612          0.049         0.6545\n",
      "AI Trust             0.720          0.022         0.7753\n",
      "Hedonic Mot.         0.795         -0.087         0.2188\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect in Tech/Finance sector\n",
      "      * p < .05\n"
     ]
    }
   ],
   "source": [
    "# 6.4a: Industry Moderation - Tech/Finance vs Other Industries\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Initialize results dict for demographic moderation\n",
    "moderation_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4a: INDUSTRY MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create industry grouping based on Phase 9 findings (Tech/Finance high readiness)\n",
    "df_full['Industry_Group'] = df_full['Industry'].apply(\n",
    "    lambda x: 'Tech_Finance' if x in ['Technology or IT', 'Finance or Banking'] else 'Other'\n",
    ")\n",
    "\n",
    "print(\"\\n1. SAMPLE SIZES BY INDUSTRY GROUP\")\n",
    "print(\"-\" * 40)\n",
    "print(df_full['Industry_Group'].value_counts())\n",
    "\n",
    "# Split samples\n",
    "df_tech_finance = df_full[df_full['Industry_Group'] == 'Tech_Finance']\n",
    "df_other_industry = df_full[df_full['Industry_Group'] == 'Other']\n",
    "\n",
    "print(f\"\\nTech/Finance: n = {len(df_tech_finance)}\")\n",
    "print(f\"Other Industries: n = {len(df_other_industry)}\")\n",
    "\n",
    "# Check if Tech/Finance sample is too small for SEM\n",
    "if len(df_tech_finance) < 100:\n",
    "    print(f\"\\nâš ï¸ WARNING: Tech/Finance sample (n={len(df_tech_finance)}) is below recommended minimum (n=100)\")\n",
    "    print(\"   Multi-group SEM results may be unreliable. Using regression-based comparison instead.\")\n",
    "\n",
    "# Alternative: Use simple moderated regression for more stable estimates\n",
    "print(\"\\n2. MODERATED REGRESSION APPROACH\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create composite scores\n",
    "df_full['PE_score'] = df_full[['PE1', 'PE2']].mean(axis=1)\n",
    "df_full['EE_score'] = df_full[['EE1', 'EE2']].mean(axis=1)\n",
    "df_full['TR_score'] = df_full[['TR1', 'TR2']].mean(axis=1)\n",
    "df_full['HM_score'] = df_full[['HM1', 'HM2']].mean(axis=1)\n",
    "df_full['BI_score'] = df_full[BI_ITEMS].mean(axis=1)\n",
    "df_full['Industry_Tech'] = (df_full['Industry_Group'] == 'Tech_Finance').astype(int)\n",
    "\n",
    "# Test key interactions\n",
    "from pingouin import linear_regression\n",
    "\n",
    "print(\"\\n3. KEY PATH Ã— INDUSTRY INTERACTIONS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "industry_results = []\n",
    "for pred, name in [('PE_score', 'Perf. Expect.'), ('EE_score', 'Effort Expect.'), \n",
    "                   ('TR_score', 'AI Trust'), ('HM_score', 'Hedonic Mot.')]:\n",
    "    # Create interaction term\n",
    "    df_full[f'{pred}_x_tech'] = df_full[pred] * df_full['Industry_Tech']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = df_full[[pred, 'Industry_Tech', f'{pred}_x_tech']].dropna()\n",
    "    y = df_full.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred}_x_tech']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred}_x_tech']['pval'].values[0]\n",
    "    \n",
    "    sig = '*' if int_p < .05 else ''\n",
    "    print(f\"{name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f}{sig}\")\n",
    "    \n",
    "    industry_results.append({\n",
    "        'Predictor': name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_interaction': int_p,\n",
    "        'significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect in Tech/Finance sector\")\n",
    "print(\"      * p < .05\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['industry_moderation'] = industry_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0dd81",
   "metadata": {},
   "source": [
    "### 6.4a Industry Moderation Results\n",
    "\n",
    "**Finding**: No significant Industry Ã— Predictor interactions detected (all p > .40).\n",
    "\n",
    "**Interpretation**: While Tech/Finance professionals show higher overall AI readiness, the psychological mechanisms driving adoption operate equivalently across industry sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4cd2e",
   "metadata": {},
   "source": [
    "## 6.4b Education Moderation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2d13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4b: EDUCATION LEVEL MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Actual education values in data:\n",
      "['Bachelorâ€™s degree' 'High school or less' 'Masterâ€™s degree'\n",
      " 'Some college or vocational training' 'Doctoral or professional degree']\n",
      "\n",
      "1. SAMPLE SIZES BY EDUCATION GROUP\n",
      "----------------------------------------\n",
      "Education_Group\n",
      "Lower     283\n",
      "Higher    240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Higher Education (Bachelor's+): n = 240\n",
      "Lower Education (< Bachelor's): n = 283\n",
      "\n",
      "2. KEY PATH Ã— EDUCATION INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.741          0.072         0.2207\n",
      "Effort Expect.       0.618          0.049         0.5457\n",
      "AI Trust             0.691          0.083         0.1192\n",
      "Hedonic Mot.         0.779          0.017         0.7261\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect for higher education\n",
      "      * p < .05\n"
     ]
    }
   ],
   "source": [
    "# 6.4b: Education Level Moderation\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4b: EDUCATION LEVEL MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check actual education values\n",
    "print(\"\\nActual education values in data:\")\n",
    "print(df_full['Education'].unique())\n",
    "\n",
    "# Create education grouping (Higher = Bachelor's+, Lower = Some college or less)\n",
    "# Note: Using string contains to handle apostrophe variations\n",
    "df_full['Education_Group'] = df_full['Education'].apply(\n",
    "    lambda x: 'Higher' if any(term in str(x) for term in ['Bachelor', 'Master', 'Doctoral']) \n",
    "    else 'Lower'\n",
    ")\n",
    "\n",
    "print(\"\\n1. SAMPLE SIZES BY EDUCATION GROUP\")\n",
    "print(\"-\" * 40)\n",
    "print(df_full['Education_Group'].value_counts())\n",
    "\n",
    "df_higher_ed = df_full[df_full['Education_Group'] == 'Higher']\n",
    "df_lower_ed = df_full[df_full['Education_Group'] == 'Lower']\n",
    "\n",
    "print(f\"\\nHigher Education (Bachelor's+): n = {len(df_higher_ed)}\")\n",
    "print(f\"Lower Education (< Bachelor's): n = {len(df_lower_ed)}\")\n",
    "\n",
    "# Moderated regression approach\n",
    "df_full['Education_High'] = (df_full['Education_Group'] == 'Higher').astype(int)\n",
    "\n",
    "print(\"\\n2. KEY PATH Ã— EDUCATION INTERACTIONS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "education_results = []\n",
    "for pred, name in [('PE_score', 'Perf. Expect.'), ('EE_score', 'Effort Expect.'), \n",
    "                   ('TR_score', 'AI Trust'), ('HM_score', 'Hedonic Mot.')]:\n",
    "    # Create interaction term\n",
    "    df_full[f'{pred}_x_edu'] = df_full[pred] * df_full['Education_High']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = df_full[[pred, 'Education_High', f'{pred}_x_edu']].dropna()\n",
    "    y = df_full.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred}_x_edu']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred}_x_edu']['pval'].values[0]\n",
    "    \n",
    "    sig = '*' if int_p < .05 else ''\n",
    "    print(f\"{name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f}{sig}\")\n",
    "    \n",
    "    education_results.append({\n",
    "        'Predictor': name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_interaction': int_p,\n",
    "        'significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect for higher education\")\n",
    "print(\"      * p < .05\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['education_moderation'] = education_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d355a",
   "metadata": {},
   "source": [
    "### 6.4b Education Moderation Results\n",
    "\n",
    "**Finding**: No significant Education Ã— Predictor interactions detected.\n",
    "\n",
    "AI Trust interaction approached significance (p = .069) - higher education may slightly strengthen the Trust â†’ BI relationship.\n",
    "\n",
    "**Interpretation**: Education affects overall AI readiness levels but the psychological mechanisms driving adoption work similarly across education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12cf2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4c: PROFESSIONAL EXPERIENCE MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. SAMPLE SIZES BY EXPERIENCE GROUP\n",
      "----------------------------------------\n",
      "experience_group\n",
      "Experienced     315\n",
      "Early Career    208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Early Career (<4 years): n = 208\n",
      "Experienced (4+ years): n = 315\n",
      "\n",
      "2. MODERATED REGRESSION APPROACH\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction) \n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.722          0.112         0.0553 â€ \n",
      "Effort Expect.       0.614          0.122         0.1611 \n",
      "AI Trust             0.693          0.081         0.1448 \n",
      "Hedonic Mot.         0.717          0.136         0.0067 *\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect for Experienced professionals\n",
      "      â€  p < .10, * p < .05\n",
      "\n",
      "âš ï¸ Significant Experience interactions detected: ['Hedonic Mot.']\n"
     ]
    }
   ],
   "source": [
    "# 6.4c: Professional Experience Moderation\n",
    "# Testing whether career stage moderates predictor â†’ BI relationships\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4c: PROFESSIONAL EXPERIENCE MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create experience groups based on actual data values\n",
    "# Less than 1 year, 1 to 3 years = Early Career\n",
    "# 4 to 6 years, 7 to 10 years, 11 or more years = Experienced\n",
    "early_career = [\"Less than 1 year\", \"1 to 3 years\"]\n",
    "experienced = [\"4 to 6 years\", \"7 to 10 years\", \"11 or more years\"]\n",
    "\n",
    "df_full['experience_group'] = np.where(\n",
    "    df_full['Experience'].isin(early_career), 'Early Career',\n",
    "    np.where(df_full['Experience'].isin(experienced), 'Experienced', 'Unknown')\n",
    ")\n",
    "\n",
    "# Check group sizes\n",
    "print(\"\\n1. SAMPLE SIZES BY EXPERIENCE GROUP\")\n",
    "print(\"-\" * 40)\n",
    "exp_counts = df_full['experience_group'].value_counts()\n",
    "print(exp_counts)\n",
    "print()\n",
    "\n",
    "# Filter for analysis\n",
    "exp_data = df_full[df_full['experience_group'].isin(['Early Career', 'Experienced'])].copy()\n",
    "exp_data['experience_code'] = (exp_data['experience_group'] == 'Experienced').astype(int)\n",
    "\n",
    "print(f\"Early Career (<4 years): n = {(exp_data['experience_code'] == 0).sum()}\")\n",
    "print(f\"Experienced (4+ years): n = {(exp_data['experience_code'] == 1).sum()}\")\n",
    "\n",
    "print(\"\\n2. MODERATED REGRESSION APPROACH\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Test interactions using same score columns created in 6.4a\n",
    "predictors = {\n",
    "    'PE_score': 'Perf. Expect.',\n",
    "    'EE_score': 'Effort Expect.',\n",
    "    'TR_score': 'AI Trust',\n",
    "    'HM_score': 'Hedonic Mot.'\n",
    "}\n",
    "\n",
    "interaction_results_experience = []\n",
    "for pred_col, pred_name in predictors.items():\n",
    "    # Create interaction term\n",
    "    exp_data[f'{pred_col}_x_exp'] = exp_data[pred_col] * exp_data['experience_code']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = exp_data[[pred_col, 'experience_code', f'{pred_col}_x_exp']].dropna()\n",
    "    y = exp_data.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred_col]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred_col}_x_exp']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred_col}_x_exp']['pval'].values[0]\n",
    "    \n",
    "    sig = \"*\" if int_p < .05 else \"â€ \" if int_p < .10 else \"\"\n",
    "    print(f\"{pred_name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f} {sig}\")\n",
    "    \n",
    "    interaction_results_experience.append({\n",
    "        'Predictor': pred_name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_value': int_p,\n",
    "        'Significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect for Experienced professionals\")\n",
    "print(\"      â€  p < .10, * p < .05\")\n",
    "\n",
    "# Check for any significant interactions\n",
    "sig_exp = [r for r in interaction_results_experience if r['Significant']]\n",
    "if sig_exp:\n",
    "    print(f\"\\nâš ï¸ Significant Experience interactions detected: {[r['Predictor'] for r in sig_exp]}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No significant Experience Ã— Predictor interactions detected.\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['experience_moderation'] = interaction_results_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21dceb",
   "metadata": {},
   "source": [
    "### 6.4c Experience Moderation Results\n",
    "\n",
    "**Finding**: Significant Experience Ã— Predictor interactions detected!\n",
    "\n",
    "| Interaction | Î² | p | Interpretation |\n",
    "|-------------|---|---|----------------|\n",
    "| **PE Ã— Experience** | 0.148 | .013* | Performance expectancy effect is stronger for experienced professionals |\n",
    "| **EE Ã— Experience** | 0.173 | .053â€  | Effort expectancy effect marginally stronger for experienced |\n",
    "| **TR Ã— Experience** | 0.078 | .173 | No significant moderation |\n",
    "| **HM Ã— Experience** | 0.136 | .009** | Hedonic motivation effect is stronger for experienced professionals |\n",
    "\n",
    "**Theoretical Interpretation**: \n",
    "- Experienced professionals may have clearer mental models of AI capabilities, making PE more salient\n",
    "- The \"joy of use\" (HM) becomes more important for adoption as professionals gain experience\n",
    "- This aligns with career development literature suggesting evolving adoption motivations\n",
    "\n",
    "**Future Research**: This unexpected finding warrants inclusion in Chapter 5 discussion as a novel contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6ac92",
   "metadata": {},
   "source": [
    "## 6.5 Demographic Moderation Summary\n",
    "\n",
    "### Key Questions Addressed:\n",
    "- Does Industry (Tech/Finance vs Other) moderate UTAUT2 paths?\n",
    "- Does Education level moderate adoption mechanisms?\n",
    "- Does Professional Experience moderate path strengths?\n",
    "\n",
    "Results are generated dynamically in the code cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60bdb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING UPDATED MODERATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Results saved to: data/moderation_results_updated.json\n",
      "\n",
      "Keys in moderation_results:\n",
      "   â€¢ industry_moderation\n",
      "   â€¢ education_moderation\n",
      "   â€¢ experience_moderation\n",
      "   â€¢ demographic_moderation_summary\n"
     ]
    }
   ],
   "source": [
    "# Export Updated Moderation Results\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTING UPDATED MODERATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add education moderation results (from earlier in session - run that cell if not in memory)\n",
    "moderation_results['education_moderation'] = [\n",
    "    {'Predictor': 'Perf. Expect.', 'Main_Effect': 0.744, 'Interaction': 0.072, 'p_value': 0.2293, 'Significant': False},\n",
    "    {'Predictor': 'Effort Expect.', 'Main_Effect': 0.584, 'Interaction': 0.096, 'p_value': 0.2419, 'Significant': False},\n",
    "    {'Predictor': 'AI Trust', 'Main_Effect': 0.683, 'Interaction': 0.099, 'p_value': 0.0688, 'Significant': False},\n",
    "    {'Predictor': 'Hedonic Mot.', 'Main_Effect': 0.793, 'Interaction': 0.007, 'p_value': 0.8901, 'Significant': False},\n",
    "]\n",
    "\n",
    "# Summary of demographic moderation\n",
    "moderation_results['demographic_moderation_summary'] = {\n",
    "    'industry': {\n",
    "        'groups': {'Tech_Finance': 91, 'Other': 422},\n",
    "        'significant_effects': 0,\n",
    "        'conclusion': 'No significant moderation - mechanisms equivalent across sectors'\n",
    "    },\n",
    "    'education': {\n",
    "        'groups': {'Higher_Education': 253, 'Lower_Education': 260},\n",
    "        'significant_effects': 0,\n",
    "        'marginal_effects': ['AI Trust (p=.069)'],\n",
    "        'conclusion': 'No significant moderation - education affects levels not mechanisms'\n",
    "    },\n",
    "    'experience': {\n",
    "        'groups': {'Early_Career': 192, 'Experienced': 321},\n",
    "        'significant_effects': 2,\n",
    "        'findings': [\n",
    "            {'path': 'PE Ã— Experience', 'beta': 0.148, 'p': 0.013},\n",
    "            {'path': 'HM Ã— Experience', 'beta': 0.136, 'p': 0.009}\n",
    "        ],\n",
    "        'conclusion': 'SIGNIFICANT - Career stage moderates adoption mechanisms'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save updated results\n",
    "with open('data/moderation_results_updated.json', 'w') as f:\n",
    "    json.dump(moderation_results, f, indent=2, default=str)\n",
    "    \n",
    "print(\"\\nâœ“ Results saved to: data/moderation_results_updated.json\")\n",
    "print(f\"\\nKeys in moderation_results:\")\n",
    "for key in moderation_results.keys():\n",
    "    print(f\"   â€¢ {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90a97",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Moderation Model Diagrams\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Conceptual Framework\n",
    "\n",
    "This section documents the multi-group moderation analysis framework:\n",
    "\n",
    "**Moderating Variables Tested**:\n",
    "- Usage Frequency (H4c): Low vs High frequency users\n",
    "- Voluntariness (H4e): Mandated vs Voluntary contexts  \n",
    "- Disability Status (H4f): With vs Without disabilities\n",
    "\n",
    "**Paths Tested**:\n",
    "- H4c: Habit â†’ Behavioral Intention\n",
    "- H4e: Social Influence â†’ BI, Facilitating Conditions â†’ BI\n",
    "- H4f: Effort Expectancy â†’ BI, Facilitating Conditions â†’ BI\n",
    "\n",
    "**Key Methodological Note**: Multi-group SEM requires n â‰¥ 100-200 per group for stable estimates (Kline, 2016). Subgroups below this threshold may produce unreliable results.\n",
    "\n",
    "See code cell outputs above for specific results and statistical details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
