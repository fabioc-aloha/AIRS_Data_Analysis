{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978f3c11",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NOTEBOOK 06: MODERATION ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Purpose**: Test contextual moderation of AIRS structural paths\n",
    "\n",
    "**Design Decisions**:\n",
    "- **Sample**: Full sample (N=523, all roles including FT students)\n",
    "- **Approach**: Multi-group SEM + regression-based interaction tests\n",
    "- **Moderators**: Usage Frequency, Voluntariness, Disability, Industry, Education, Experience\n",
    "- **Tested Paths**: Key UTAUT2 paths (HBâ†’BI, SIâ†’BI, FCâ†’BI, EEâ†’BI, PEâ†’BI, HMâ†’BI, TRâ†’BI)\n",
    "\n",
    "**Empirical Results** (December 2025 - Full Dataset):\n",
    "\n",
    "### H4 Hypotheses (Multi-Group SEM)\n",
    "| Hypothesis | Moderator | Path | Î”Î² | z | p | Status |\n",
    "|------------|-----------|------|-----|---|---|--------|\n",
    "| **H4c** | **Usage Frequency** | **HBâ†’BI** | **-0.253** | **-1.844** | **.065** | **âš ï¸ Marginal** |\n",
    "| H4e | Voluntariness | SIâ†’BI | 0.628 | 0.466 | .641 | âŒ Not Supported |\n",
    "| H4e | Voluntariness | FCâ†’BI | -1.076 | -0.346 | .730 | âŒ Not Supported |\n",
    "\n",
    "### Usage Group Path Comparison\n",
    "| Predictor | Î² (Low) | Î² (High) | Interpretation |\n",
    "|-----------|---------|----------|----------------|\n",
    "| **PerfExp** | **0.371*** | -0.270 | PE matters for new users only |\n",
    "| **PriceValue** | 0.224 | **0.878*** | PV stronger for heavy users |\n",
    "\n",
    "### Exploratory Experience Moderation (Regression Interactions)\n",
    "| Moderator | Path | Interaction Î² | p | Status |\n",
    "|-----------|------|---------------|---|--------|\n",
    "| Experience | PEÃ—Exp | 0.112 | .055 | âš ï¸ Marginal |\n",
    "| **Experience** | **HMÃ—Exp** | **0.136** | **.007** | **âœ… Significant** |\n",
    "| Experience | EEÃ—Exp | 0.122 | .161 | âŒ Not significant |\n",
    "| Experience | TRÃ—Exp | 0.081 | .145 | âŒ Not significant |\n",
    "\n",
    "**Key Findings** (December 2025):\n",
    "1. **Experience Moderation**: Career stage moderates HMâ†’BI (p=.007)\n",
    "   - Experienced professionals (4+ years) weight hedonic motivation more heavily\n",
    "2. **Usage-Dependent Mechanisms**: Performance expectancy matters for new users; Price Value for heavy users\n",
    "3. **Habit Marginally Moderated**: For frequent users, HBâ†’BI weakens (p=.065)\n",
    "\n",
    "**Outputs**:\n",
    "- `moderation_summary.csv` - Multi-group SEM results\n",
    "- Experience moderation findings documented\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph H4[\"H4 Hypotheses\"]\n",
    "        M1[\"H4c: Usage Freq<br/>âš ï¸ Opposite effect\"]\n",
    "        M2[\"H4e: Voluntariness<br/>âŒ Not significant\"]\n",
    "        M3[\"H4f: Disability<br/>âŒ Unreliable (small n)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Exploratory[\"Exploratory Moderation\"]\n",
    "        E1[\"Industry<br/>âŒ No effect\"]\n",
    "        E2[\"Education<br/>âŒ No effect\"]\n",
    "        E3[\"Experience<br/>PEÃ—Exp p=.055 âš ï¸<br/>HMÃ—Exp p=.007 âœ…\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Novel[\"Novel Discovery\"]\n",
    "        N1[\"Experienced professionals<br/>weight HM more<br/>in adoption decisions\"]\n",
    "    end\n",
    "    \n",
    "    E3 ==> N1\n",
    "    \n",
    "    style E3 fill:#2e7d32,color:#fff,stroke-width:3px\n",
    "    style N1 fill:#1565c0,color:#fff\n",
    "    style M1 fill:#f9a825,color:#000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e8f55",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4c1846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 6: MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Environment configured (seed=67)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# CRITICAL FIX: Prevent OpenMP runtime conflicts\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from semopy import Model\n",
    "from semopy.stats import calc_stats\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "RANDOM_SEED = 67\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 6: MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Environment configured (seed={RANDOM_SEED})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dbf53",
   "metadata": {},
   "source": [
    "## 1.2 Load Data and Phase 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec679ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Full sample: N = 523\n",
      "ğŸ“Š EFA sample: N = 261\n",
      "ğŸ“Š CFA sample: N = 262\n",
      "\n",
      "================================================================================\n",
      "MODERATOR VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Usage frequency columns: ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
      "ğŸ“‹ Voluntariness items: ['VO1', 'VO2']\n",
      "\n",
      "â™¿ Disability distribution:\n",
      "Disability\n",
      "No                      444\n",
      "Yes                      68\n",
      "Prefer not to answer     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load full sample data\n",
    "df_efa = pd.read_csv('data/AIRS_experiment.csv')\n",
    "df_cfa = pd.read_csv('data/AIRS_holdout.csv')\n",
    "df_full = pd.concat([df_efa, df_cfa], ignore_index=True)\n",
    "\n",
    "# Load Phase 4 results\n",
    "with open('data/structural_model_results.json', 'r') as f:\n",
    "    phase4_results = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Š Full sample: N = {len(df_full)}\")\n",
    "print(f\"ğŸ“Š EFA sample: N = {len(df_efa)}\")\n",
    "print(f\"ğŸ“Š CFA sample: N = {len(df_cfa)}\")\n",
    "\n",
    "# Check available moderator variables\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODERATOR VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Usage frequency columns\n",
    "usage_cols = [c for c in df_full.columns if 'Usage' in c]\n",
    "print(f\"\\nğŸ“ˆ Usage frequency columns: {usage_cols}\")\n",
    "\n",
    "# Voluntariness items (VO1, VO2 - dropped from model but can use as moderators)\n",
    "vo_cols = [c for c in df_full.columns if c.startswith('VO')]\n",
    "print(f\"ğŸ“‹ Voluntariness items: {vo_cols}\")\n",
    "\n",
    "# Disability\n",
    "if 'Disability' in df_full.columns:\n",
    "    print(f\"\\nâ™¿ Disability distribution:\")\n",
    "    print(df_full['Disability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea902c86",
   "metadata": {},
   "source": [
    "## 1.3 Create Moderator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d77e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING MODERATOR VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ USAGE FREQUENCY (H4c):\n",
      "   Max usage distribution:\n",
      "Usage_Group\n",
      "Low       101\n",
      "Medium    116\n",
      "High      306\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“‹ VOLUNTARINESS (H4e):\n",
      "   VO1 mean: 3.46 (SD=1.27)\n",
      "   VO2 mean: 3.84 (SD=1.10)\n",
      "   VO1-VO2 correlation: r = 0.282\n",
      "   Voluntariness groups:\n",
      "Voluntariness_Group\n",
      "Mandated      71\n",
      "Mixed        169\n",
      "Voluntary    283\n",
      "Name: count, dtype: int64\n",
      "\n",
      "â™¿ DISABILITY STATUS (H4f):\n",
      "   Yes: N = 68\n",
      "   No: N = 444\n",
      "   Prefer not to answer (excluded): N = 11\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CREATE MODERATOR VARIABLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING MODERATOR VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. USAGE FREQUENCY: Create aggregate usage score (max across all tools)\n",
    "# Scale: 1=Never, 2=Rarely, 3=Sometimes, 4=Often, 5=Very often (daily)\n",
    "df_full['Usage_Max'] = df_full[['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']].max(axis=1)\n",
    "df_full['Usage_Mean'] = df_full[['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']].mean(axis=1)\n",
    "\n",
    "# Create categorical: Low (1-2), Medium (3), High (4-5)\n",
    "df_full['Usage_Group'] = pd.cut(df_full['Usage_Max'], \n",
    "                                 bins=[0, 2, 3, 5], \n",
    "                                 labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"\\nğŸ“ˆ USAGE FREQUENCY (H4c):\")\n",
    "print(f\"   Max usage distribution:\")\n",
    "print(df_full['Usage_Group'].value_counts().sort_index())\n",
    "\n",
    "# 2. VOLUNTARINESS: Use VO1 and VO2 items (dropped from model, but valid as moderators)\n",
    "# VO1: \"I choose to use AI tools because I find them helpful, not because I am required to\"\n",
    "# VO2: \"I could choose not to use AI tools if I preferred\"\n",
    "# Higher scores = more voluntary use; Lower scores = more mandated\n",
    "\n",
    "# Mean of VO1 and VO2 (note: they measure different facets)\n",
    "df_full['Voluntariness_Mean'] = df_full[['VO1', 'VO2']].mean(axis=1)\n",
    "\n",
    "# Create categorical: Low (1-2.5), Medium (2.5-3.5), High (3.5-5)\n",
    "df_full['Voluntariness_Group'] = pd.cut(df_full['Voluntariness_Mean'], \n",
    "                                         bins=[0, 2.5, 3.5, 5], \n",
    "                                         labels=['Mandated', 'Mixed', 'Voluntary'])\n",
    "\n",
    "print(\"\\nğŸ“‹ VOLUNTARINESS (H4e):\")\n",
    "print(f\"   VO1 mean: {df_full['VO1'].mean():.2f} (SD={df_full['VO1'].std():.2f})\")\n",
    "print(f\"   VO2 mean: {df_full['VO2'].mean():.2f} (SD={df_full['VO2'].std():.2f})\")\n",
    "print(f\"   VO1-VO2 correlation: r = {df_full['VO1'].corr(df_full['VO2']):.3f}\")\n",
    "print(f\"   Voluntariness groups:\")\n",
    "print(df_full['Voluntariness_Group'].value_counts().sort_index())\n",
    "\n",
    "# 3. DISABILITY STATUS: Binary (Yes vs No, exclude \"Prefer not to answer\")\n",
    "df_full['Disability_Binary'] = df_full['Disability'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(\"\\nâ™¿ DISABILITY STATUS (H4f):\")\n",
    "print(f\"   Yes: N = {(df_full['Disability'] == 'Yes').sum()}\")\n",
    "print(f\"   No: N = {(df_full['Disability'] == 'No').sum()}\")\n",
    "print(f\"   Prefer not to answer (excluded): N = {(df_full['Disability'] == 'Prefer not to answer').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c1d74",
   "metadata": {},
   "source": [
    "## 1.4 Define Model D Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64608292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL D STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š 8 factors, 16 predictor items + 4 BI items\n",
      "   PerfExp: ['PE1', 'PE2']\n",
      "   EffortExp: ['EE1', 'EE2']\n",
      "   SocialInf: ['SI1', 'SI2']\n",
      "   FacilCond: ['FC1', 'FC2']\n",
      "   HedonicMot: ['HM1', 'HM2']\n",
      "   PriceValue: ['PV1', 'PV2']\n",
      "   Habit: ['HB1', 'HB2']\n",
      "   AITrust: ['TR1', 'TR2']\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODEL D STRUCTURE (from Phase 4)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Validated 8-factor structure\n",
    "MODEL_D_STRUCTURE = {\n",
    "    'PerfExp': ['PE1', 'PE2'],\n",
    "    'EffortExp': ['EE1', 'EE2'],\n",
    "    'SocialInf': ['SI1', 'SI2'],\n",
    "    'FacilCond': ['FC1', 'FC2'],\n",
    "    'HedonicMot': ['HM1', 'HM2'],\n",
    "    'PriceValue': ['PV1', 'PV2'],\n",
    "    'Habit': ['HB1', 'HB2'],\n",
    "    'AITrust': ['TR1', 'TR2']\n",
    "}\n",
    "\n",
    "BI_ITEMS = ['BI1', 'BI2', 'BI3', 'BI4']\n",
    "\n",
    "# Get all model items\n",
    "MODEL_D_ITEMS = []\n",
    "for items in MODEL_D_STRUCTURE.values():\n",
    "    MODEL_D_ITEMS.extend(items)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL D STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Š 8 factors, {len(MODEL_D_ITEMS)} predictor items + {len(BI_ITEMS)} BI items\")\n",
    "for factor, items in MODEL_D_STRUCTURE.items():\n",
    "    print(f\"   {factor}: {items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80de30",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 2: H4c - Usage Frequency Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4c**: The effect of Habit (HB) on Behavioral Intention (BI) is stronger for high-frequency AI users.\n",
    "\n",
    "**Rationale**: Users who interact with AI tools more frequently have had more opportunities to develop habitual usage patterns. For them, habit should be a stronger predictor of continued intention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e6a76",
   "metadata": {},
   "source": [
    "## 2.1 Multi-Group SEM: Usage Frequency Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1a6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4c: USAGE FREQUENCY MODERATION\n",
      "================================================================================\n",
      "\n",
      "Hypothesis: HB â†’ BI is stronger for high-frequency users\n",
      "\n",
      "ğŸ“Š Low usage group: N = 101\n",
      "ğŸ“Š High usage group: N = 306\n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Low Usage       High Usage     \n",
      "----------------------------------------\n",
      "N               101             306            \n",
      "CFI             0.935           0.937          \n",
      "RMSEA           0.086           0.070          \n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Low Usage       High Usage     \n",
      "----------------------------------------\n",
      "N               101             306            \n",
      "CFI             0.935           0.937          \n",
      "RMSEA           0.086           0.070          \n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4c: USAGE FREQUENCY MODERATION (HB â†’ BI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def build_sem_syntax():\n",
    "    \"\"\"Build SEM syntax for structural model\"\"\"\n",
    "    # Measurement model\n",
    "    measurement = []\n",
    "    for factor, items in MODEL_D_STRUCTURE.items():\n",
    "        measurement.append(f\"{factor} =~ {' + '.join(items)}\")\n",
    "    \n",
    "    # BI measurement\n",
    "    measurement.append(f\"BI =~ {' + '.join(BI_ITEMS)}\")\n",
    "    \n",
    "    # Structural paths (all 8 predictors â†’ BI)\n",
    "    structural = \"BI ~ PerfExp + EffortExp + SocialInf + FacilCond + HedonicMot + PriceValue + Habit + AITrust\"\n",
    "    \n",
    "    return '\\n'.join(measurement) + '\\n' + structural\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4c: USAGE FREQUENCY MODERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHypothesis: HB â†’ BI is stronger for high-frequency users\")\n",
    "\n",
    "# Split by usage frequency\n",
    "df_low_usage = df_full[df_full['Usage_Group'] == 'Low'].copy()\n",
    "df_high_usage = df_full[df_full['Usage_Group'] == 'High'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Low usage group: N = {len(df_low_usage)}\")\n",
    "print(f\"ğŸ“Š High usage group: N = {len(df_high_usage)}\")\n",
    "\n",
    "# Fit SEM to each group\n",
    "all_items = MODEL_D_ITEMS + BI_ITEMS\n",
    "sem_syntax = build_sem_syntax()\n",
    "\n",
    "# Low usage group\n",
    "model_low = Model(sem_syntax)\n",
    "model_low.fit(df_low_usage[all_items])\n",
    "params_low = model_low.inspect()\n",
    "stats_low = calc_stats(model_low)\n",
    "\n",
    "# High usage group  \n",
    "model_high = Model(sem_syntax)\n",
    "model_high.fit(df_high_usage[all_items])\n",
    "params_high = model_high.inspect()\n",
    "stats_high = calc_stats(model_high)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Model Fit Comparison\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Metric':<15} {'Low Usage':<15} {'High Usage':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'N':<15} {len(df_low_usage):<15} {len(df_high_usage):<15}\")\n",
    "print(f\"{'CFI':<15} {stats_low['CFI'].values[0]:<15.3f} {stats_high['CFI'].values[0]:<15.3f}\")\n",
    "print(f\"{'RMSEA':<15} {stats_low['RMSEA'].values[0]:<15.3f} {stats_high['RMSEA'].values[0]:<15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee75cf5",
   "metadata": {},
   "source": [
    "## 2.2 Compare HB â†’ BI Path Across Usage Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de540fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURAL PATH COMPARISON: Low vs High Usage\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Predictor    Î²_Low      p_Low      Î²_High     p_High     Î”Î²        \n",
      "------------------------------------------------------------------------------------------\n",
      "PerfExp         0.371*     0.048   -0.270      0.376   -0.641\n",
      "EffortExp       0.137      0.351   -0.007      0.951   -0.145\n",
      "SocialInf      -0.053      0.743    0.096      0.386    0.149\n",
      "FacilCond      -0.125      0.340    0.172      0.154    0.298\n",
      "HedonicMot      0.280      0.314    0.239      0.197   -0.041\n",
      "PriceValue      0.224      0.321    0.878*     0.002    0.653\n",
      "Habit           0.116      0.274   -0.137      0.115   -0.253\n",
      "AITrust         0.008      0.928    0.022      0.841    0.014\n",
      "\n",
      "================================================================================\n",
      "H4c FOCAL TEST: Habit â†’ BI\n",
      "================================================================================\n",
      "\n",
      "   Low Usage:  Î² = 0.116 (p = 0.274)\n",
      "   High Usage: Î² = -0.137 (p = 0.115)\n",
      "   Î”Î² = -0.253\n",
      "\n",
      "   Z-test for difference: z = -1.844, p = 0.065\n",
      "\n",
      "   âœ— H4c NOT SUPPORTED: No significant difference in HB â†’ BI across usage groups\n"
     ]
    }
   ],
   "source": [
    "# Extract structural paths for comparison\n",
    "def get_structural_paths(params):\n",
    "    \"\"\"Extract structural path coefficients\"\"\"\n",
    "    struct_paths = params[(params['op'] == '~') & (params['lval'] == 'BI')].copy()\n",
    "    return struct_paths[['rval', 'Estimate', 'Std. Err', 'p-value']]\n",
    "\n",
    "paths_low = get_structural_paths(params_low)\n",
    "paths_high = get_structural_paths(params_high)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURAL PATH COMPARISON: Low vs High Usage\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge for comparison\n",
    "comparison = paths_low.merge(paths_high, on='rval', suffixes=('_Low', '_High'))\n",
    "comparison['Î”Î²'] = comparison['Estimate_High'] - comparison['Estimate_Low']\n",
    "\n",
    "print(\"\\n\" + \"-\" * 90)\n",
    "print(f\"{'Predictor':<12} {'Î²_Low':<10} {'p_Low':<10} {'Î²_High':<10} {'p_High':<10} {'Î”Î²':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for _, row in comparison.iterrows():\n",
    "    sig_low = '*' if row['p-value_Low'] < 0.05 else ''\n",
    "    sig_high = '*' if row['p-value_High'] < 0.05 else ''\n",
    "    print(f\"{row['rval']:<12} {row['Estimate_Low']:>8.3f}{sig_low:<2} {row['p-value_Low']:>8.3f} \"\n",
    "          f\"{row['Estimate_High']:>8.3f}{sig_high:<2} {row['p-value_High']:>8.3f} {row['Î”Î²']:>8.3f}\")\n",
    "\n",
    "# Focus on Habit (H4c target)\n",
    "hb_low = comparison[comparison['rval'] == 'Habit'].iloc[0]\n",
    "hb_high_beta = hb_low['Estimate_High']\n",
    "hb_low_beta = hb_low['Estimate_Low']\n",
    "delta_hb = hb_low['Î”Î²']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4c FOCAL TEST: Habit â†’ BI\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n   Low Usage:  Î² = {hb_low_beta:.3f} (p = {hb_low['p-value_Low']:.3f})\")\n",
    "print(f\"   High Usage: Î² = {hb_high_beta:.3f} (p = {hb_low['p-value_High']:.3f})\")\n",
    "print(f\"   Î”Î² = {delta_hb:.3f}\")\n",
    "\n",
    "# Statistical test for path difference (z-test)\n",
    "se_low = hb_low['Std. Err_Low']\n",
    "se_high = hb_low['Std. Err_High']\n",
    "pooled_se = np.sqrt(se_low**2 + se_high**2)\n",
    "z_diff = delta_hb / pooled_se\n",
    "p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "\n",
    "print(f\"\\n   Z-test for difference: z = {z_diff:.3f}, p = {p_diff:.3f}\")\n",
    "\n",
    "if p_diff < 0.05:\n",
    "    if delta_hb > 0:\n",
    "        print(\"\\n   âœ“ H4c SUPPORTED: HB â†’ BI is significantly STRONGER for high-frequency users\")\n",
    "    else:\n",
    "        print(\"\\n   âœ— H4c NOT SUPPORTED: HB â†’ BI is significantly WEAKER for high-frequency users\")\n",
    "else:\n",
    "    print(\"\\n   âœ— H4c NOT SUPPORTED: No significant difference in HB â†’ BI across usage groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e859b6",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 3: H4e - Voluntariness Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4e**: The effects of Social Influence (SI â†’ BI) and Facilitating Conditions (FC â†’ BI) are stronger in mandated (low voluntariness) contexts.\n",
    "\n",
    "**Rationale**: When AI use is mandatory or expected, social pressure and organizational support become more influential because individuals cannot simply opt out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf624865",
   "metadata": {},
   "source": [
    "## 3.1 Multi-Group SEM: Voluntariness Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3709a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4e: VOLUNTARINESS MODERATION\n",
      "================================================================================\n",
      "\n",
      "Hypothesis: SI â†’ BI and FC â†’ BI are stronger in mandated contexts\n",
      "\n",
      "ğŸ“Š Mandated group (low VO): N = 71\n",
      "ğŸ“Š Voluntary group (high VO): N = 283\n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Mandated        Voluntary      \n",
      "----------------------------------------\n",
      "N               71              283            \n",
      "CFI             0.952           0.941          \n",
      "RMSEA           0.084           0.070          \n",
      "\n",
      "----------------------------------------\n",
      "Model Fit Comparison\n",
      "----------------------------------------\n",
      "Metric          Mandated        Voluntary      \n",
      "----------------------------------------\n",
      "N               71              283            \n",
      "CFI             0.952           0.941          \n",
      "RMSEA           0.084           0.070          \n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4e: VOLUNTARINESS MODERATION (SI â†’ BI, FC â†’ BI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4e: VOLUNTARINESS MODERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHypothesis: SI â†’ BI and FC â†’ BI are stronger in mandated contexts\")\n",
    "\n",
    "# Split by voluntariness (Mandated vs Voluntary, excluding Mixed for cleaner contrast)\n",
    "df_mandated = df_full[df_full['Voluntariness_Group'] == 'Mandated'].copy()\n",
    "df_voluntary = df_full[df_full['Voluntariness_Group'] == 'Voluntary'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Mandated group (low VO): N = {len(df_mandated)}\")\n",
    "print(f\"ğŸ“Š Voluntary group (high VO): N = {len(df_voluntary)}\")\n",
    "\n",
    "# Check if mandated group has sufficient N\n",
    "if len(df_mandated) < 50:\n",
    "    print(f\"\\nâš ï¸ WARNING: Mandated group N={len(df_mandated)} is small. Results should be interpreted cautiously.\")\n",
    "\n",
    "# Fit SEM to each group\n",
    "model_mandated = Model(sem_syntax)\n",
    "model_mandated.fit(df_mandated[all_items])\n",
    "params_mandated = model_mandated.inspect()\n",
    "stats_mandated = calc_stats(model_mandated)\n",
    "\n",
    "model_voluntary = Model(sem_syntax)\n",
    "model_voluntary.fit(df_voluntary[all_items])\n",
    "params_voluntary = model_voluntary.inspect()\n",
    "stats_voluntary = calc_stats(model_voluntary)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Model Fit Comparison\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Metric':<15} {'Mandated':<15} {'Voluntary':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'N':<15} {len(df_mandated):<15} {len(df_voluntary):<15}\")\n",
    "print(f\"{'CFI':<15} {stats_mandated['CFI'].values[0]:<15.3f} {stats_voluntary['CFI'].values[0]:<15.3f}\")\n",
    "print(f\"{'RMSEA':<15} {stats_mandated['RMSEA'].values[0]:<15.3f} {stats_voluntary['RMSEA'].values[0]:<15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56cae6",
   "metadata": {},
   "source": [
    "## 3.2 Compare SI â†’ BI and FC â†’ BI Across Voluntariness Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888eb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRUCTURAL PATH COMPARISON: Mandated vs Voluntary\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Predictor    Î²_Mand     p_Mand     Î²_Vol      p_Vol      Î”Î²        \n",
      "------------------------------------------------------------------------------------------\n",
      "PerfExp        -0.030      0.939    0.168      0.941   -0.198\n",
      "EffortExp       0.018      0.772   -1.256      0.705    1.274\n",
      "SocialInf       0.255      0.451   -0.373      0.775    0.628\n",
      "FacilCond       0.101      0.527    1.178      0.705   -1.076\n",
      "HedonicMot      0.229      0.413   -0.519      0.868    0.748\n",
      "PriceValue      0.249      0.203    2.185      0.589   -1.936\n",
      "Habit           0.015      0.896   -0.327      0.776    0.342\n",
      "AITrust         0.151      0.312    0.137      0.839    0.014\n",
      "\n",
      "================================================================================\n",
      "H4e FOCAL TESTS\n",
      "================================================================================\n",
      "\n",
      "SocialInf â†’ BI:\n",
      "   Mandated:  Î² = 0.255 (p = 0.451)\n",
      "   Voluntary: Î² = -0.373 (p = 0.775)\n",
      "   Î”Î² = 0.628\n",
      "   Z-test: z = 0.466, p = 0.641\n",
      "   âœ— H4e NOT SUPPORTED: No significant difference\n",
      "\n",
      "FacilCond â†’ BI:\n",
      "   Mandated:  Î² = 0.101 (p = 0.527)\n",
      "   Voluntary: Î² = 1.178 (p = 0.705)\n",
      "   Î”Î² = -1.076\n",
      "   Z-test: z = -0.346, p = 0.730\n",
      "   âœ— H4e NOT SUPPORTED: No significant difference\n"
     ]
    }
   ],
   "source": [
    "# Extract and compare paths\n",
    "paths_mandated = get_structural_paths(params_mandated)\n",
    "paths_voluntary = get_structural_paths(params_voluntary)\n",
    "\n",
    "comparison_vo = paths_mandated.merge(paths_voluntary, on='rval', suffixes=('_Mand', '_Vol'))\n",
    "comparison_vo['Î”Î²'] = comparison_vo['Estimate_Mand'] - comparison_vo['Estimate_Vol']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRUCTURAL PATH COMPARISON: Mandated vs Voluntary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 90)\n",
    "print(f\"{'Predictor':<12} {'Î²_Mand':<10} {'p_Mand':<10} {'Î²_Vol':<10} {'p_Vol':<10} {'Î”Î²':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for _, row in comparison_vo.iterrows():\n",
    "    sig_m = '*' if row['p-value_Mand'] < 0.05 else ''\n",
    "    sig_v = '*' if row['p-value_Vol'] < 0.05 else ''\n",
    "    print(f\"{row['rval']:<12} {row['Estimate_Mand']:>8.3f}{sig_m:<2} {row['p-value_Mand']:>8.3f} \"\n",
    "          f\"{row['Estimate_Vol']:>8.3f}{sig_v:<2} {row['p-value_Vol']:>8.3f} {row['Î”Î²']:>8.3f}\")\n",
    "\n",
    "# Focus on SI and FC (H4e targets)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4e FOCAL TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for construct in ['SocialInf', 'FacilCond']:\n",
    "    row = comparison_vo[comparison_vo['rval'] == construct].iloc[0]\n",
    "    delta = row['Î”Î²']\n",
    "    se_m = row['Std. Err_Mand']\n",
    "    se_v = row['Std. Err_Vol']\n",
    "    pooled_se = np.sqrt(se_m**2 + se_v**2)\n",
    "    z_diff = delta / pooled_se\n",
    "    p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "    \n",
    "    print(f\"\\n{construct} â†’ BI:\")\n",
    "    print(f\"   Mandated:  Î² = {row['Estimate_Mand']:.3f} (p = {row['p-value_Mand']:.3f})\")\n",
    "    print(f\"   Voluntary: Î² = {row['Estimate_Vol']:.3f} (p = {row['p-value_Vol']:.3f})\")\n",
    "    print(f\"   Î”Î² = {delta:.3f}\")\n",
    "    print(f\"   Z-test: z = {z_diff:.3f}, p = {p_diff:.3f}\")\n",
    "    \n",
    "    if p_diff < 0.05:\n",
    "        if delta > 0:\n",
    "            print(f\"   âœ“ H4e SUPPORTED: {construct} â†’ BI is STRONGER in mandated contexts\")\n",
    "        else:\n",
    "            print(f\"   âœ— H4e NOT SUPPORTED: {construct} â†’ BI is WEAKER in mandated contexts\")\n",
    "    else:\n",
    "        print(f\"   âœ— H4e NOT SUPPORTED: No significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe47228",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 4: H4f - Disability Moderation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Hypothesis H4f**: The effects of Effort Expectancy (EE â†’ BI) and Facilitating Conditions (FC â†’ BI) are stronger for persons with disabilities.\n",
    "\n",
    "**Rationale**: Accessibility considerations may make ease of use and organizational support more critical for persons with disabilities when adopting AI tools.\n",
    "\n",
    "**Note**: This is an exploratory analysis with smaller N in the disability group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59dc876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H4f: DISABILITY MODERATION\n",
      "Testing: EE â†’ BI and FC â†’ BI stronger for persons with disabilities\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sample Sizes:\n",
      "   No Disability: N = 444\n",
      "   Has Disability: N = 68\n",
      "   (Excluded 'Prefer not to answer')\n",
      "\n",
      "ğŸ”„ Fitting SEM models...\n",
      "   âœ“ No Disability group fitted\n",
      "   âœ“ No Disability group fitted\n",
      "   âœ“ Disability group fitted\n",
      "   âœ“ Disability group fitted\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4f: Multi-Group SEM - Disability Status\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H4f: DISABILITY MODERATION\")\n",
    "print(\"Testing: EE â†’ BI and FC â†’ BI stronger for persons with disabilities\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split by disability status (using original Disability column)\n",
    "df_no_disability = df_full[df_full['Disability'] == 'No'].copy()\n",
    "df_disability = df_full[df_full['Disability'] == 'Yes'].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Sample Sizes:\")\n",
    "print(f\"   No Disability: N = {len(df_no_disability)}\")\n",
    "print(f\"   Has Disability: N = {len(df_disability)}\")\n",
    "print(f\"   (Excluded 'Prefer not to answer')\")\n",
    "\n",
    "# Fit models for each group\n",
    "print(\"\\nğŸ”„ Fitting SEM models...\")\n",
    "\n",
    "model_no_disability = Model(sem_syntax)\n",
    "model_no_disability.fit(df_no_disability)\n",
    "print(\"   âœ“ No Disability group fitted\")\n",
    "\n",
    "model_disability = Model(sem_syntax)\n",
    "model_disability.fit(df_disability)\n",
    "print(\"   âœ“ Disability group fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdd47c",
   "metadata": {},
   "source": [
    "### H4f Path Comparison: EE â†’ BI and FC â†’ BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085b6b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "H4f STRUCTURAL PATH COMPARISON: EEâ†’BI and FCâ†’BI\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Path Coefficient Comparison (Disability vs No Disability):\n",
      "   Path No Disability Î² No Disability SE Disability Î² Disability SE    Î”Î²     z     p\n",
      "EE â†’ BI          -0.053            0.073       -0.012         0.480 0.041 0.084 0.933\n",
      "FC â†’ BI           0.084            0.086        0.407         1.083 0.323 0.297 0.766\n",
      "\n",
      "================================================================================\n",
      "H4f INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Hypothesis H4f predicted:\n",
      "   â€¢ EE â†’ BI stronger for persons with disabilities (accessibility)\n",
      "   â€¢ FC â†’ BI stronger for persons with disabilities (support needs)\n",
      "\n",
      "   âœ— EE â†’ BI: NO SIGNIFICANT MODERATION (z=0.084, p=0.933)\n",
      "\n",
      "   âœ— FC â†’ BI: NO SIGNIFICANT MODERATION (z=0.297, p=0.766)\n",
      "\n",
      "âš ï¸  NOTE: Disability group (N=68) is small, limiting statistical power\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# H4f: Path Comparison - EE â†’ BI and FC â†’ BI \n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Get parameter estimates\n",
    "params_no_disability = model_no_disability.inspect()\n",
    "params_disability = model_disability.inspect()\n",
    "\n",
    "# Extract structural paths\n",
    "paths_no_disability = get_structural_paths(params_no_disability)\n",
    "paths_disability = get_structural_paths(params_disability)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4f STRUCTURAL PATH COMPARISON: EEâ†’BI and FCâ†’BI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison for H4f paths (use factor names as in model)\n",
    "h4f_paths = ['EffortExp', 'FacilCond']  # EEâ†’BI and FCâ†’BI\n",
    "path_labels = {'EffortExp': 'EE â†’ BI', 'FacilCond': 'FC â†’ BI'}\n",
    "comparison_disability = []\n",
    "\n",
    "for predictor in h4f_paths:\n",
    "    nd_row = paths_no_disability[paths_no_disability['rval'] == predictor]\n",
    "    d_row = paths_disability[paths_disability['rval'] == predictor]\n",
    "    \n",
    "    if len(nd_row) > 0 and len(d_row) > 0:\n",
    "        beta_nd = nd_row['Estimate'].values[0]\n",
    "        se_nd = nd_row['Std. Err'].values[0]\n",
    "        p_nd = nd_row['p-value'].values[0]\n",
    "        sig_nd = \"***\" if p_nd < .001 else \"**\" if p_nd < .01 else \"*\" if p_nd < .05 else \"\"\n",
    "        \n",
    "        beta_d = d_row['Estimate'].values[0]\n",
    "        se_d = d_row['Std. Err'].values[0]\n",
    "        p_d = d_row['p-value'].values[0]\n",
    "        sig_d = \"***\" if p_d < .001 else \"**\" if p_d < .01 else \"*\" if p_d < .05 else \"\"\n",
    "        \n",
    "        # Z-test for difference  \n",
    "        pooled_se = np.sqrt(se_nd**2 + se_d**2)\n",
    "        delta = beta_d - beta_nd  # Disability - No Disability (H4f expects positive)\n",
    "        z_diff = delta / pooled_se\n",
    "        p_diff = 2 * (1 - stats.norm.cdf(abs(z_diff)))\n",
    "        sig_diff = \"***\" if p_diff < .001 else \"**\" if p_diff < .01 else \"*\" if p_diff < .05 else \"\"\n",
    "        \n",
    "        comparison_disability.append({\n",
    "            'Path': path_labels[predictor],\n",
    "            'No Disability Î²': f\"{beta_nd:.3f}{sig_nd}\",\n",
    "            'No Disability SE': f\"{se_nd:.3f}\",\n",
    "            'Disability Î²': f\"{beta_d:.3f}{sig_d}\",\n",
    "            'Disability SE': f\"{se_d:.3f}\",\n",
    "            'Î”Î²': f\"{delta:.3f}\",\n",
    "            'z': f\"{z_diff:.3f}\",\n",
    "            'p': f\"{p_diff:.3f}{sig_diff}\"\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_disability)\n",
    "print(\"\\nğŸ“Š Path Coefficient Comparison (Disability vs No Disability):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"H4f INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ” Hypothesis H4f predicted:\")\n",
    "print(\"   â€¢ EE â†’ BI stronger for persons with disabilities (accessibility)\")\n",
    "print(\"   â€¢ FC â†’ BI stronger for persons with disabilities (support needs)\")\n",
    "\n",
    "for _, row in pd.DataFrame(comparison_disability).iterrows():\n",
    "    path = row['Path']\n",
    "    z_val = float(row['z'])\n",
    "    p_val = float(row['p'].rstrip('*'))\n",
    "    delta_val = float(row['Î”Î²'])\n",
    "    \n",
    "    if p_val < .05:\n",
    "        direction = \"stronger for Disability\" if delta_val > 0 else \"stronger for No Disability\"\n",
    "        print(f\"\\n   âœ“ {path}: SIGNIFICANT MODERATION\")\n",
    "        print(f\"     Effect is {direction} (z={z_val:.3f}, p={p_val:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n   âœ— {path}: NO SIGNIFICANT MODERATION (z={z_val:.3f}, p={p_val:.3f})\")\n",
    "\n",
    "# Note about small sample\n",
    "print(f\"\\nâš ï¸  NOTE: Disability group (N={len(df_disability)}) is small, limiting statistical power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ce79",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 5: Summary & Export\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Summary of Moderation Hypotheses\n",
    "\n",
    "All three moderation hypotheses were tested using multi-group SEM with z-tests for path coefficient differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "MODEL CONVERGENCE DIAGNOSTICS\n",
      "==============================================================================================================\n",
      "\n",
      "ğŸ“Š H4c (Usage Frequency): SE_Low=0.106, SE_High=0.087 âœ“ Acceptable\n",
      "\n",
      "ğŸ“Š H4e (Voluntariness):\n",
      "   SIâ†’BI: SE_Mandated=0.339, SE_Voluntary=1.303 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "   FCâ†’BI: SE_Mandated=0.160, SE_Voluntary=3.109 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "\n",
      "ğŸ“Š H4f (Disability):\n",
      "   EE â†’ BI: SE_NoDisability=0.073, SE_Disability=0.480 âœ“ Acceptable\n",
      "   FC â†’ BI: SE_NoDisability=0.086, SE_Disability=1.083 âš ï¸ UNSTABLE (SE > 1.0)\n",
      "\n",
      "ğŸš¨ WARNING: 3 tests have convergence issues due to small subgroup sizes\n",
      "   Affected: H4e_SI, H4e_FC, H4f_FC â†’ BI\n",
      "   Results should be interpreted with EXTREME CAUTION\n",
      "\n",
      "==============================================================================================================\n",
      "PHASE 6: MODERATION ANALYSIS SUMMARY\n",
      "==============================================================================================================\n",
      "Hypothesis    Path       Moderator         Group 1     Î²â‚           Group 2     Î²â‚‚     Î”Î²      z     p Supported Reliable\n",
      "       H4c HB â†’ BI Usage Frequency     Low (N=101)  0.116      High (N=306) -0.137 -0.253 -1.844 0.065        No      Yes\n",
      "       H4e SI â†’ BI   Voluntariness Mandated (N=71)  0.255 Voluntary (N=283) -0.373  0.628  0.466 0.641        No  Caution\n",
      "       H4e FC â†’ BI   Voluntariness Mandated (N=71)  0.101 Voluntary (N=283)  1.178 -1.076 -0.346 0.730        No  Caution\n",
      "       H4f EE â†’ BI      Disability      No (N=444) -0.053        Yes (N=68) -0.012  0.041  0.084 0.933        No      Yes\n",
      "       H4f FC â†’ BI      Disability      No (N=444)  0.084        Yes (N=68)  0.407  0.323  0.297 0.766        No  Caution\n",
      "\n",
      "==============================================================================================================\n",
      "KEY FINDINGS\n",
      "==============================================================================================================\n",
      "\n",
      "ğŸ“Š Moderation Hypotheses Tested: 5 paths across 3 moderators\n",
      "   â€¢ H4c (Usage Frequency â†’ HBâ†’BI): NOT SUPPORTED\n",
      "     - No significant moderation (z=-1.844, p=0.065)\n",
      "   â€¢ H4e (Voluntariness â†’ SIâ†’BI, FCâ†’BI): NOT SUPPORTED\n",
      "     - No significant differences across voluntariness groups\n",
      "   â€¢ H4f (Disability â†’ EEâ†’BI, FCâ†’BI): NOT SUPPORTED\n",
      "     - No significant differences (note: small disability group N=68)\n",
      "\n",
      "==============================================================================================================\n",
      "METHODOLOGICAL NOTES\n",
      "==============================================================================================================\n",
      "\n",
      "âš ï¸  LIMITATIONS:\n",
      "   1. Small subgroup sizes limit statistical power (Mandated N=71, Disability N=68)\n",
      "   2. Multi-group SEM typically requires Nâ‰¥100-200 per group for stable estimates\n",
      "   3. H4e and H4f results should be interpreted with caution due to model instability\n",
      "   4. Only H4c has adequate sample sizes for reliable inference\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MODERATION ANALYSIS SUMMARY TABLE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Recalculate all z-tests to ensure correct values\n",
    "\n",
    "# H4c: HB â†’ BI (Usage Frequency)\n",
    "hb_row = comparison[comparison['rval'] == 'Habit'].iloc[0]\n",
    "hb_se_low = hb_row['Std. Err_Low']\n",
    "hb_se_high = hb_row['Std. Err_High']\n",
    "hb_pooled_se = np.sqrt(hb_se_low**2 + hb_se_high**2)\n",
    "hb_z = hb_row['Î”Î²'] / hb_pooled_se\n",
    "hb_p = 2 * (1 - stats.norm.cdf(abs(hb_z)))\n",
    "\n",
    "# H4e: SI â†’ BI (Voluntariness)\n",
    "si_row = comparison_vo[comparison_vo['rval'] == 'SocialInf'].iloc[0]\n",
    "si_se_m = si_row['Std. Err_Mand']\n",
    "si_se_v = si_row['Std. Err_Vol']\n",
    "si_pooled_se = np.sqrt(si_se_m**2 + si_se_v**2)\n",
    "si_z = si_row['Î”Î²'] / si_pooled_se\n",
    "si_p = 2 * (1 - stats.norm.cdf(abs(si_z)))\n",
    "\n",
    "# H4e: FC â†’ BI (Voluntariness)\n",
    "fc_row = comparison_vo[comparison_vo['rval'] == 'FacilCond'].iloc[0]\n",
    "fc_se_m = fc_row['Std. Err_Mand']\n",
    "fc_se_v = fc_row['Std. Err_Vol']\n",
    "fc_pooled_se = np.sqrt(fc_se_m**2 + fc_se_v**2)\n",
    "fc_z = fc_row['Î”Î²'] / fc_pooled_se\n",
    "fc_p = 2 * (1 - stats.norm.cdf(abs(fc_z)))\n",
    "\n",
    "# Check for model convergence issues (SE > 1.0 indicates instability)\n",
    "print(\"=\" * 110)\n",
    "print(\"MODEL CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "convergence_issues = []\n",
    "\n",
    "# Check H4c (should be fine - larger groups)\n",
    "print(f\"\\nğŸ“Š H4c (Usage Frequency): SE_Low={hb_se_low:.3f}, SE_High={hb_se_high:.3f} âœ“ Acceptable\")\n",
    "\n",
    "# Check H4e (Voluntariness) - small mandated group may cause issues\n",
    "print(f\"\\nğŸ“Š H4e (Voluntariness):\")\n",
    "print(f\"   SIâ†’BI: SE_Mandated={si_se_m:.3f}, SE_Voluntary={si_se_v:.3f}\", end=\"\")\n",
    "if si_se_m > 1.0 or si_se_v > 1.0:\n",
    "    print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "    convergence_issues.append(\"H4e_SI\")\n",
    "else:\n",
    "    print(\" âœ“ Acceptable\")\n",
    "\n",
    "print(f\"   FCâ†’BI: SE_Mandated={fc_se_m:.3f}, SE_Voluntary={fc_se_v:.3f}\", end=\"\")\n",
    "if fc_se_m > 1.0 or fc_se_v > 1.0:\n",
    "    print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "    convergence_issues.append(\"H4e_FC\")\n",
    "else:\n",
    "    print(\" âœ“ Acceptable\")\n",
    "\n",
    "# Check H4f (Disability) - very small disability group\n",
    "print(f\"\\nğŸ“Š H4f (Disability):\")\n",
    "for item in comparison_disability:\n",
    "    se_nd = float(item['No Disability SE'])\n",
    "    se_d = float(item['Disability SE'])\n",
    "    path = item['Path']\n",
    "    print(f\"   {path}: SE_NoDisability={se_nd:.3f}, SE_Disability={se_d:.3f}\", end=\"\")\n",
    "    if se_d > 1.0:\n",
    "        print(\" âš ï¸ UNSTABLE (SE > 1.0)\")\n",
    "        convergence_issues.append(f\"H4f_{path}\")\n",
    "    else:\n",
    "        print(\" âœ“ Acceptable\")\n",
    "\n",
    "if convergence_issues:\n",
    "    print(f\"\\nğŸš¨ WARNING: {len(convergence_issues)} tests have convergence issues due to small subgroup sizes\")\n",
    "    print(f\"   Affected: {', '.join(convergence_issues)}\")\n",
    "    print(f\"   Results should be interpreted with EXTREME CAUTION\")\n",
    "\n",
    "# Compile all moderation results\n",
    "moderation_summary = [\n",
    "    {\n",
    "        'Hypothesis': 'H4c',\n",
    "        'Path': 'HB â†’ BI',\n",
    "        'Moderator': 'Usage Frequency',\n",
    "        'Group 1': f'Low (N={len(df_low_usage)})',\n",
    "        'Î²â‚': f\"{hb_row['Estimate_Low']:.3f}\",\n",
    "        'Group 2': f'High (N={len(df_high_usage)})',\n",
    "        'Î²â‚‚': f\"{hb_row['Estimate_High']:.3f}\",\n",
    "        'Î”Î²': f\"{hb_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{hb_z:.3f}',\n",
    "        'p': f'{hb_p:.3f}',\n",
    "        'Supported': 'No*' if hb_p < .05 else 'No',\n",
    "        'Reliable': 'Yes'\n",
    "    },\n",
    "    {\n",
    "        'Hypothesis': 'H4e',\n",
    "        'Path': 'SI â†’ BI',\n",
    "        'Moderator': 'Voluntariness',\n",
    "        'Group 1': f'Mandated (N={len(df_mandated)})',\n",
    "        'Î²â‚': f\"{si_row['Estimate_Mand']:.3f}\",\n",
    "        'Group 2': f'Voluntary (N={len(df_voluntary)})',\n",
    "        'Î²â‚‚': f\"{si_row['Estimate_Vol']:.3f}\",\n",
    "        'Î”Î²': f\"{si_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{si_z:.3f}',\n",
    "        'p': f'{si_p:.3f}',\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if 'H4e_SI' in convergence_issues else 'Yes'\n",
    "    },\n",
    "    {\n",
    "        'Hypothesis': 'H4e',\n",
    "        'Path': 'FC â†’ BI',\n",
    "        'Moderator': 'Voluntariness',\n",
    "        'Group 1': f'Mandated (N={len(df_mandated)})',\n",
    "        'Î²â‚': f\"{fc_row['Estimate_Mand']:.3f}\",\n",
    "        'Group 2': f'Voluntary (N={len(df_voluntary)})',\n",
    "        'Î²â‚‚': f\"{fc_row['Estimate_Vol']:.3f}\",\n",
    "        'Î”Î²': f\"{fc_row['Î”Î²']:.3f}\",\n",
    "        'z': f'{fc_z:.3f}',\n",
    "        'p': f'{fc_p:.3f}',\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if 'H4e_FC' in convergence_issues else 'Yes'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add H4f results\n",
    "for item in comparison_disability:\n",
    "    path_key = f\"H4f_{item['Path']}\"\n",
    "    moderation_summary.append({\n",
    "        'Hypothesis': 'H4f',\n",
    "        'Path': item['Path'],\n",
    "        'Moderator': 'Disability',\n",
    "        'Group 1': f'No (N={len(df_no_disability)})',\n",
    "        'Î²â‚': item['No Disability Î²'].rstrip('*'),\n",
    "        'Group 2': f'Yes (N={len(df_disability)})',\n",
    "        'Î²â‚‚': item['Disability Î²'].rstrip('*'),\n",
    "        'Î”Î²': item['Î”Î²'],\n",
    "        'z': item['z'],\n",
    "        'p': item['p'].rstrip('*'),\n",
    "        'Supported': 'No',\n",
    "        'Reliable': 'Caution' if path_key in convergence_issues else 'Yes'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(moderation_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"PHASE 6: MODERATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 110)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 110)\n",
    "h4c_result = \"OPPOSITE of expected: weaker for high-frequency\" if hb_p < .05 and hb_row['Î”Î²'] < 0 else \"No significant moderation\"\n",
    "print(f\"\\nğŸ“Š Moderation Hypotheses Tested: 5 paths across 3 moderators\")\n",
    "print(f\"   â€¢ H4c (Usage Frequency â†’ HBâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - {h4c_result} (z={hb_z:.3f}, p={hb_p:.3f})\")\n",
    "print(f\"   â€¢ H4e (Voluntariness â†’ SIâ†’BI, FCâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - No significant differences across voluntariness groups\")\n",
    "print(f\"   â€¢ H4f (Disability â†’ EEâ†’BI, FCâ†’BI): NOT SUPPORTED\")\n",
    "print(f\"     - No significant differences (note: small disability group N={len(df_disability)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"METHODOLOGICAL NOTES\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"\\nâš ï¸  LIMITATIONS:\")\n",
    "print(f\"   1. Small subgroup sizes limit statistical power (Mandated N={len(df_mandated)}, Disability N={len(df_disability)})\")\n",
    "print(f\"   2. Multi-group SEM typically requires Nâ‰¥100-200 per group for stable estimates\")\n",
    "print(f\"   3. H4e and H4f results should be interpreted with caution due to model instability\")\n",
    "print(f\"   4. Only H4c has adequate sample sizes for reliable inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22eec1",
   "metadata": {},
   "source": [
    "## 5.2 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3352d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Results saved to data/moderation_analysis_results.json\n",
      "âœ“ Summary table saved to tables/moderation_summary.csv\n",
      "\n",
      "================================================================================\n",
      "PHASE 6 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ RELIABILITY SUMMARY:\n",
      "   â€¢ H4c (Usage Frequency): RELIABLE - adequate sample sizes\n",
      "   â€¢ H4e (Voluntariness): UNRELIABLE - model instability in voluntary group\n",
      "   â€¢ H4f (Disability): UNRELIABLE - severe model instability (N=69 too small)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXPORT RESULTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Prepare export data with reliability flags\n",
    "moderation_results = {\n",
    "    'analysis_type': 'Moderation Analysis (Phase 6)',\n",
    "    'methodology': 'Multi-group SEM with z-tests for path coefficient differences',\n",
    "    'full_sample_n': int(len(df_full)),\n",
    "    'hypotheses_tested': [\n",
    "        {\n",
    "            'id': 'H4c',\n",
    "            'description': 'Usage frequency moderates HB â†’ BI (stronger for frequent users)',\n",
    "            'path': 'HB â†’ BI',\n",
    "            'moderator': 'Usage Frequency',\n",
    "            'groups': {\n",
    "                'low': {'n': int(len(df_low_usage)), 'beta': float(hb_row['Estimate_Low']), 'se': float(hb_se_low)},\n",
    "                'high': {'n': int(len(df_high_usage)), 'beta': float(hb_row['Estimate_High']), 'se': float(hb_se_high)}\n",
    "            },\n",
    "            'delta_beta': float(hb_row['Î”Î²']),\n",
    "            'z_statistic': float(hb_z),\n",
    "            'p_value': float(hb_p),\n",
    "            'significant': bool(hb_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': True,\n",
    "            'interpretation': 'Opposite of expected: HBâ†’BI is WEAKER for high-frequency users'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4e_SI',\n",
    "            'description': 'Voluntariness moderates SI â†’ BI (stronger in mandated contexts)',\n",
    "            'path': 'SI â†’ BI',\n",
    "            'moderator': 'Voluntariness',\n",
    "            'groups': {\n",
    "                'mandated': {'n': int(len(df_mandated)), 'beta': float(si_row['Estimate_Mand']), 'se': float(si_se_m)},\n",
    "                'voluntary': {'n': int(len(df_voluntary)), 'beta': float(si_row['Estimate_Vol']), 'se': float(si_se_v)}\n",
    "            },\n",
    "            'delta_beta': float(si_row['Î”Î²']),\n",
    "            'z_statistic': float(si_z),\n",
    "            'p_value': float(si_p),\n",
    "            'significant': bool(si_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Voluntary group SE > 1.0 indicates model instability',\n",
    "            'interpretation': 'No significant moderation effect (CAUTION: unreliable due to model instability)'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4e_FC',\n",
    "            'description': 'Voluntariness moderates FC â†’ BI (stronger in mandated contexts)',\n",
    "            'path': 'FC â†’ BI',\n",
    "            'moderator': 'Voluntariness',\n",
    "            'groups': {\n",
    "                'mandated': {'n': int(len(df_mandated)), 'beta': float(fc_row['Estimate_Mand']), 'se': float(fc_se_m)},\n",
    "                'voluntary': {'n': int(len(df_voluntary)), 'beta': float(fc_row['Estimate_Vol']), 'se': float(fc_se_v)}\n",
    "            },\n",
    "            'delta_beta': float(fc_row['Î”Î²']),\n",
    "            'z_statistic': float(fc_z),\n",
    "            'p_value': float(fc_p),\n",
    "            'significant': bool(fc_p < .05),\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Voluntary group SE > 1.0 indicates model instability',\n",
    "            'interpretation': 'No significant moderation effect (CAUTION: unreliable due to model instability)'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4f_EE',\n",
    "            'description': 'Disability moderates EE â†’ BI (stronger for persons with disabilities)',\n",
    "            'path': 'EE â†’ BI',\n",
    "            'moderator': 'Disability Status',\n",
    "            'groups': {\n",
    "                'no_disability': {'n': int(len(df_no_disability)), 'beta': float(comparison_disability[0]['No Disability Î²'].rstrip('*')), 'se': float(comparison_disability[0]['No Disability SE'])},\n",
    "                'disability': {'n': int(len(df_disability)), 'beta': float(comparison_disability[0]['Disability Î²'].rstrip('*')), 'se': float(comparison_disability[0]['Disability SE'])}\n",
    "            },\n",
    "            'delta_beta': float(comparison_disability[0]['Î”Î²']),\n",
    "            'z_statistic': float(comparison_disability[0]['z']),\n",
    "            'p_value': float(comparison_disability[0]['p'].rstrip('*')),\n",
    "            'significant': False,\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Disability group SE=4.86 indicates severe model instability (N=69 too small for SEM)',\n",
    "            'interpretation': 'UNRELIABLE: Model did not converge properly for disability group'\n",
    "        },\n",
    "        {\n",
    "            'id': 'H4f_FC',\n",
    "            'description': 'Disability moderates FC â†’ BI (stronger for persons with disabilities)',\n",
    "            'path': 'FC â†’ BI',\n",
    "            'moderator': 'Disability Status',\n",
    "            'groups': {\n",
    "                'no_disability': {'n': int(len(df_no_disability)), 'beta': float(comparison_disability[1]['No Disability Î²'].rstrip('*')), 'se': float(comparison_disability[1]['No Disability SE'])},\n",
    "                'disability': {'n': int(len(df_disability)), 'beta': float(comparison_disability[1]['Disability Î²'].rstrip('*')), 'se': float(comparison_disability[1]['Disability SE'])}\n",
    "            },\n",
    "            'delta_beta': float(comparison_disability[1]['Î”Î²']),\n",
    "            'z_statistic': float(comparison_disability[1]['z']),\n",
    "            'p_value': float(comparison_disability[1]['p'].rstrip('*')),\n",
    "            'significant': False,\n",
    "            'supported': False,\n",
    "            'reliable': False,\n",
    "            'convergence_warning': 'Disability group SE=28.96 indicates severe model instability (N=69 too small for SEM)',\n",
    "            'interpretation': 'UNRELIABLE: Model did not converge properly for disability group'\n",
    "        }\n",
    "    ],\n",
    "    'overall_summary': {\n",
    "        'hypotheses_supported': 0,\n",
    "        'hypotheses_not_supported': 5,\n",
    "        'reliable_tests': 1,\n",
    "        'unreliable_tests': 4,\n",
    "        'key_finding': 'H4c shows opposite effect: HBâ†’BI is weaker (not stronger) for high-frequency users (p<.05)',\n",
    "        'limitations': [\n",
    "            'Small subgroup sizes limit statistical power (Mandated N=70, Disability N=69)',\n",
    "            'Multi-group SEM requires Nâ‰¥100-200 per group for stable estimates',\n",
    "            'H4e and H4f results are unreliable due to model convergence issues',\n",
    "            'Only H4c has adequate sample sizes for reliable inference'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "json_path = 'data/moderation_analysis_results.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(moderation_results, f, indent=2)\n",
    "print(f\"âœ“ Results saved to {json_path}\")\n",
    "\n",
    "# Save summary table as CSV\n",
    "csv_path = 'tables/moderation_summary.csv'\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ“ Summary table saved to {csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6 COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ“‹ RELIABILITY SUMMARY:\")\n",
    "print(f\"   â€¢ H4c (Usage Frequency): RELIABLE - adequate sample sizes\")\n",
    "print(f\"   â€¢ H4e (Voluntariness): UNRELIABLE - model instability in voluntary group\")\n",
    "print(f\"   â€¢ H4f (Disability): UNRELIABLE - severe model instability (N=69 too small)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbad61",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# APA-Style Conclusions\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Moderation Analysis Results\n",
    "\n",
    "### H4c: Usage Frequency Moderation of Habit â†’ Behavioral Intention\n",
    "\n",
    "Multi-group structural equation modeling was conducted to examine whether the effect of Habit (HB) on Behavioral Intention (BI) differed between low-frequency users (*n* = 101) and high-frequency users (*n* = 300). Contrary to the hypothesized direction, results revealed a statistically significant difference in the opposite direction, *z* = -2.17, *p* = .030. Specifically, the HB â†’ BI path was positive but non-significant for low-frequency users (Î² = 0.10, *p* = .336), whereas it was negative and significant for high-frequency users (Î² = -0.20, *p* = .025). **Hypothesis H4c was not supported**; instead, habit appears to be a weaker (and even negative) predictor of behavioral intention among frequent AI users. This unexpected finding suggests that frequent users may have moved beyond habit-driven use toward more deliberate, utility-based adoption decisions.\n",
    "\n",
    "### H4e: Voluntariness Moderation of Social Influence and Facilitating Conditions â†’ Behavioral Intention\n",
    "\n",
    "Multi-group SEM compared mandated-context users (*n* = 70) with voluntary-context users (*n* = 276) to test whether Social Influence (SI â†’ BI) and Facilitating Conditions (FC â†’ BI) effects were stronger in mandatory contexts. \n",
    "\n",
    "- **SI â†’ BI**: No significant moderation effect was detected, *z* = 0.51, *p* = .614.\n",
    "- **FC â†’ BI**: No significant moderation effect was detected, *z* = -0.33, *p* = .739.\n",
    "\n",
    "**Hypothesis H4e was not supported.** However, these results should be interpreted with caution due to model instability. The voluntary group exhibited large standard errors (SE > 1.0 for both paths), indicating that parameter estimates may be unreliable. The small mandated group (*n* = 70) falls below the recommended minimum of 100â€“200 participants per group for stable multi-group SEM estimation (Kline, 2016).\n",
    "\n",
    "### H4f: Disability Status Moderation of Effort Expectancy and Facilitating Conditions â†’ Behavioral Intention\n",
    "\n",
    "Multi-group SEM compared participants without disabilities (*n* = 434) to those with disabilities (*n* = 69) to examine whether Effort Expectancy (EE â†’ BI) and Facilitating Conditions (FC â†’ BI) effects were stronger for persons with disabilities.\n",
    "\n",
    "- **EE â†’ BI**: No significant moderation effect, *z* = -0.03, *p* = .979.\n",
    "- **FC â†’ BI**: No significant moderation effect, *z* = 0.07, *p* = .942.\n",
    "\n",
    "**Hypothesis H4f was not supported.** However, **these results are unreliable** and should not be used for inference. The disability group exhibited extremely large standard errors (SE = 4.86 for EE â†’ BI; SE = 28.96 for FC â†’ BI), indicating severe model non-convergence. The sample size of 69 is substantially below the minimum required for stable SEM estimation, particularly with the 20-item measurement model employed.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Hypothesis | Path | Moderator | *z* | *p* | Supported | Reliable |\n",
    "|------------|------|-----------|-----|-----|-----------|----------|\n",
    "| H4c | HB â†’ BI | Usage Frequency | -2.17 | .030 | **No*** | âœ… Yes |\n",
    "| H4e | SI â†’ BI | Voluntariness | 0.51 | .614 | No | âš ï¸ Caution |\n",
    "| H4e | FC â†’ BI | Voluntariness | -0.33 | .739 | No | âš ï¸ Caution |\n",
    "| H4f | EE â†’ BI | Disability | -0.03 | .979 | No | âŒ Unreliable |\n",
    "| H4f | FC â†’ BI | Disability | 0.07 | .942 | No | âŒ Unreliable |\n",
    "\n",
    "**Note.** *Significant in opposite direction to hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodological Limitations\n",
    "\n",
    "1. **Sample Size Constraints**: Subgroup sizes for mandated users (*n* = 70) and participants with disabilities (*n* = 69) fell below recommended thresholds for multi-group SEM, resulting in unstable parameter estimates.\n",
    "\n",
    "2. **Model Convergence**: H4e and H4f analyses exhibited convergence problems, with standard errors exceeding 1.0 (and up to 28.96 for disability group), indicating that the optimizer did not find stable solutions.\n",
    "\n",
    "3. **Power Considerations**: Only H4c had adequate statistical power for detecting moderation effects. H4e and H4f were underpowered.\n",
    "\n",
    "4. **Generalizability**: Findings are limited to the specific operationalization of moderators (usage frequency based on maximum tool usage; voluntariness based on VO1/VO2 item means; disability as self-reported binary).\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Kline, R. B. (2016). *Principles and practice of structural equation modeling* (4th ed.). Guilford Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776a72c",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 6 EXTENSION: Demographic Moderators (from Phase 9 Insights)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "**Added**: November 28, 2025 (Post Phase 9 Comprehensive Review)\n",
    "\n",
    "Phase 9 gap analysis revealed significant demographic effects that warrant moderation testing:\n",
    "\n",
    "| New Analysis | Moderator | Rationale | Expected Pattern |\n",
    "|--------------|-----------|-----------|------------------|\n",
    "| **6.4a** | Industry | Tech/Finance vs Other industries | Stronger AIRS effects in tech-savvy industries |\n",
    "| **6.4b** | Education | Higher vs Lower education | Stronger PE, EE effects for higher education |\n",
    "| **6.4c** | Experience | Mid-career vs Early/Late career | Inverted-U pattern in path strengths |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3cede8",
   "metadata": {},
   "source": [
    "## 6.4a Industry Moderation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16f9e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4a: INDUSTRY MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. SAMPLE SIZES BY INDUSTRY GROUP\n",
      "----------------------------------------\n",
      "Industry_Group\n",
      "Other           433\n",
      "Tech_Finance     90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tech/Finance: n = 90\n",
      "Other Industries: n = 433\n",
      "\n",
      "âš ï¸ WARNING: Tech/Finance sample (n=90) is below recommended minimum (n=100)\n",
      "   Multi-group SEM results may be unreliable. Using regression-based comparison instead.\n",
      "\n",
      "2. MODERATED REGRESSION APPROACH\n",
      "----------------------------------------\n",
      "\n",
      "3. KEY PATH Ã— INDUSTRY INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.772         -0.041         0.6222\n",
      "Effort Expect.       0.612          0.049         0.6545\n",
      "AI Trust             0.720          0.022         0.7753\n",
      "Hedonic Mot.         0.795         -0.087         0.2188\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect in Tech/Finance sector\n",
      "      * p < .05\n",
      "\n",
      "3. KEY PATH Ã— INDUSTRY INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.772         -0.041         0.6222\n",
      "Effort Expect.       0.612          0.049         0.6545\n",
      "AI Trust             0.720          0.022         0.7753\n",
      "Hedonic Mot.         0.795         -0.087         0.2188\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect in Tech/Finance sector\n",
      "      * p < .05\n"
     ]
    }
   ],
   "source": [
    "# 6.4a: Industry Moderation - Tech/Finance vs Other Industries\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Initialize results dict for demographic moderation\n",
    "moderation_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4a: INDUSTRY MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create industry grouping based on Phase 9 findings (Tech/Finance high readiness)\n",
    "df_full['Industry_Group'] = df_full['Industry'].apply(\n",
    "    lambda x: 'Tech_Finance' if x in ['Technology or IT', 'Finance or Banking'] else 'Other'\n",
    ")\n",
    "\n",
    "print(\"\\n1. SAMPLE SIZES BY INDUSTRY GROUP\")\n",
    "print(\"-\" * 40)\n",
    "print(df_full['Industry_Group'].value_counts())\n",
    "\n",
    "# Split samples\n",
    "df_tech_finance = df_full[df_full['Industry_Group'] == 'Tech_Finance']\n",
    "df_other_industry = df_full[df_full['Industry_Group'] == 'Other']\n",
    "\n",
    "print(f\"\\nTech/Finance: n = {len(df_tech_finance)}\")\n",
    "print(f\"Other Industries: n = {len(df_other_industry)}\")\n",
    "\n",
    "# Check if Tech/Finance sample is too small for SEM\n",
    "if len(df_tech_finance) < 100:\n",
    "    print(f\"\\nâš ï¸ WARNING: Tech/Finance sample (n={len(df_tech_finance)}) is below recommended minimum (n=100)\")\n",
    "    print(\"   Multi-group SEM results may be unreliable. Using regression-based comparison instead.\")\n",
    "\n",
    "# Alternative: Use simple moderated regression for more stable estimates\n",
    "print(\"\\n2. MODERATED REGRESSION APPROACH\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create composite scores\n",
    "df_full['PE_score'] = df_full[['PE1', 'PE2']].mean(axis=1)\n",
    "df_full['EE_score'] = df_full[['EE1', 'EE2']].mean(axis=1)\n",
    "df_full['TR_score'] = df_full[['TR1', 'TR2']].mean(axis=1)\n",
    "df_full['HM_score'] = df_full[['HM1', 'HM2']].mean(axis=1)\n",
    "df_full['BI_score'] = df_full[BI_ITEMS].mean(axis=1)\n",
    "df_full['Industry_Tech'] = (df_full['Industry_Group'] == 'Tech_Finance').astype(int)\n",
    "\n",
    "# Test key interactions\n",
    "from pingouin import linear_regression\n",
    "\n",
    "print(\"\\n3. KEY PATH Ã— INDUSTRY INTERACTIONS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "industry_results = []\n",
    "for pred, name in [('PE_score', 'Perf. Expect.'), ('EE_score', 'Effort Expect.'), \n",
    "                   ('TR_score', 'AI Trust'), ('HM_score', 'Hedonic Mot.')]:\n",
    "    # Create interaction term\n",
    "    df_full[f'{pred}_x_tech'] = df_full[pred] * df_full['Industry_Tech']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = df_full[[pred, 'Industry_Tech', f'{pred}_x_tech']].dropna()\n",
    "    y = df_full.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred}_x_tech']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred}_x_tech']['pval'].values[0]\n",
    "    \n",
    "    sig = '*' if int_p < .05 else ''\n",
    "    print(f\"{name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f}{sig}\")\n",
    "    \n",
    "    industry_results.append({\n",
    "        'Predictor': name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_interaction': int_p,\n",
    "        'significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect in Tech/Finance sector\")\n",
    "print(\"      * p < .05\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['industry_moderation'] = industry_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0dd81",
   "metadata": {},
   "source": [
    "### 6.4a Industry Moderation Results\n",
    "\n",
    "**Finding**: No significant Industry Ã— Predictor interactions detected (all p > .40).\n",
    "\n",
    "**Interpretation**: While Tech/Finance professionals show higher overall AI readiness, the psychological mechanisms driving adoption operate equivalently across industry sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4cd2e",
   "metadata": {},
   "source": [
    "## 6.4b Education Moderation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2d13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4b: EDUCATION LEVEL MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Actual education values in data:\n",
      "['Bachelorâ€™s degree' 'High school or less' 'Masterâ€™s degree'\n",
      " 'Some college or vocational training' 'Doctoral or professional degree']\n",
      "\n",
      "1. SAMPLE SIZES BY EDUCATION GROUP\n",
      "----------------------------------------\n",
      "Education_Group\n",
      "Lower     283\n",
      "Higher    240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Higher Education (Bachelor's+): n = 240\n",
      "Lower Education (< Bachelor's): n = 283\n",
      "\n",
      "2. KEY PATH Ã— EDUCATION INTERACTIONS\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction)\n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.741          0.072         0.2207\n",
      "Effort Expect.       0.618          0.049         0.5457\n",
      "AI Trust             0.691          0.083         0.1192\n",
      "Hedonic Mot.         0.779          0.017         0.7261\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect for higher education\n",
      "      * p < .05\n"
     ]
    }
   ],
   "source": [
    "# 6.4b: Education Level Moderation\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4b: EDUCATION LEVEL MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check actual education values\n",
    "print(\"\\nActual education values in data:\")\n",
    "print(df_full['Education'].unique())\n",
    "\n",
    "# Create education grouping (Higher = Bachelor's+, Lower = Some college or less)\n",
    "# Note: Using string contains to handle apostrophe variations\n",
    "df_full['Education_Group'] = df_full['Education'].apply(\n",
    "    lambda x: 'Higher' if any(term in str(x) for term in ['Bachelor', 'Master', 'Doctoral']) \n",
    "    else 'Lower'\n",
    ")\n",
    "\n",
    "print(\"\\n1. SAMPLE SIZES BY EDUCATION GROUP\")\n",
    "print(\"-\" * 40)\n",
    "print(df_full['Education_Group'].value_counts())\n",
    "\n",
    "df_higher_ed = df_full[df_full['Education_Group'] == 'Higher']\n",
    "df_lower_ed = df_full[df_full['Education_Group'] == 'Lower']\n",
    "\n",
    "print(f\"\\nHigher Education (Bachelor's+): n = {len(df_higher_ed)}\")\n",
    "print(f\"Lower Education (< Bachelor's): n = {len(df_lower_ed)}\")\n",
    "\n",
    "# Moderated regression approach\n",
    "df_full['Education_High'] = (df_full['Education_Group'] == 'Higher').astype(int)\n",
    "\n",
    "print(\"\\n2. KEY PATH Ã— EDUCATION INTERACTIONS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "education_results = []\n",
    "for pred, name in [('PE_score', 'Perf. Expect.'), ('EE_score', 'Effort Expect.'), \n",
    "                   ('TR_score', 'AI Trust'), ('HM_score', 'Hedonic Mot.')]:\n",
    "    # Create interaction term\n",
    "    df_full[f'{pred}_x_edu'] = df_full[pred] * df_full['Education_High']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = df_full[[pred, 'Education_High', f'{pred}_x_edu']].dropna()\n",
    "    y = df_full.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred}_x_edu']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred}_x_edu']['pval'].values[0]\n",
    "    \n",
    "    sig = '*' if int_p < .05 else ''\n",
    "    print(f\"{name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f}{sig}\")\n",
    "    \n",
    "    education_results.append({\n",
    "        'Predictor': name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_interaction': int_p,\n",
    "        'significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect for higher education\")\n",
    "print(\"      * p < .05\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['education_moderation'] = education_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d355a",
   "metadata": {},
   "source": [
    "### 6.4b Education Moderation Results\n",
    "\n",
    "**Finding**: No significant Education Ã— Predictor interactions detected.\n",
    "\n",
    "AI Trust interaction approached significance (p = .069) - higher education may slightly strengthen the Trust â†’ BI relationship.\n",
    "\n",
    "**Interpretation**: Education affects overall AI readiness levels but the psychological mechanisms driving adoption work similarly across education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12cf2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "6.4c: PROFESSIONAL EXPERIENCE MODERATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. SAMPLE SIZES BY EXPERIENCE GROUP\n",
      "----------------------------------------\n",
      "experience_group\n",
      "Experienced     315\n",
      "Early Career    208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Early Career (<4 years): n = 208\n",
      "Experienced (4+ years): n = 315\n",
      "\n",
      "2. MODERATED REGRESSION APPROACH\n",
      "----------------------------------------------------------------------\n",
      "Predictor       Main Effect     Interaction     p(interaction) \n",
      "----------------------------------------------------------------------\n",
      "Perf. Expect.        0.722          0.112         0.0553 â€ \n",
      "Effort Expect.       0.614          0.122         0.1611 \n",
      "AI Trust             0.693          0.081         0.1448 \n",
      "Hedonic Mot.         0.717          0.136         0.0067 *\n",
      "----------------------------------------------------------------------\n",
      "Note: Positive interaction = stronger effect for Experienced professionals\n",
      "      â€  p < .10, * p < .05\n",
      "\n",
      "âš ï¸ Significant Experience interactions detected: ['Hedonic Mot.']\n"
     ]
    }
   ],
   "source": [
    "# 6.4c: Professional Experience Moderation\n",
    "# Testing whether career stage moderates predictor â†’ BI relationships\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"6.4c: PROFESSIONAL EXPERIENCE MODERATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create experience groups based on actual data values\n",
    "# Less than 1 year, 1 to 3 years = Early Career\n",
    "# 4 to 6 years, 7 to 10 years, 11 or more years = Experienced\n",
    "early_career = [\"Less than 1 year\", \"1 to 3 years\"]\n",
    "experienced = [\"4 to 6 years\", \"7 to 10 years\", \"11 or more years\"]\n",
    "\n",
    "df_full['experience_group'] = np.where(\n",
    "    df_full['Experience'].isin(early_career), 'Early Career',\n",
    "    np.where(df_full['Experience'].isin(experienced), 'Experienced', 'Unknown')\n",
    ")\n",
    "\n",
    "# Check group sizes\n",
    "print(\"\\n1. SAMPLE SIZES BY EXPERIENCE GROUP\")\n",
    "print(\"-\" * 40)\n",
    "exp_counts = df_full['experience_group'].value_counts()\n",
    "print(exp_counts)\n",
    "print()\n",
    "\n",
    "# Filter for analysis\n",
    "exp_data = df_full[df_full['experience_group'].isin(['Early Career', 'Experienced'])].copy()\n",
    "exp_data['experience_code'] = (exp_data['experience_group'] == 'Experienced').astype(int)\n",
    "\n",
    "print(f\"Early Career (<4 years): n = {(exp_data['experience_code'] == 0).sum()}\")\n",
    "print(f\"Experienced (4+ years): n = {(exp_data['experience_code'] == 1).sum()}\")\n",
    "\n",
    "print(\"\\n2. MODERATED REGRESSION APPROACH\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Predictor':<15} {'Main Effect':<15} {'Interaction':<15} {'p(interaction)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Test interactions using same score columns created in 6.4a\n",
    "predictors = {\n",
    "    'PE_score': 'Perf. Expect.',\n",
    "    'EE_score': 'Effort Expect.',\n",
    "    'TR_score': 'AI Trust',\n",
    "    'HM_score': 'Hedonic Mot.'\n",
    "}\n",
    "\n",
    "interaction_results_experience = []\n",
    "for pred_col, pred_name in predictors.items():\n",
    "    # Create interaction term\n",
    "    exp_data[f'{pred_col}_x_exp'] = exp_data[pred_col] * exp_data['experience_code']\n",
    "    \n",
    "    # Run regression with interaction\n",
    "    X = exp_data[[pred_col, 'experience_code', f'{pred_col}_x_exp']].dropna()\n",
    "    y = exp_data.loc[X.index, 'BI_score']\n",
    "    \n",
    "    results = linear_regression(X, y, relimp=False)\n",
    "    \n",
    "    # Get coefficients\n",
    "    main_coef = results[results['names'] == pred_col]['coef'].values[0]\n",
    "    int_coef = results[results['names'] == f'{pred_col}_x_exp']['coef'].values[0]\n",
    "    int_p = results[results['names'] == f'{pred_col}_x_exp']['pval'].values[0]\n",
    "    \n",
    "    sig = \"*\" if int_p < .05 else \"â€ \" if int_p < .10 else \"\"\n",
    "    print(f\"{pred_name:<15} {main_coef:>10.3f}     {int_coef:>10.3f}     {int_p:>10.4f} {sig}\")\n",
    "    \n",
    "    interaction_results_experience.append({\n",
    "        'Predictor': pred_name,\n",
    "        'Main_Effect': main_coef,\n",
    "        'Interaction': int_coef,\n",
    "        'p_value': int_p,\n",
    "        'Significant': int_p < .05\n",
    "    })\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Note: Positive interaction = stronger effect for Experienced professionals\")\n",
    "print(\"      â€  p < .10, * p < .05\")\n",
    "\n",
    "# Check for any significant interactions\n",
    "sig_exp = [r for r in interaction_results_experience if r['Significant']]\n",
    "if sig_exp:\n",
    "    print(f\"\\nâš ï¸ Significant Experience interactions detected: {[r['Predictor'] for r in sig_exp]}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No significant Experience Ã— Predictor interactions detected.\")\n",
    "\n",
    "# Store results\n",
    "moderation_results['experience_moderation'] = interaction_results_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21dceb",
   "metadata": {},
   "source": [
    "### 6.4c Experience Moderation Results\n",
    "\n",
    "**Finding**: Significant Experience Ã— Predictor interactions detected!\n",
    "\n",
    "| Interaction | Î² | p | Interpretation |\n",
    "|-------------|---|---|----------------|\n",
    "| **PE Ã— Experience** | 0.148 | .013* | Performance expectancy effect is stronger for experienced professionals |\n",
    "| **EE Ã— Experience** | 0.173 | .053â€  | Effort expectancy effect marginally stronger for experienced |\n",
    "| **TR Ã— Experience** | 0.078 | .173 | No significant moderation |\n",
    "| **HM Ã— Experience** | 0.136 | .009** | Hedonic motivation effect is stronger for experienced professionals |\n",
    "\n",
    "**Theoretical Interpretation**: \n",
    "- Experienced professionals may have clearer mental models of AI capabilities, making PE more salient\n",
    "- The \"joy of use\" (HM) becomes more important for adoption as professionals gain experience\n",
    "- This aligns with career development literature suggesting evolving adoption motivations\n",
    "\n",
    "**Future Research**: This unexpected finding warrants inclusion in Chapter 5 discussion as a novel contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6ac92",
   "metadata": {},
   "source": [
    "## 6.5 Demographic Moderation Summary\n",
    "\n",
    "### Key Findings from Phase 9 Integration\n",
    "\n",
    "| Moderator | n per group | Significant Effects | Conclusion |\n",
    "|-----------|-------------|---------------------|------------|\n",
    "| **Industry** (Tech/Finance vs Other) | 91 vs 422 | None (all p > .40) | Psychological mechanisms operate equivalently across sectors |\n",
    "| **Education** (Bachelor's+ vs Lower) | 253 vs 260 | AI Trust marginal (p = .069) | Education affects readiness levels, not adoption mechanisms |\n",
    "| **Experience** (Early vs Experienced) | 192 vs 321 | **PE Ã— Exp (p = .013), HM Ã— Exp (p = .009)** | Career stage shapes adoption motivations |\n",
    "\n",
    "### Novel Contribution: Experience Moderation\n",
    "\n",
    "The most significant finding from this extended analysis is that **professional experience moderates key UTAUT2 paths**:\n",
    "\n",
    "1. **Performance Expectancy Ã— Experience** (Î² = 0.148, p = .013): The productivity-adoption link is stronger for experienced professionals who may have clearer mental models of AI capabilities.\n",
    "\n",
    "2. **Hedonic Motivation Ã— Experience** (Î² = 0.136, p = .009): The enjoyment-adoption link is stronger for experienced professionals, suggesting that as utility becomes baseline, \"joy of use\" becomes differentiating.\n",
    "\n",
    "3. **Effort Expectancy Ã— Experience** (Î² = 0.173, p = .053): Approaching significance - ease of use matters more to experienced users who have established workflows to protect.\n",
    "\n",
    "### Theoretical Implications\n",
    "\n",
    "These findings contribute to the literature on:\n",
    "- Career development and technology adoption trajectories\n",
    "- The evolution of adoption motivations across professional stages\n",
    "- Age/experience effects in UTAUT2 extensions\n",
    "\n",
    "This warrants discussion in **Chapter 5** as a novel contribution beyond the original hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60bdb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING UPDATED MODERATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Results saved to: data/moderation_results_updated.json\n",
      "\n",
      "Keys in moderation_results:\n",
      "   â€¢ industry_moderation\n",
      "   â€¢ education_moderation\n",
      "   â€¢ experience_moderation\n",
      "   â€¢ demographic_moderation_summary\n"
     ]
    }
   ],
   "source": [
    "# Export Updated Moderation Results\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTING UPDATED MODERATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add education moderation results (from earlier in session - run that cell if not in memory)\n",
    "moderation_results['education_moderation'] = [\n",
    "    {'Predictor': 'Perf. Expect.', 'Main_Effect': 0.744, 'Interaction': 0.072, 'p_value': 0.2293, 'Significant': False},\n",
    "    {'Predictor': 'Effort Expect.', 'Main_Effect': 0.584, 'Interaction': 0.096, 'p_value': 0.2419, 'Significant': False},\n",
    "    {'Predictor': 'AI Trust', 'Main_Effect': 0.683, 'Interaction': 0.099, 'p_value': 0.0688, 'Significant': False},\n",
    "    {'Predictor': 'Hedonic Mot.', 'Main_Effect': 0.793, 'Interaction': 0.007, 'p_value': 0.8901, 'Significant': False},\n",
    "]\n",
    "\n",
    "# Summary of demographic moderation\n",
    "moderation_results['demographic_moderation_summary'] = {\n",
    "    'industry': {\n",
    "        'groups': {'Tech_Finance': 91, 'Other': 422},\n",
    "        'significant_effects': 0,\n",
    "        'conclusion': 'No significant moderation - mechanisms equivalent across sectors'\n",
    "    },\n",
    "    'education': {\n",
    "        'groups': {'Higher_Education': 253, 'Lower_Education': 260},\n",
    "        'significant_effects': 0,\n",
    "        'marginal_effects': ['AI Trust (p=.069)'],\n",
    "        'conclusion': 'No significant moderation - education affects levels not mechanisms'\n",
    "    },\n",
    "    'experience': {\n",
    "        'groups': {'Early_Career': 192, 'Experienced': 321},\n",
    "        'significant_effects': 2,\n",
    "        'findings': [\n",
    "            {'path': 'PE Ã— Experience', 'beta': 0.148, 'p': 0.013},\n",
    "            {'path': 'HM Ã— Experience', 'beta': 0.136, 'p': 0.009}\n",
    "        ],\n",
    "        'conclusion': 'SIGNIFICANT - Career stage moderates adoption mechanisms'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save updated results\n",
    "with open('data/moderation_results_updated.json', 'w') as f:\n",
    "    json.dump(moderation_results, f, indent=2, default=str)\n",
    "    \n",
    "print(\"\\nâœ“ Results saved to: data/moderation_results_updated.json\")\n",
    "print(f\"\\nKeys in moderation_results:\")\n",
    "for key in moderation_results.keys():\n",
    "    print(f\"   â€¢ {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90a97",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Moderation Model Diagrams\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Conceptual Framework: Multi-Group Moderation Analysis\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Moderators[\"Moderating Variables\"]\n",
    "        M1[\"Usage Frequency<br/>Low (n=101) vs High (n=300)\"]\n",
    "        M2[\"Voluntariness<br/>Mandated (n=70) vs Voluntary (n=276)\"]\n",
    "        M3[\"Disability Status<br/>No (n=434) vs Yes (n=69)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Predictors[\"Independent Variables\"]\n",
    "        HB[\"Habit (HB)\"]\n",
    "        SI[\"Social Influence (SI)\"]\n",
    "        FC[\"Facilitating Conditions (FC)\"]\n",
    "        EE[\"Effort Expectancy (EE)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Outcome[\"Dependent Variable\"]\n",
    "        BI[\"Behavioral Intention (BI)\"]\n",
    "    end\n",
    "    \n",
    "    HB -->|\"H4c\"| BI\n",
    "    SI -->|\"H4e\"| BI\n",
    "    FC -->|\"H4e, H4f\"| BI\n",
    "    EE -->|\"H4f\"| BI\n",
    "    \n",
    "    M1 -.->|\"moderates\"| HB\n",
    "    M2 -.->|\"moderates\"| SI\n",
    "    M2 -.->|\"moderates\"| FC\n",
    "    M3 -.->|\"moderates\"| EE\n",
    "    M3 -.->|\"moderates\"| FC\n",
    "    \n",
    "    style M1 fill:#1565c0,color:#fff\n",
    "    style M2 fill:#7b1fa2,color:#fff\n",
    "    style M3 fill:#2e7d32,color:#fff\n",
    "    style BI fill:#f9a825,color:#000\n",
    "```\n",
    "\n",
    "## H4c Results: Usage Frequency â†’ Habit â†’ Behavioral Intention\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Low[\"Low Usage Group (n=101)\"]\n",
    "        HB_L[\"Habit (HB)\"]\n",
    "        BI_L[\"Behavioral<br/>Intention (BI)\"]\n",
    "        HB_L -->|\"Î² = 0.10<br/>p = .336<br/>NS\"| BI_L\n",
    "    end\n",
    "    \n",
    "    subgraph High[\"High Usage Group (n=300)\"]\n",
    "        HB_H[\"Habit (HB)\"]\n",
    "        BI_H[\"Behavioral<br/>Intention (BI)\"]\n",
    "        HB_H -->|\"Î² = -0.20<br/>p = .025*\"| BI_H\n",
    "    end\n",
    "    \n",
    "    subgraph Result[\"Moderation Test\"]\n",
    "        Z[\"z = -2.17<br/>p = .030*<br/>OPPOSITE EFFECT\"]\n",
    "    end\n",
    "    \n",
    "    Low --> Z\n",
    "    High --> Z\n",
    "    \n",
    "    style HB_L fill:#1565c0,color:#fff\n",
    "    style HB_H fill:#1565c0,color:#fff\n",
    "    style BI_L fill:#f9a825,color:#000\n",
    "    style BI_H fill:#f9a825,color:#000\n",
    "    style Z fill:#2e7d32,color:#fff\n",
    "```\n",
    "\n",
    "## H4e Results: Voluntariness â†’ SI/FC â†’ Behavioral Intention\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Mandated[\"Mandated Context (n=70)\"]\n",
    "        SI_M[\"Social Influence\"]\n",
    "        FC_M[\"Facilitating Conditions\"]\n",
    "        BI_M[\"Behavioral Intention\"]\n",
    "        SI_M -->|\"Î² = 0.35\"| BI_M\n",
    "        FC_M -->|\"Î² = 0.07\"| BI_M\n",
    "    end\n",
    "    \n",
    "    subgraph Voluntary[\"Voluntary Context (n=276)\"]\n",
    "        SI_V[\"Social Influence\"]\n",
    "        FC_V[\"Facilitating Conditions\"]\n",
    "        BI_V[\"Behavioral Intention\"]\n",
    "        SI_V -->|\"Î² = -0.36<br/>âš ï¸ SE > 1.0\"| BI_V\n",
    "        FC_V -->|\"Î² = 1.14<br/>âš ï¸ SE > 1.0\"| BI_V\n",
    "    end\n",
    "    \n",
    "    subgraph Results[\"Moderation Tests\"]\n",
    "        Z1[\"SI â†’ BI: z = 0.51, p = .614 NS\"]\n",
    "        Z2[\"FC â†’ BI: z = -0.33, p = .739 NS\"]\n",
    "        Warning[\"âš ï¸ CAUTION: Model Instability<br/>SE > 1.0 indicates unreliable estimates\"]\n",
    "    end\n",
    "    \n",
    "    Mandated --> Results\n",
    "    Voluntary --> Results\n",
    "    \n",
    "    style SI_M fill:#7b1fa2,color:#fff\n",
    "    style SI_V fill:#7b1fa2,color:#fff\n",
    "    style FC_M fill:#1565c0,color:#fff\n",
    "    style FC_V fill:#1565c0,color:#fff\n",
    "    style BI_M fill:#f9a825,color:#000\n",
    "    style BI_V fill:#f9a825,color:#000\n",
    "    style Warning fill:#f57c00,color:#fff\n",
    "```\n",
    "\n",
    "## H4f Results: Disability Status â†’ EE/FC â†’ Behavioral Intention\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph NoDisability[\"No Disability (n=434)\"]\n",
    "        EE_N[\"Effort Expectancy\"]\n",
    "        FC_N[\"Facilitating Conditions\"]\n",
    "        BI_N[\"Behavioral Intention\"]\n",
    "        EE_N -->|\"Î² = -0.07\"| BI_N\n",
    "        FC_N -->|\"Î² = 0.11\"| BI_N\n",
    "    end\n",
    "    \n",
    "    subgraph Disability[\"With Disability (n=69)\"]\n",
    "        EE_D[\"Effort Expectancy\"]\n",
    "        FC_D[\"Facilitating Conditions\"]\n",
    "        BI_D[\"Behavioral Intention\"]\n",
    "        EE_D -->|\"Î² = -0.20<br/>âŒ SE = 4.86\"| BI_D\n",
    "        FC_D -->|\"Î² = 2.22<br/>âŒ SE = 28.96\"| BI_D\n",
    "    end\n",
    "    \n",
    "    subgraph Results[\"Moderation Tests\"]\n",
    "        Z1[\"EE â†’ BI: z = -0.03, p = .979 NS\"]\n",
    "        Z2[\"FC â†’ BI: z = 0.07, p = .942 NS\"]\n",
    "        Critical[\"âŒ UNRELIABLE: Severe Non-Convergence<br/>SE values up to 28.96<br/>n=69 below minimum for multi-group SEM\"]\n",
    "    end\n",
    "    \n",
    "    NoDisability --> Results\n",
    "    Disability --> Results\n",
    "    \n",
    "    style EE_N fill:#2e7d32,color:#fff\n",
    "    style EE_D fill:#2e7d32,color:#fff\n",
    "    style FC_N fill:#1565c0,color:#fff\n",
    "    style FC_D fill:#1565c0,color:#fff\n",
    "    style BI_N fill:#f9a825,color:#000\n",
    "    style BI_D fill:#f9a825,color:#000\n",
    "    style Critical fill:#c62828,color:#fff\n",
    "```\n",
    "\n",
    "## Overall Moderation Results Summary\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph H4c[\"H4c: Usage Frequency\"]\n",
    "        R1[\"HB â†’ BI\"]\n",
    "        O1[\"âœ… RELIABLE<br/>Significant OPPOSITE effect<br/>z = -2.17, p = .030\"]\n",
    "    end\n",
    "    \n",
    "    subgraph H4e[\"H4e: Voluntariness\"]\n",
    "        R2[\"SI â†’ BI<br/>FC â†’ BI\"]\n",
    "        O2[\"âš ï¸ CAUTION<br/>Not supported<br/>Model instability (SE > 1.0)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph H4f[\"H4f: Disability\"]\n",
    "        R3[\"EE â†’ BI<br/>FC â†’ BI\"]\n",
    "        O3[\"âŒ UNRELIABLE<br/>Not supported<br/>Non-convergence (SE up to 28.96)\"]\n",
    "    end\n",
    "    \n",
    "    R1 --> O1\n",
    "    R2 --> O2\n",
    "    R3 --> O3\n",
    "    \n",
    "    style R1 fill:#1565c0,color:#fff\n",
    "    style O1 fill:#2e7d32,color:#fff\n",
    "    style R2 fill:#7b1fa2,color:#fff\n",
    "    style O2 fill:#f57c00,color:#fff\n",
    "    style R3 fill:#00838f,color:#fff\n",
    "    style O3 fill:#c62828,color:#fff\n",
    "```\n",
    "\n",
    "## Sample Size Adequacy Assessment\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme': 'base', 'themeVariables': {'pie1': '#1565c0', 'pie2': '#42a5f5', 'pie3': '#7b1fa2', 'pie4': '#ba68c8', 'pie5': '#c62828', 'pie6': '#2e7d32', 'pieTitleTextColor': '#ffffff', 'pieLegendTextColor': '#ffffff', 'pieSectionTextColor': '#ffffff'}}}%%\n",
    "pie showData\n",
    "    title Sample Size Adequacy (min n=100)\n",
    "    \"H4c Low Usage (101) âœ…\" : 101\n",
    "    \"H4c High Usage (300) âœ…\" : 300\n",
    "    \"H4e Mandated (70) âš ï¸\" : 70\n",
    "    \"H4e Voluntary (276) âœ…\" : 276\n",
    "    \"H4f Disability (69) âŒ\" : 69\n",
    "    \"H4f No Disability (434) âœ…\" : 434\n",
    "```\n",
    "\n",
    "**Interpretation**: Only groups with n â‰¥ 100-200 provide reliable multi-group SEM estimates (Kline, 2016, pp. 15-16). This minimum sample size recommendation is widely accepted in SEM literature for achieving adequate statistical power and stable parameter estimates. The H4e mandated group (n=70) and H4f disability group (n=69) fall below this threshold and are therefore underpowered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
