Literature Review:
Evolution of Technology Adoption Frameworks and the
Emergence of AI-Specific Readiness Models

The velocity that enterprises turn toward artificial intelligence has outpaced the explanatory comfort zone of classic technology adoption theories. Foundational models such as the Theory of Planned Behavior, the Technology Acceptance Model, and Diffusion of Innovations clarified why people intend to use technologies and how innovations spread, but they were not designed for systems that learn, operate probabilistically, and affect accountability at organizational scale (Ajzen, 1991; Davis, 1989; Rogers, 2003).
A major consolidation followed with the Unified Theory of Acceptance and Use of Technology and its consumer extension. These models integrate performance and effort expectancies with social influence, facilitating conditions, and later hedonic motivation, price value, and habit. They explain substantial variance in intention and use across many contexts and therefore provide a durable baseline for acceptance research (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, 2012).
Artificial intelligence stresses this baseline in three ways. First, many AI systems function as partially opaque decision aids, which raises the salience of interpretability and explainability for users and for stakeholders who must justify decisions. Second, AI introduces distinct ethical exposures related to bias, privacy, and accountability that shape willingness to rely on outputs. Third, AI’s autonomy and probabilistic behavior shift attention from simple usability to governance and trust. Recent syntheses and domain studies therefore argue that adoption models in enterprise settings must incorporate constructs such as trust in AI, perceived explainability, and perceived ethical risk to remain predictive (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Dwivedi et al., 2021; Shin, 2021).
Empirical reviews of trust in AI underscore its role as a gateway condition for sustained use. Evidence indicates that explainability mechanisms can strengthen trust by making model reasoning sufficiently clear to justify action, while governance practices reduce perceived ethical risk that would otherwise depress intention to use (Langer, König, & Papathanasiou, 2023; Stevens & Stetson, 2023).
Given this trajectory, the present review proceeds in two moves. First, it traces the evolution from general behavioral models to integrated acceptance frameworks, clarifying what each explains well and where they fall short for AI. Second, it synthesizes current evidence to motivate an enterprise-ready extension of UTAUT that retains the validated core while adding AI-specific enablers and inhibitors. The objective is to offer a measurement blueprint that supports valid assessment, responsible deployment, and practical change management in organizations.
The objective of this literature review is to identify the theoretical foundations to develop the Artificial Intelligence Readiness Scale (AIRS): a validated, enterprise-focused extension or contextualization of the UTAUT/UTAUT2 tradition. Specifically, we (a) define four AI-specific constructs that operate as enablers (Trust in AI, Perceived Explainability) and inhibitors (Perceived Ethical Risk, AI-Related Anxiety); (b) generate job-anchored item pools aligned with established UTAUT2 constructs; (c) evaluate reliability and construct validity via exploratory and confirmatory factor analyses; (d) test structural relations, including mediation (explainability → trust → intention) and moderation (ethical risk and anxiety attenuating core paths); and (e) examine measurement invariance across roles, exposure, and voluntariness. The product is AIRS: a practical instrument with scoring guidance that organizations can use to assess readiness, inform governance and enablement, and track change over time.
Foundations of Technology Adoption
Ajzen’s Theory of Planned Behavior explains behavior through intentions shaped by attitudes, subjective norms, and perceived behavioral control. Its breadth and parsimony made it adaptable to early information systems contexts, yet it remains technology-agnostic and does not directly model perceptions unique to digital systems such as perceived usefulness or ease of use (Ajzen, 1991).
Davis’s Technology Acceptance Model addressed that gap by introducing perceived usefulness and perceived ease of use as proximal predictors of behavioral intention and use. TAM’s strengths are its clarity and consistent empirical support across many information systems domains. Its limits appear when technologies carry salient ethical, organizational, or epistemic properties, where instrumental beliefs alone do not capture adoption dynamics (Davis, 1989).
Rogers’s Diffusion of Innovations complements these individual-level lenses by describing how innovations spread over time through social systems. Attributes such as relative advantage, compatibility, complexity, trialability, and observability help explain organizational uptake and social influence. However, diffusion theory is less diagnostic for the psychological mechanisms behind an individual’s decision to rely on complex, partially opaque systems such as modern AI (Rogers, 2003).
Consolidation: UTAUT and UTAUT2
The Unified Theory of Acceptance and Use of Technology synthesized core predictors from prior models into performance expectancy, effort expectancy, social influence, and facilitating conditions, moderated by factors such as age, gender, experience, and voluntariness. UTAUT provides a durable baseline for explaining intention and use across organizational settings (Venkatesh, Morris, Davis, & Davis, 2003).
UTAUT2 extended the framework to consumer contexts by adding hedonic motivation, price value, and habit, improving explanatory reach where enjoyment, perceived cost, and routine matter. Together, UTAUT and UTAUT2 have become the field’s standard point of departure for theorizing technology acceptance (Venkatesh, Thong, & Xu, 2012).
Why AI Stresses the Baseline
Artificial intelligence shifts the acceptance problem in three ways. First, many AI systems are partially opaque and probabilistic, making interpretability and explainability central to justified reliance. Second, AI raises distinctive ethical exposures around bias, privacy, and accountability that shape willingness to depend on outputs. Third, autonomy and continuous learning move adoption beyond usability toward governance and trust. Contemporary reviews and domain studies converge on the need to incorporate constructs such as trust in AI, perceived explainability, and perceived ethical risk to retain predictive validity in enterprise settings (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Dwivedi et al., 2021; Shin, 2021).
Empirical syntheses position trust as a gateway condition for meaningful and sustained use. Evidence also indicates that explainability mechanisms can strengthen trust by making model reasoning sufficiently clear for action, and that governance practices can reduce perceived ethical risk that would otherwise depress intention to use (Langer, König, & Papathanasiou, 2023; Stevens & Stetson, 2023).
Emerging AI-Specific Determinants of Adoption
Several streams of research extend established acceptance models with constructs tailored to AI’s distinctive features. In consumer contexts, augmented models add domain-relevant beliefs and show improved explanatory power over UTAUT2 alone (Gansser & Reich, 2021). In organizational settings, human-centric extensions propose predictors such as perceived humanness, transparency, bias, job threat, and privacy, reflecting workplace concerns about accountability, fairness, and role security (Alasmari, 2024). Across domains, four constructs recur as especially consequential for enterprise AI: trust in AI, perceived explainability, perceived ethical risk, and AI-related anxiety (Siau & Wang, 2018; Shin, 2021; Langer, König, & Papathanasiou, 2023).
Trust in AI functions as a gateway condition for reliance under uncertainty. Conceptually, it draws on the ability, integrity, and benevolence triad from the trust literature and adapts these facets to algorithmic systems. Empirically, trust mediates the influence of system properties on intention and use, particularly when decisions are consequential or auditable. Instruments developed in safety-critical domains provide multi-facet trust scales that can be adapted to enterprise environments without loss of construct clarity (Stevens & Stetson, 2023).
Perceived explainability captures whether users can understand and use reasons behind AI outputs. Studies distinguish explainability from causability, where the latter reflects the user’s ability to infer cause–effect logic sufficient for action. Evidence indicates that higher perceived explainability increases trust and intention, and can even increase trust in the human teams responsible for deployment, not only in the system itself (Shin, 2021; Cheung & Ho, 2025).
Perceived ethical risk reflects anticipated harms related to bias, fairness, privacy, and accountability. These concerns reduce intention to use and can moderate the effects of core predictors such as performance expectancy and social influence. In enterprise contexts, perceived ethical risk is tightly coupled to organizational legitimacy and governance practices, including bias audits, privacy safeguards, and clear lines of accountability for errors (Floridi et al., 2018; Dwivedi et al., 2021).
AI-related anxiety captures affective responses to autonomy, opacity, and rapid change. Research has conceptualized anxiety both globally and in subdimensions, including privacy-related anxiety, bias-related anxiety, and opacity-related anxiety. Recent work also identifies anticipatory anxiety and annihilation anxiety, with some evidence for a curvilinear relationship in which moderate exposure reduces anxiety while very low or very high exposure increases it (Tao et al., 2020/2021; Frenkenberg & Hochman, 2025; Kim et al., 2025).
Recent Extensions, 2021 to 2025: State of the Art
Trust-centered reviews conclude that trust is a decisive predictor across sectors, and that interventions which increase transparency and explainability tend to raise trust and downstream intention. Fielded instruments show that trust and acceptance can be measured with reliable, multi-item scales in professional settings and that these scales retain their structure outside the original clinical context (Langer, König, & Papathanasiou, 2023; Stevens & Stetson, 2023).
Explainability has moved from a technical desideratum to a psychological determinant of adoption. Empirical studies report that perceived clarity, sufficiency, and usefulness of explanations increase trust and willingness to act on AI recommendations, with effects observed both at the system level and in perceptions of the people behind the system (Shin, 2021; Cheung & Ho, 2025).
Ethical and privacy risks are now integral to adoption models. Studies show that perceived ethical risk exerts a direct negative effect on intention and can attenuate the influence of performance expectancy and social influence. This inhibitor role appears consistently in enterprise contexts where regulatory exposure and accountability are salient (Floridi et al., 2018; Dwivedi et al., 2021).
Continuance intentions have been analyzed with UTAUT-based models that incorporate trust–commitment mechanisms and accuracy perceptions. Findings indicate that performance expectancy, social influence, and perceived accuracy predict continued use, while privacy risk weakens these relationships, underscoring the need to model both positive drivers and inhibitors over time (Salih, Tarhini, & Acikgoz, 2025).
Together, these extensions converge on a practical architecture for enterprise AI adoption: retain UTAUT2’s validated core and add trust and explainability as enablers and ethical risk and anxiety as inhibitors. This structure aligns with observed effect patterns in recent studies and provides actionable levers for enterprise interventions through explainability design, governance and auditing, privacy and bias safeguards, and enablement programs that reduce anxiety and build calibrated trust (Doshi-Velez & Kim, 2017; Floridi et al., 2018; Langer, König, & Papathanasiou, 2023).
Human-Centric Additions in Organizational Contexts
In enterprise settings, scholars have adapted acceptance models to capture workplace realities such as accountability, fairness, and role security. Human-centric extensions introduce predictors that reflect how employees experience algorithmic systems in their day-to-day work. Proposed additions include perceived humanness of interaction, transparency, perceived bias, job threat, and privacy concerns, each of which aligns with organizational responsibilities for auditability and employee well-being (Alasmari, 2024). Parallel work in higher education and professional services adds trust and privacy explicitly to UTAUT pathways and finds that trust reliably increases intention to adopt while privacy concerns reliably reduce it, a pattern that mirrors enterprise environments where data sensitivity and reputational exposure are high (Rana, Siddiqee, Sakib, & Ahamed, 2024). Together, these studies position trust and privacy as cross-domain determinants that travel well from academia to corporate settings.
Continuance research extends the focus from initial adoption to sustained use. UTAUT-based models that incorporate trust–commitment mechanisms and perceived accuracy report that performance expectancy, social influence, and accuracy predict continued use, while privacy risk weakens these relationships. This points to the importance of modeling both drivers and inhibitors over time, rather than treating adoption as a single decision point (Salih, Tarhini, & Acikgoz, 2025). In safety-critical and professional domains, validated instruments for trust and acceptance, such as TrAAIT, demonstrate that multi-facet trust scales retain structure and reliability outside the original clinical context, which makes them promising templates for organizational surveys in nonclinical settings as well (Stevens & Stetson, 2023).
Explainability has moved from technical desideratum to organizational requirement. Studies show that perceived clarity, sufficiency, and actionability of explanations increase trust and willingness to act on AI recommendations. Some findings extend beyond the system itself to trust in the human teams responsible for deployment and oversight. This suggests that explainability influences both system-level and human-level trust, which are both relevant to enterprise governance and accountability (Shin, 2021; Cheung & Ho, 2025). Ethical risk remains a consistent inhibitor, with concerns about fairness, bias, privacy, and accountability exerting a negative effect on intention and weakening the influence of performance expectancy and social influence in regulated or high-stakes contexts (Floridi et al., 2018; Dwivedi et al., 2021).
Synthesis: What UTAUT2 Misses in AI
Across industries, performance expectancy and effort expectancy continue to predict intention, but they do not fully account for adoption when systems are opaque or consequential. Recent evidence identifies four constructs that repeatedly add explanatory power in enterprise AI: trust in AI and perceived explainability as enablers, and perceived ethical risk and AI-related anxiety as inhibitors. Trust functions as a gateway. Without sufficient trust, employees hesitate to rely on outputs even when systems appear useful. Explainability strengthens trust by making model reasoning accessible enough to justify action. Ethical risk depresses intention directly and attenuates the influence of core predictors in contexts where fairness, privacy, and accountability are salient. AI-related anxiety captures affective responses to autonomy, opacity, and change, with subdimensions such as privacy-related anxiety, bias-related anxiety, and opacity-related anxiety, and with emerging evidence for nonlinear exposure effects (Langer, König, & Papathanasiou, 2023; Shin, 2021; Floridi et al., 2018; Dwivedi et al., 2021; Tao et al., 2020/2021; Frenkenberg & Hochman, 2025; Kim et al., 2025).
These patterns motivate a principled extension to UTAUT2 for enterprise AI. Retain the validated core of performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit. Add trust and explainability as enablers that increase intention directly and by mediating core effects. Add perceived ethical risk and AI-related anxiety as inhibitors that exert negative direct effects and moderate positive pathways. This architecture is consistent with accumulated findings in organizational and professional settings and provides clear levers for intervention through explainability design, governance and auditing, privacy and bias safeguards, and enablement programs that reduce anxiety and build calibrated trust (Venkatesh, Thong, & Xu, 2012; Stevens & Stetson, 2023; Langer, König, & Papathanasiou, 2023; Floridi et al., 2018; Dwivedi et al., 2021).
Convergence Across Industries and Geographies
A growing body of empirical work indicates that the same small set of AI-specific determinants—trust, explainability, perceived ethical risk, and AI-related anxiety—recur across sectors, even as the salience of each construct shifts with domain constraints and regulatory exposure. In healthcare, where decisions are safety-critical and auditable, trust and accountability dominate the adoption calculus. Validated instruments such as the TrAAIT scale demonstrate that multidimensional trust (e.g., reliability, competence, predictability, and willingness to rely) can be measured with strong psychometric properties among clinicians, and initial evidence suggests these facets transport reasonably well to other professional settings that share accountability demands (Stevens & Stetson, 2023). Studies in clinical decision support further report that when model outputs are perceived as transparent enough to justify action, clinicians’ reliance increases, consistent with the theorized explainability-to-trust pathway foregrounded in acceptance research (Shin, 2021).
In financial services, adoption studies center on explainability and auditability due to stringent compliance requirements. Here, perceived clarity and actionability of explanations are repeatedly associated with higher intention to use, even when traditional ease-of-use beliefs are already favorable. This pattern suggests that for opaque or probabilistic systems, the marginal gains from improved explainability can outweigh additional gains from effort expectancy, aligning with the claim that explainability is not merely a usability attribute but an epistemic condition for justified reliance in regulated contexts (Shin, 2021). Parallel findings appear in risk management and fraud detection pilots where human review remains in the loop; users report greater willingness to escalate or overrule model outputs when organizations signal clear accountability and provide post-hoc or contrastive explanations, which together reduce perceived ethical risk.
Higher education and professional services emphasize the privacy–trust nexus. Survey research that augments UTAUT with trust and privacy consistently shows trust as a positive predictor and privacy concern as a negative predictor of intention to adopt AI tools in academic work, teaching, and knowledge production (Rana, Siddiqee, Sakib, & Ahamed, 2024). These effects mirror enterprise environments in which data sensitivity, confidentiality obligations, and reputational exposure are salient. The cross-domain similarity underscores that privacy risk operates as an inhibitor with both direct and moderating roles, dampening the otherwise reliable effects of performance expectancy and social influence when personal or sensitive data are implicated.
Consumer-facing domains replicate these themes while adding larger roles for hedonic motivation and habit. An augmented acceptance model calibrated for consumer AI products reports incremental explanatory power when health, convenience, sustainability, or safety beliefs are added to the UTAUT2 core, yet trust, explainability, and perceived risk remain competitive predictors of intention (Gansser & Reich, 2021). The consumer evidence complements enterprise findings by demonstrating that even where enjoyment and routine are strong, socio-technical evaluations of how and why an AI system produces its outputs continue to shape adoption.
Broad, integrative reviews converge on these cross-sector regularities. Syntheses of empirical research on trust in AI conclude that trust mediates the relationship between system properties and behavioral outcomes across application areas, while governance interventions—bias audits, privacy safeguards, and clear lines of accountability—reduce perceived ethical risk and thereby strengthen intention (Langer, König, & Papathanasiou, 2023). Information-systems overviews similarly argue that AI’s distinct properties warrant explicit modeling of trust, transparency or explainability, accountability, and privacy within acceptance frameworks, since these concerns systematically modulate the standard expectancy paths emphasized in the technology-acceptance canon (Dwivedi et al., 2021; Jöhnk, Weißert, & Wyrtki, 2021).
Taken together, the sectoral evidence suggests convergence rather than fragmentation: trust and explainability emerge as enablers, and perceived ethical risk and privacy-linked anxiety emerge as inhibitors, across industries and geographies. What varies by domain is not whether these constructs matter, but how much they matter and through which organizational levers they can be influenced. In safety-critical contexts, explainability and formal accountability channels are decisive; in data-sensitive knowledge work, privacy assurances and provenance become the fulcrum; in consumer settings, hedonic and habitual forces join the mix but do not erase the effects of trust and perceived risk. This cross-domain pattern justifies a common measurement architecture that can be tuned by sector while preserving theoretical coherence.
Measurement Quality: Reliability and Validity
Studies extending acceptance models for AI generally report encouraging psychometric performance when trust, explainability, perceived ethical risk, and AI-related anxiety are added to the UTAUT2 core. Typical development workflows follow established guidance: concept definition, item generation from theory and prior instruments, expert review for content validity, cognitive pretesting, and staged exploratory to confirmatory factor analyses with iterative item pruning (Hinkin, 1998; DeVellis, 2017). In exploratory phases, acceptable sampling adequacy and factorability are usually reported through KMO statistics at or above .60 and a significant Bartlett’s test, with retention rules emphasizing primary loadings at or above .50 and minimal cross-loadings to preserve construct distinctiveness (Comrey & Lee, 1992; Hinkin, 1998).
Reliability estimates for the added constructs are commonly at or above conventional thresholds. Studies report coefficient alpha and omega values of .70 or higher for first-order factors, and composite reliability estimates exceeding .70 for confirmatory models, suggesting adequate internal consistency for multi-item scales that operationalize trust, explainability, ethical risk, and anxiety in workplace settings (Gansser & Reich, 2021; Stevens & Stetson, 2023; Salih, Tarhini, & Acikgoz, 2025). Convergent validity is typically supported by average variance extracted values at or above .50 and by statistically significant standardized loadings. Discriminant validity is more sensitive to construct labeling and item wording. When researchers treat “transparency” as visibility into data flows and system processes, and “explainability” as reason-giving for particular outputs, Fornell–Larcker criteria and heterotrait–monotrait ratios generally support discriminant separation. When items blend these meanings, cross-loadings rise and HTMT values approach rule-of-thumb thresholds, indicating potential construct proliferation that can undermine interpretability of structural paths (Shin, 2021; Langer, König, & Papathanasiou, 2023).
Model fit improves when AI-specific constructs are added to UTAUT2 baselines. Confirmatory models frequently achieve comparative fit index and Tucker–Lewis index values near or above .90 and root mean square error of approximation values at or below .08 after item refinement, indicating better explanatory power without untenable complexity (Gansser & Reich, 2021; Stevens & Stetson, 2023). Incremental value is often demonstrated by comparing a UTAUT2-only model with an extended model that includes the four AI constructs, where the extended model explains additional variance in intention and continuance and yields superior fit indices (Salih et al., 2025). These improvements are theoretically coherent: trust and explainability capture epistemic evaluations of model outputs, while ethical risk and anxiety capture inhibitor states that standard expectancy beliefs do not fully address.
Two recurring measurement challenges merit attention. First, privacy may appear as a subdimension of ethical risk, as a distinct risk factor, or as an antecedent to trust. Each placement is defensible, but mixing roles across studies complicates synthesis and can obscure whether privacy primarily acts as an inhibitor, a trust antecedent, or a moderator of positive determinants such as performance expectancy and social influence (Dwivedi et al., 2021; Rana, Siddiqee, Sakib, & Ahamed, 2024). Second, anxiety constructs vary from global AI anxiety to subdimensions focused on privacy, bias, and opacity, with some studies adding anticipatory and existential components. Clear construct maps and item banks that align anxiety subdomains with theorized mechanisms improve comparability and reduce redundancy across instruments (Tao, Fu, Wang, & Zhang, 2020/2021; Kim et al., 2025; Frenkenberg & Hochman, 2025).
Design features that mitigate bias and strengthen inference are increasingly standard. Procedural remedies for common method variance include proximal separation of predictors and outcomes, varied response anchors, and confidentiality assurances tailored to organizational sensitivities around AI (Dillman, Smyth, & Christian, 2014; Fowler, 2014). Statistical checks range from marker variables to unmeasured latent method factors in the confirmatory framework (Hinkin, 1998). Because enterprise adoption often differs by role or voluntariness, multi-group confirmatory factor analysis for measurement invariance across organizational roles or exposure levels helps ensure that path comparisons are not confounded by differences in scale functioning (Venkatesh, Thong, & Xu, 2012). Where continuance or post-adoption adaptation is a focus, two-wave or longitudinal designs reduce same-time reporting artifacts and enable tests of stability in moderator effects such as privacy risk after initial rollout (Salih et al., 2025).
In sum, the measurement literature indicates that adding trust, explainability, perceived ethical risk, and AI-related anxiety to UTAUT2 is psychometrically tractable. Reliability and validity evidence is generally strong when constructs are carefully defined, item content is disciplined to avoid label conflation, and study designs incorporate both procedural and statistical controls. These practices support robust structural testing in enterprise contexts without sacrificing the parsimony that made the underlying acceptance frameworks durable.
AIRS Framework: Enablers and Inhibitors
The Artificial Intelligence Readiness Scale (AIRS) is a theory-grounded instrument that extends the UTAUT/UTAUT2 lineage for enterprise AI. AIRS does not discard the validated core performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit. Instead, it formalizes how AI-specific perceptions systematically reweight or gate those pathways. In AIRS, two constructs act as enablers, Trust in AI and Perceived Explainability, and two operate as inhibitors, Perceived Ethical Risk and AI-Related Anxiety.
Crucially, AIRS is designed for organizational realities. It nests these enablers and inhibitors within enterprise facilitating conditions, not just infrastructure and training, but governance artifacts such as model cards, audit trails, bias and performance monitoring, escalation pathways, privacy safeguards, and explicit human-in-the-loop procedures. It also accommodates practical moderators (role, exposure, voluntariness) that shift effect sizes: mandated use and visible governance tend to amplify social influence and facilitating conditions, whereas discretionary use places greater weight on trust, explainability, and anxiety relief.

Enablers: Trust in AI and Perceived Explainability
Trust in AI has moved from a background assumption to a foreground condition for reliance on AI outputs in organizational settings. Conceptually, this construct adapts the ability–integrity–benevolence triad from the interpersonal trust literature to algorithmic agents, often operationalized through perceptions of system competence or accuracy, fairness or integrity, and alignment with user or organizational goals. Empirical syntheses show that trust mediates the relationship between system properties and behavioral outcomes and that it is sensitive to context-specific concerns such as auditability and accountability in regulated environments (Langer, König, & Papathanasiou, 2023; Stevens & Stetson, 2023). In models that include both UTAUT2 predictors and trust, performance expectancy typically remains positive, but its effect on intention is stronger when trust is high, suggesting that trust amplifies the diagnostic value of perceived usefulness in opaque or high-stakes tasks.
Perceived explainability captures whether users can understand and utilize reasons behind AI outputs with sufficient clarity and timeliness to justify action. Studies distinguish explainability from causability, defined as the user’s ability to infer cause–effect logic adequate for decision making. Across domains, higher perceived explainability is associated with greater trust and intention to use, including spillover effects on trust in the human teams responsible for deployment and oversight. These findings support the view that explainability is not merely a technical property but a psychological enabler that strengthens both direct intention and the trust pathway (Shin, 2021; Stevens & Stetson, 2023).
Taken together, trust and perceived explainability function as enablers by increasing the probability that performance and effort expectancies translate into intention and use. They also provide concrete levers—explanation design, validation transparency, performance monitoring, and role-clarity for human oversight—that organizations can adjust without altering core functionality.

Inhibitors: Perceived Ethical Risk and AI-Related Anxiety
Perceived ethical risk aggregates concerns about bias, fairness, privacy, and accountability. In enterprise contexts, these concerns are tied to legitimacy, regulatory exposure, and employee well-being. Empirical studies report direct negative effects of ethical risk on intention and moderating effects that weaken the influence of performance expectancy and social influence when fairness or privacy concerns are salient. Organizational safeguards—bias audits, privacy-by-design, escalation pathways, and clear lines of accountability—are repeatedly identified as conditions that reduce perceived ethical risk and restore the predictive strength of core UTAUT2 paths (Floridi et al., 2018; Dwivedi et al., 2021).
AI-related anxiety captures affective responses to autonomy, opacity, and rapid change. Evidence ranges from global measures of AI anxiety to multidimensional structures that differentiate privacy-related, bias-related, and opacity-related anxiety, with additional components such as anticipatory anxiety about future disruptions and existential concerns about human obsolescence. Several studies observe that anxiety exerts direct negative effects on intention and can display nonlinear associations with exposure, where moderate exposure reduces anxiety but very low or very high exposure increases it. This implies that training and calibrated, hands-on experience can shift affective responses in favorable directions without overstating system capabilities (Tao, Fu, Wang, & Zhang, 2020/2021; Frenkenberg & Hochman, 2025; Kim et al., 2025).
In sum, perceived ethical risk and AI-related anxiety operate as inhibitors that depress intention directly and attenuate positive drivers when unaddressed. Their influence is strongest where accountability, privacy, or job-security concerns are salient, which is common in enterprise deployments involving sensitive data or consequential decisions.
Integrative Architecture
A principled extension toward AIRS for enterprise AI retains the UTAUT2 core—performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit—while adding four AI-specific constructs with distinct theoretical roles:
1.	Trust in AI (enabler): positively predicts intention; mediates and amplifies the effects of explainability and performance expectancy, especially under opacity and high stakes.
2.	Perceived explainability (enabler): positively predicts intention and trust; improves the translation of expectancy beliefs into willingness to rely on outputs.
3.	Perceived ethical risk (inhibitor): negatively predicts intention; moderates and weakens positive paths from performance expectancy and social influence when fairness, privacy, or accountability are in doubt.
4.	AI-related anxiety (inhibitor): negatively predicts intention; may exhibit nonlinear relations with exposure and interacts with explainability and governance to shape readiness.
This architecture is not a wholesale reinvention of acceptance theory. It is a calibrated update that respects the durability of the expectancy-and-support core while acknowledging AI’s socio-technical distinctives. It also maps cleanly to organizational levers: explanation quality and transparency to build trust, governance and auditing to reduce ethical risk, and enablement and exposure design to manage anxiety.
Operationalizing the Constructs: A Measurement Blueprint
This section translates the extended framework into measurable constructs that align with enterprise use. The approach is to retain the validated UTAUT2 core and add four AI-specific constructs that have shown consistent effects across domains. Item wording should be anchored to the respondent’s work tasks and responsibilities to preserve content validity and reduce ambiguity.


UTAUT2 core retained for enterprise contexts
Performance expectancy, effort expectancy, social influence, facilitating conditions, hedonic motivation, price value, and habit remain the baseline predictors of intention and use. For enterprise respondents, items should reference job outcomes, collaboration, and available support. Example stems include “Using AI tools helps me complete complex analytical tasks more quickly” for performance expectancy and “Learning to work effectively with AI assistants is easy for me” for effort expectancy (Venkatesh et al., 2003; Venkatesh et al., 2012).
Trust in AI
Concept. Confidence that the AI system is reliable, competent, and aligned with user and organizational values. This adapts the ability, integrity, and benevolence triad from trust research to algorithmic agents (Siau & Wang, 2018; Langer et al., 2023).
Item domains. Accuracy or competence, fairness or integrity, predictability or reliability, alignment with user goals, and willingness to rely. Example items: “I trust the accuracy of this AI’s outputs,” “I believe this AI operates fairly across cases,” “I am willing to rely on this AI’s recommendations in my work.” Multidimensional instruments such as TrAAIT in clinical settings show that trust facets can be measured reliably and adapted to nonclinical enterprise contexts with careful wording changes (Stevens & Stetson, 2023).
Design notes. Calibrate items to the decision stakes. For safety critical or audit heavy tasks, add facets for predictability and accountability. Include a few reverse keyed items to reduce acquiescence bias.
Perceived explainability
Concept. The perceived clarity, sufficiency, and actionability of reasons provided for AI outputs. Explainability is distinct from causability, which is the user’s ability to infer cause and effect logic adequate for action (Shin, 2021).
Item domains. Clarity of reasons, sufficiency for decision justification, timeliness, and usefulness for next steps. Example items: “The AI provides understandable reasons for its recommendations,” “The explanations are sufficient for me to justify decisions,” “The explanations tell me what to do next.”
Design notes. To avoid conflating transparency and explainability, keep items focused on explanation quality for specific outputs rather than general process visibility. If needed, a separate transparency subscale can cover data flows and general system visibility.
Perceived ethical risk
Concept. Anticipated harms related to bias, fairness, privacy, accountability, and job threat. Ethical risk typically acts as an inhibitor and can moderate positive predictors such as performance expectancy and social influence (Floridi et al., 2018; Dwivedi et al., 2021).
Item domains. Bias and fairness, privacy and confidentiality, auditability and accountability, potential job or role impact. Example items: “I am concerned this AI could introduce bias into decisions,” “I worry about privacy or confidentiality when using this AI,” “It is clear who is accountable when the AI makes an error” [reverse coded], “I am concerned AI could negatively affect roles in my organization.”
Design notes. Model ethical risk as a higher order factor with subdimensions if item counts permit. This preserves diagnostic value for governance while maintaining parsimony.
AI related anxiety
Concept. Affective responses to AI autonomy, opacity, and pace of change. Evidence supports both a global factor and subdimensions, with potential nonlinear relations to exposure where moderate use reduces anxiety (Tao et al., 2020/2021; Frenkenberg & Hochman, 2025; Kim et al., 2025).
Item domains. Privacy related anxiety, bias related anxiety, opacity related anxiety, anticipatory concerns about disruption, and existential concerns about human obsolescence. Example items: “I feel uneasy relying on an AI whose reasoning I cannot see,” “I worry that AI may treat some cases unfairly,” “The speed of AI advances makes me anxious about my future work.”
Design notes. Keep items distinct from ethical risk by emphasizing felt tension or unease rather than perceived harms. Where programs plan training or staged exposure, include items that reference comfort change after usage.
Scale development workflow
Process. Define constructs and item domains from theory and prior instruments, conduct expert review, pretest with cognitive interviews, pilot for initial item statistics, then perform exploratory and confirmatory factor analyses with iterative pruning (Hinkin, 1998; DeVellis, 2017).
Psychometrics. In EFA, target KMO at or above .60 and retain items with primary loadings at or above .50 and minimal cross loadings (Comrey & Lee, 1992). In CFA, aim for comparative fit index and Tucker–Lewis index at or above .90 and root mean square error of approximation at or below .08. Report coefficient alpha and omega at or above .70, composite reliability at or above .70, and average variance extracted at or above .50. For discriminant validity, use Fornell–Larcker and heterotrait–monotrait checks, paying close attention to separation between transparency and explainability (Shin, 2021; Langer et al., 2023).
Design safeguards. Use consistent Likert scaling, group conceptually related items, and separate predictors from outcomes to reduce common method variance. Provide confidentiality assurances and plain language definitions for key terms to improve comprehension and candor in enterprise settings (Dillman et al., 2014; Fowler, 2014; Floridi et al., 2018).

Integration with the UTAUT2 core
Placement in models. Trust in AI and perceived explainability function as enablers with direct paths to intention and indirect effects through mediation or amplification of performance expectancy. Perceived ethical risk and AI related anxiety function as inhibitors with direct negative paths and plausible moderations of performance expectancy and social influence. This placement preserves comparability with UTAUT2 while capturing the socio technical distinctives of AI in the workplace (Venkatesh et al., 2012; Shin, 2021; Langer et al., 2023; Dwivedi et al., 2021).
Enterprise-Relevant Moderators and Facilitating Conditions
Enterprise deployments surface contextual moderators that shape the size and even the direction of adoption paths. The original UTAUT framework positions age, gender, experience, and voluntariness as moderators of performance expectancy, effort expectancy, social influence, and facilitating conditions. In organizational AI rollouts, voluntariness typically varies by program or role, making it especially consequential for early use. Where use is mandated, social influence and facilitating conditions tend to dominate initial uptake; where use is discretionary, trust in AI, perceived explainability, and perceived ethical risk exert stronger effects on intention because employees retain latitude to rely on or ignore AI outputs (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, 2012).
Role and exposure function as practical moderators in enterprise settings. Managers and executives often weight accountability, fairness, and reputational risk more heavily than individual contributors, which elevates the impact of perceived ethical risk on their intentions. Frequent users, by contrast, show stronger effects of habit and performance expectancy as familiarity grows and tasks become routinized. This divergence helps explain why the same AI service can show high satisfaction among hands-on specialists and continued skepticism among leaders responsible for downstream consequences. Studies that include exposure or tenure with the tool typically find that the negative effect of AI-related anxiety weakens with calibrated use, especially when explainability mechanisms and governance channels are visible to users.
Facilitating conditions require domain-specific expansion for AI. In addition to infrastructure, access, and training, employees evaluate whether the organization has put in place guardrails that make reliance reasonable. These guardrails include bias and performance monitoring, privacy and confidentiality safeguards, audit trails and model cards, escalation pathways to contest or override outputs, and clear human-in-the-loop procedures that allocate accountability. Such conditions do more than enable access; they reduce perceived ethical risk and increase the perceived safety of relying on probabilistic outputs. In settings with sensitive data or regulatory exposure, these governance resources are often cited by users as decisive for moving from experimentation to consequential use.
Technology readiness at the individual level can also shape baseline receptivity. Dispositions such as optimism and innovativeness act as enablers, while discomfort and insecurity act as inhibitors. Although these dispositions predate modern AI, their inhibitor facets conceptually overlap with AI-related anxiety. Modeling technology readiness as an antecedent to trust and anxiety, or as a control, helps isolate the incremental effects of the four AI-specific constructs without conflating stable traits with perceptions of a particular system (Parasuraman, 2000).
Privacy concerns sit at the intersection of moderators and inhibitors. In some models privacy risk operates as a direct inhibitor; in others it serves as a moderator that dampens the positive effects of performance expectancy and social influence, particularly when tasks involve personal or confidential data. A practical implication is that privacy assurances and provenance transparency are not peripheral communications tasks. They are facilitating conditions that change the structural weights in adoption models by lowering perceived ethical risk and anxiety, thereby restoring the predictive strength of core expectancy paths.
Taken together, enterprise moderators and facilitating conditions do not replace the UTAUT and UTAUT2 core. They specify when and how those core relationships hold under AI’s socio-technical constraints. Voluntariness, role, and exposure determine the relative weight of social influence, habit, and expectancy beliefs, while AI-specific facilitating conditions—governance, auditability, escalation, and human oversight—operate as levers that reduce ethical risk and enable justified reliance.
Scale Development and Validation Procedures
Rigorous measurement is the hinge between persuasive theory and credible findings. Studies that extend acceptance models for AI follow a broadly consistent workflow that is well supported in the measurement literature and readily adapted to enterprise contexts.
Conceptualization and item generation
Constructs should be defined at the outset with crisp conceptual boundaries that distinguish enablers (trust in AI, perceived explainability) from inhibitors (perceived ethical risk, AI-related anxiety) and from the UTAUT2 core. Item pools are then generated deductively from theory and inductively from prior instruments and domain artifacts (e.g., model cards, governance policies). Expert review is used to trim redundancy, surface jargon, and verify coverage of each construct domain. Recommended practice emphasizes clear, job-anchored wording to ensure items reference the respondent’s actual decision context rather than generic technology use (Hinkin, 1998; DeVellis, 2017; Venkatesh et al., 2012).
Cognitive pretesting and pilot administration
Cognitive interviews probe comprehension, retrieval, judgment, and response mapping. Particular attention is given to terms that have neighboring meanings (e.g., transparency versus explainability), to negatively keyed items that may introduce artifactual factors, and to the readability of items for nontechnical roles. A short pilot assesses time burden, missingness patterns, response scale functioning, and preliminary item–total statistics. Tailored design elements—plain definitions, consistent Likert anchors, and guidance about confidentiality—help reduce satisficing and social desirability in enterprise rollouts (Dillman, Smyth, & Christian, 2014; Fowler, 2014).
Exploratory factor analysis
Exploratory factor analysis on the pilot or development sample tests dimensionality before confirmatory modeling. Sampling adequacy (KMO) should meet acceptable thresholds and Bartlett’s test should be significant. Items with primary loadings of .50 or higher and minimal cross-loadings are retained; problematic items that conflate constructs (e.g., mixing process visibility with reason-giving) are revised or removed. Ethical risk can be modeled as a higher-order factor with subdomains (bias/fairness, privacy/confidentiality, accountability/job threat) when item counts permit, preserving diagnostic value for governance while maintaining parsimony (Comrey & Lee, 1992; Hinkin, 1998).
Confirmatory factor analysis
Confirmatory factor analysis on a holdout sample evaluates the proposed multi-construct measurement model. Target fit thresholds such as CFI and TLI at or above .90 and RMSEA at or below .08, coupled with significant standardized loadings, indicate acceptable fit. Internal consistency is evaluated with coefficient alpha and omega at or above .70, and composite reliability at or above .70. Convergent validity is typically demonstrated by average variance extracted at or above .50 (DeVellis, 2017).
Discriminant validity and construct clarity
Discriminant validity is checked with Fornell–Larcker criteria and heterotrait–monotrait ratios. The most frequent pressure point is separation of transparency from explainability. Transparency items should capture visibility into data flows and governance processes, whereas explainability items should capture the clarity, sufficiency, and actionability of reasons for specific outputs. Keeping these item families conceptually disciplined prevents construct proliferation and preserves interpretability of downstream structural paths (Shin, 2021; Langer, König, & Papathanasiou, 2023).
Common method variance and design safeguards
Procedural remedies include separating predictors from outcomes in the instrument, varying response anchors across sections, and assuring confidentiality in language calibrated to organizational sensitivities around AI. Statistical checks such as marker variables or an unmeasured latent method factor can be used to probe residual method effects. Multi-group CFA for measurement invariance across roles (e.g., leaders versus individual contributors) and exposure levels (novice versus frequent users) guards against biased path comparisons due to scale noninvariance (Hinkin, 1998; Venkatesh, Thong, & Xu, 2012).
Incremental value over UTAUT2
To establish the necessity of AI-specific constructs, compare a UTAUT2-only baseline with an extended model that adds trust, explainability, ethical risk, and AI-related anxiety. Improvements in fit indices and explained variance for intention and continuance, coupled with theoretically coherent path changes, constitute evidence that the extensions capture variance that standard expectancy beliefs cannot. Report both absolute and incremental fit, along with nested-model comparisons, to make the case transparent (Gansser & Reich, 2021; Stevens & Stetson, 2023; Salih, Tarhini, & Acikgoz, 2025).
Design for continuance and adaptation
Where ongoing use and adaptation matter, two-wave or longitudinal designs reduce same-time reporting artifacts and enable tests of stability in moderator effects, such as whether privacy risk continues to dampen positive predictors after users gain experience. Including calibrated exposure (e.g., training completion or number of assisted tasks) allows tests of hypothesized nonlinear relations between exposure and anxiety that have begun to appear in the literature (Tao et al., 2020/2021; Frenkenberg & Hochman, 2025; Kim et al., 2025).
As a whole, this procedural spine—conceptual clarity, disciplined item content, staged EFA→CFA, strong reliability and validity checks, and explicit incremental-value tests—yields instruments that are both psychometrically credible and practically informative for enterprise AI programs.
Theoretical Contribution: AI as a Socio-Technical Adoption Case
The reviewed literature positions AI adoption not as a simple extension of general IT acceptance but as a socio-technical case in which epistemic, ethical, and affective evaluations systematically condition the familiar expectancy, social, and infrastructure mechanisms. Across settings, users do not merely ask whether an AI tool is useful and easy to use. They also ask whether the system’s reasons are knowable enough to justify action, whether reliance is fair and privacy-preserving, and whether they can trust the people and processes behind deployment. These questions introduce construct families that classic acceptance models leave implicit. Trust in AI and perceived explainability capture a user’s ability to form justified reliance under uncertainty; perceived ethical risk and AI-related anxiety capture inhibitors that dampen intention even when performance and effort expectancies are favorable.
This socio-technical lens explains several otherwise puzzling findings in enterprise rollouts. First, strong performance expectancy does not guarantee uptake when fairness or privacy concerns remain unresolved; perceived ethical risk operates as a gate that can attenuate core paths. Second, usability investments that reduce effort expectancy can show diminishing returns when systems remain opaque; perceived explainability and governance signals, rather than incremental ease of use, become the decisive levers. Third, social influence may spur initial trials under mandate, but sustained, consequential use depends on trust and perceived safety, which are shaped by organization-level safeguards as much as by interface qualities. In this way, AI foregrounds organizational governance as part of “facilitating conditions,” expanding that construct beyond access and training to include bias audits, accountability pathways, and human-in-the-loop procedures.
A second contribution is conceptual discipline among neighboring terms. Transparency and explainability have often been conflated in applied studies; the literature supports treating transparency as visibility into data and process, and explainability as reason-giving for specific outputs. Disentangling these improves discriminant validity and clarifies levers: transparency interventions target oversight and auditability, whereas explainability interventions target user comprehension for decision justification. Likewise, privacy can be modeled as a subdimension of ethical risk, a predictor of trust, or a moderator of expectancy paths. The review recommends declaring its role a priori to prevent interpretive drift and to enable clearer cumulative evidence.
Finally, AI reframes the role of affect. Anxiety is not noise around rational beliefs; it is a patterned inhibitor that reflects autonomy, opacity, and pace of change, sometimes exhibiting nonlinear relationships with exposure. This helps explain why calibrated practice, staged enablement, and visible guardrails are repeatedly associated with improved willingness to rely on AI, even when raw accuracy metrics are unchanged. The theoretical upshot is a parsimonious extension to UTAUT2 in which trust and explainability function as enablers and ethical risk and anxiety function as inhibitors, mapping directly to enterprise interventions without discarding the durable expectancy-and-support core.

 
References
Ajzen, I. (1991). The theory of planned behavior. Organizational Behavior and Human Decision Processes, 50(2), 179–211. https://doi.org/10.1016/0749-5978(91)90020-T
Cheung, J. C., & Ho, S. S. (2025). The effectiveness of explainable AI on human factors in trust models. Scientific Reports, 15, 23337. https://doi.org/10.1038/s41598-025-04189-9
Comrey, A. L., & Lee, H. B. (1992). A first course in factor analysis (2nd ed.). Psychology Press.
Creswell, J. W., & Plano Clark, V. L. (2018). Designing and conducting mixed methods research (3rd ed.). SAGE.
Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319–340. https://doi.org/10.2307/249008
DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). SAGE.
Dillman, D. A., Smyth, J. D., & Christian, L. M. (2014). Internet, phone, mail, and mixed-mode surveys: The tailored design method (4th ed.). Wiley.
Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv:1702.08608.
Dwivedi, Y. K., et al. (2021). Artificial intelligence: Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy. International Journal of Information Management, 57, 101994. https://doi.org/10.1016/j.ijinfomgt.2019.08.002
Fetters, M. D., Curry, L. A., & Creswell, J. W. (2013). Achieving integration in mixed methods designs—Principles and practices. Health Services Research, 48(6 Pt 2), 2134–2156. https://doi.org/10.1111/1475-6773.12117
Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., … Vayena, E. (2018). AI4People—An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations. Minds and Machines, 28(4), 689–707. https://doi.org/10.1007/s11023-018-9482-5
Fowler, F. J. (2014). Survey research methods (5th ed.). SAGE.
Frenkenberg, A., & Hochman, G. (2025). It’s scary to use it, it’s scary to refuse it: The psychological dimensions of AI anxiety. Systems, 13(2), 82. https://doi.org/10.3390/systems13020082
Gansser, O. A., & Reich, C. S. (2021). A new acceptance model for artificial intelligence with extensions to UTAUT2: An empirical study in three segments of application. Technology in Society, 65, 101535. https://doi.org/10.1016/j.techsoc.2021.101535
Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM Computing Surveys, 51(5), Article 93. https://doi.org/10.1145/3236009
Hinkin, T. R. (1998). A brief tutorial on the development of measures for use in survey questionnaires. Organizational Research Methods, 1(1), 104–121. https://doi.org/10.1177/109442819800100106
Jöhnk, J., Weißert, M., & Wyrtki, K. (2021). Ready or not, AI comes—An interview study of organizational AI readiness factors. Business & Information Systems Engineering, 63, 5–20.
Kim, J. J. H., Soh, J., Kadkol, S., Solomon, I., Yeh, H., Srivatsa, A. V., … Ajilore, O. (2025). AI Anxiety: A comprehensive analysis of psychological factors and interventions. AI and Ethics, 5, 3993–4009. https://doi.org/10.1007/s43681-025-00686-9
Langer, M., König, C. J., & Papathanasiou, M. (2023). Trust in artificial intelligence: A review of empirical research. Journal of Business Research, 157, 113609. https://doi.org/10.1016/j.jbusres.2022.113609
Parasuraman, A. (2000). Technology Readiness Index (TRI): A multiple-item scale to measure readiness to embrace new technologies. Journal of Service Research, 2(4), 307–320. https://doi.org/10.1177/109467050024001
Patton, M. Q. (2015). Qualitative research & evaluation methods (4th ed.). SAGE.
Rana, M. M., Siddiqee, M. S., Sakib, M. N., & Ahamed, M. R. (2024). Assessing AI adoption in developing country academia: A trust- and privacy-augmented UTAUT framework. Heliyon, 10(18), e37569. https://doi.org/10.1016/j.heliyon.2024.e37569
Rogers, E. M. (2003). Diffusion of innovations (5th ed.). Free Press.
Salih, M., Tarhini, A., & Acikgoz, A. (2025). Continuance intention in AI-enabled services: An extended UTAUT2 model with trust–commitment and privacy risk. Journal of Computer Information Systems (in press).
Shin, D. (2021). The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International Journal of Human-Computer Studies, 146, 102551. https://doi.org/10.1016/j.ijhcs.2020.102551
Siau, K., & Wang, W. (2018). Building trust in artificial intelligence, machine learning, and robotics. Cutter Business Technology Journal, 31(2), 47–53.
Stevens, A. F., & Stetson, P. (2023). Theory of trust and acceptance of artificial intelligence technology (TrAAIT): An instrument to assess clinician trust and acceptance of AI. Journal of Biomedical Informatics, 148, 104550. https://doi.org/10.1016/j.jbi.2023.104550
Tao, D., Fu, P., Wang, Y., & Zhang, T. (2020/2021). The influence of AI anxiety on user acceptance of intelligent products: Evidence and dimensions. Technology in Society, 63, 101407. https://doi.org/10.1016/j.techsoc.2020.101407
Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 27(3), 425–478. https://doi.org/10.2307/30036540
Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. MIS Quarterly, 36(1), 157–178. https://doi.org/10.2307/41410412
Weidinger, L., et al. (2022). Ethical and social risks of harm from language models. arXiv:2112.04359.

