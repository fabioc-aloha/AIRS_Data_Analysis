\begin{titlepage}
\centering
\vspace*{2cm}

{\Large\textbf{Artificial Intelligence Readiness Scale:\\
Extending UTAUT2 for Enterprise AI Adoption}}

\vspace{1cm}

{\large A Dissertation Summary}

\vspace{2cm}

{\large\textbf{Fabio Correa}}

\vspace{0.5cm}

{\normalsize Doctoral Candidate, Doctor of Business Administration\\
Touro University Worldwide\\
December 2025}

\vfill
\end{titlepage}

\newpage

# Introduction and Research Problem

The rapid proliferation of artificial intelligence in organizational settings has created an urgent need to understand the psychological factors that drive or inhibit AI adoption. While organizations worldwide have embraced AI at unprecedented rates (rising from approximately 50% adoption historically to 72% in 2024 and 88% by late 2025) this adoption has not translated into proportional value capture. Industry research reveals a troubling pattern: Boston Consulting Group reports that only 5% of companies achieve measurable business value from AI initiatives, while MIT Media Lab's NANDA Initiative found that 90-95% of generative AI pilots fail to scale or deliver measurable improvements.

This adoption-value paradox presents both a theoretical puzzle and a practical challenge. Traditional technology acceptance models, including the widely-validated Unified Theory of Acceptance and Use of Technology (UTAUT2), have demonstrated robust explanatory power for conventional technologies. However, the unique characteristics of AI systems (including opacity, probabilistic reasoning, and ethical implications) may necessitate theoretical extension. The question driving this research was whether established technology acceptance frameworks adequately explain AI-specific adoption patterns, or whether new constructs are needed to understand this theoretically distinct technology category.

This dissertation addressed this gap through two primary objectives: (1) to develop and validate a psychometrically sound AI Readiness Scale (AIRS) extending UTAUT2 with AI-specific constructs, and (2) to identify the key drivers of AI adoption intention in professional and academic contexts. The research contributes both theoretical advancement through framework extension and practical utility through a validated diagnostic instrument.

## Theoretical Framework and Literature Review

The study's theoretical foundation rests on Venkatesh et al.'s (2003, 2012) UTAUT and UTAUT2 frameworks, which synthesize decades of technology acceptance research into a unified model. UTAUT specifies four core determinants of behavioral intention: Performance Expectancy (beliefs about technology enhancing job performance), Effort Expectancy (perceived ease of use), Social Influence (perceptions of important others' opinions), and Facilitating Conditions (organizational and technical infrastructure). UTAUT2 extended this framework for consumer contexts by adding Hedonic Motivation (enjoyment derived from technology use), Price Value (cost-benefit trade-offs), and Habit (automaticity from repeated use).

Blut et al.'s (2022) comprehensive meta-analysis of UTAUT research, synthesizing 25,619 effect sizes from 737,112 users across 1,935 independent samples, confirmed the framework's core predictions while revealing nuanced patterns. Performance Expectancy emerged as the consistently strongest predictor of behavioral intention (ρ = .60), with Effort Expectancy showing moderate but context-dependent effects (ρ = .45). Critically, this meta-analysis identified substantial unexplained variance and called for domain-specific extensions incorporating constructs relevant to emerging technologies, directly motivating this study's AI-specific extensions.

The literature review identified three AI-specific constructs warranting investigation. AI Trust addresses the unique challenges of trusting systems that operate through opaque algorithms and probabilistic reasoning, where users cannot directly observe or verify decision-making processes. AI Anxiety captures apprehension about AI systems that may stem from concerns about job displacement, loss of control, or fundamental uncertainty about machine capabilities. Explainability and Ethical Risk address emerging concerns about AI transparency and the moral implications of algorithmic decision-making. These constructs represent psychological dimensions that traditional technology acceptance models, designed for deterministic and transparent systems, may not adequately capture.

## Research Methodology

The study employed a rigorous ten-phase psychometric validation approach designed to exceed typical scale development standards. Data collection yielded 523 complete responses from a United States sample comprising students (41.3%, n=216), professionals (35.2%, n=184), and organizational leaders (23.5%, n=123). This population distribution enabled examination of AI adoption factors across the career development spectrum while providing sufficient statistical power for multi-group analyses.

The methodological design incorporated a split-sample validation strategy, randomly dividing the dataset into an exploratory factor analysis (EFA) development sample (n=261) and a confirmatory factor analysis (CFA) holdout sample (n=262) using a fixed random seed for reproducibility. This cross-validation approach, rarely employed in scale development research, provides strong evidence for the generalizability of the factor structure by demonstrating that patterns discovered in the development sample replicate in independent data.

The ten phases proceeded systematically: sample splitting, exploratory factor analysis to identify the underlying factor structure, confirmatory factor analysis for cross-validation, measurement invariance testing across student and professional populations, structural equation modeling to test hypothesized relationships, mediation analysis for indirect effects, moderation analysis examining experience and usage frequency as boundary conditions, behavioral validation correlating intentions with actual AI tool usage patterns, qualitative analysis of open-ended responses, and final synthesis integrating quantitative and qualitative insights.

Model fit was evaluated against established thresholds from Hu and Bentler (1999) and Hair et al. (2019): CFI ≥ .95, TLI ≥ .95, RMSEA ≤ .08, and SRMR ≤ .08. Reliability was assessed through Cronbach's alpha, composite reliability, and average variance extracted, with thresholds of α > .70, CR > .70, and AVE > .50. Discriminant validity was evaluated using the Fornell-Larcker criterion, requiring the square root of AVE to exceed inter-factor correlations.

## Results and Key Findings

### Diagnostic Instrument Validation

The exploratory factor analysis identified an 8-factor structure comprising the seven UTAUT2 constructs (Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Conditions, Hedonic Motivation, Price Value, and Habit) plus AI Trust. Four proposed AI-specific constructs (Voluntariness, Explainability, Ethical Risk, and AI Anxiety) demonstrated inadequate reliability (α = .301-.582) and were excluded from the validated model. This exclusion represents an empirical finding about measurement challenges rather than a design failure, indicating that these constructs require more comprehensive operationalization in future research.

The confirmatory factor analysis on the independent holdout sample demonstrated excellent model fit: CFI = .975, TLI = .960, RMSEA = .065, and SRMR = .048. All indices exceeded conventional thresholds, supporting the 8-factor, 16-item measurement model. Reliability analyses confirmed strong internal consistency, with Cronbach's alpha ranging from .743 (Facilitating Conditions) to .909 (Habit), composite reliability ranging from .750 to .909, and average variance extracted ranging from .601 to .833. All factors exceeded minimum thresholds, demonstrating adequate convergent validity. Discriminant validity was confirmed through the Fornell-Larcker criterion, with no inter-factor correlations approaching the .85 threshold.

Measurement invariance testing established configural invariance across student and professional populations, supporting the instrument's utility for diverse workplace contexts. Metric invariance was not fully achieved (mean Δλ = .082, max Δλ = .326), suggesting that while the same factor structure applies across groups, some item loadings may vary, a finding with implications for cross-group comparisons that future research should address.

### Hypothesis Testing

The structural model testing produced several significant findings that depart substantially from traditional UTAUT research:

**Price Value emerged as the dominant predictor of behavioral intention** (β = .505, p < .001), representing the strongest effect in the model. This finding represents a significant departure from traditional UTAUT research where Performance Expectancy typically dominates. Blut et al.'s (2022) meta-analysis found Performance Expectancy to be the strongest predictor (ρ = .60) across technology contexts; the reversal observed in this study suggests fundamental differences in how users evaluate AI tools compared to conventional technologies.

**Hedonic Motivation was the second strongest predictor** (β = .217, p = .014), indicating that enjoyment and engagement quality significantly influence AI adoption intention. This finding aligns with the consumer-oriented nature of contemporary AI tools and suggests that intrinsic satisfaction plays an important role in adoption decisions.

**Social Influence demonstrated a modest but significant effect** (β = .136, p = .024), confirming that peer influence and organizational norms matter for AI adoption. This finding supports the theoretical importance of social factors while indicating they play a secondary role to value perceptions.

**AI Trust approached but did not reach conventional significance** (β = .106, p = .064), providing tentative support for the theoretical extension while highlighting the need for larger samples or refined measurement in future research.

**Traditional UTAUT predictors were not significant**: Performance Expectancy (β = -.028, p = .791), Effort Expectancy (β = -.008, p = .875), Facilitating Conditions (β = .059, p = .338), and Habit (β = .023, p = .631) did not predict AI adoption intention. This unexpected finding challenges fundamental assumptions about technology adoption drivers and suggests that AI represents a psychologically distinct technology category.

The complete 8-factor model explained 85.2% of variance in behavioral intention (R² = .852), an exceptionally high value for technology adoption research. While a 7-factor UTAUT2-only model achieved marginally higher fit (R² = .861), the 8-factor model was selected because AI Trust provides essential diagnostic capability: practitioners can identify trust deficits and design targeted interventions, a critical feature for translating research into organizational practice.

### Moderation and Behavioral Validation

Moderation analysis revealed that professional experience significantly strengthened the Hedonic Motivation -> Behavioral Intention path (β = .136, p = .009). This finding suggests that as professionals advance in their careers, intrinsic satisfaction becomes more important in technology evaluation, a novel integration of career development theory with technology acceptance research.

Behavioral validation confirmed a strong correlation between intention and self-reported AI tool usage (ρ = .69, p < .001), providing criterion validity evidence. Analysis of tool usage patterns revealed that organizational leaders demonstrated substantially higher AI tool usage than other groups (d = 0.74–1.14 across tools), suggesting that leadership engagement may be a key factor in organizational AI adoption.

Cluster analysis identified four distinct user segments: AI Enthusiasts (16%) characterized by high trust, high intention, and low anxiety; Cautious Adopters (30%) with moderate trust and an evaluative stance; Moderate Users (37%) with balanced profiles and pragmatic approaches; and Anxious Avoiders (17%) with elevated anxiety and low adoption intention. This typology provides a framework for future intervention research targeting heterogeneous adoption readiness patterns.

## Discussion and Theoretical Contributions

This dissertation makes four primary contributions to technology acceptance theory. First, the study extends UTAUT2 with AI-specific constructs, demonstrating that traditional technology acceptance frameworks require modification for AI adoption contexts. The near-significant AI Trust effect and the dramatic shift from Performance Expectancy to Price Value dominance suggest that AI represents a theoretically distinct technology category requiring tailored conceptual frameworks.

Second, the finding that Price Value rather than Performance Expectancy drives AI adoption represents a significant theoretical departure. Users appear to evaluate AI tools through a value lens ("Is it worth it?") rather than a utility lens ("Will it help me?"). This shift may reflect several factors unique to the AI context: the rapid commoditization of AI capabilities, uncertainty about long-term benefits, and awareness of both direct and hidden costs including learning curves, workflow disruption, and data privacy concerns.

Third, the experience moderation effect introduces career development as a relevant theoretical domain for technology acceptance research. As professionals advance, intrinsic satisfaction becomes more important, suggesting that adoption models should incorporate career-stage considerations, a finding that connects technology acceptance to vocational psychology in novel ways.

Fourth, the empirically-derived four-segment typology provides insights into adoption heterogeneity that can inform future intervention research. Rather than treating users as a homogeneous population, this segmentation reveals that different psychological profiles may respond to different adoption strategies, a hypothesis warranting future experimental validation.

## Practical Implications and Recommendations

The research findings offer several implications for organizations navigating AI adoption challenges. The dominance of Price Value suggests that demonstrating clear return on investment may be more effective than highlighting AI capabilities alone. The significant Hedonic Motivation effect indicates that designing AI tools for engagement, not just utility, may enhance adoption. The Social Influence finding suggests that visible AI champions and peer communities may facilitate adoption efforts.

The four-segment typology provides a framework for considering tailored approaches: Enthusiasts may serve as organizational champions, Cautious Adopters may respond to evidence-based communication, Moderate Users may benefit from targeted use-case demonstrations, and Anxious Avoiders may require anxiety-reduction approaches before capability training. However, these represent evidence-informed hypotheses requiring experimental validation rather than prescriptive recommendations.

For researchers, the validated 16-item AIRS instrument provides a psychometrically sound foundation for investigating AI adoption across diverse contexts. Immediate research priorities include replication with larger samples (n > 600) to adequately power detection of small effects like the Trust coefficient, redesigning measures for excluded constructs using enhanced operationalization, and longitudinal validation tracking actual adoption behavior over 6–12 months following intention measurement.

## Limitations and Future Directions

Several limitations should guide interpretation of these findings. The cross-sectional design precludes causal inference; while structural equation modeling suggests directional relationships, alternative causal orderings cannot be ruled out. The panel sample, while benefiting from Centiment's topic-blinded recruitment to mitigate self-selection bias, limits generalizability to other countries, cultures, and organizational contexts. The exclusion of four proposed constructs due to reliability issues limits the comprehensiveness of the theoretical extension.

Future research should prioritize longitudinal designs tracking intention-behavior relationships over time, cross-cultural validation in collectivist cultures and developing economies, industry-specific adaptation examining whether AI adoption drivers differ across sectors, and intervention effectiveness studies testing segment-specific approaches. The research roadmap includes developing formal AIRS Score algorithms, diagnostic protocols, and intervention frameworks as subsequent research phases.

## Conclusion

This dissertation addresses the timely challenge of understanding why individuals adopt or resist AI tools in professional contexts. The findings reveal that AI adoption operates through different mechanisms than previous technology adoption, with cost-benefit perceptions and intrinsic enjoyment mattering more than conventional utility considerations. The validated AIRS diagnostic instrument provides researchers with a psychometrically sound foundation for investigating AI adoption, while the 8-factor structure enables practitioners to identify specific adoption barriers and design targeted interventions.

As AI transforms professional work, understanding adoption psychology becomes increasingly critical. This dissertation establishes a validated foundation that enables both research advancement and practical diagnostic applications to help organizations close the persistent gap between AI adoption and value realization.
