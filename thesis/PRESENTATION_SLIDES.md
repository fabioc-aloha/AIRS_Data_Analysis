# Artificial Intelligence Readiness Scale (AIRS): Dissertation Defense

## Artificial Intelligence Readiness Scale: Extending UTAUT2 for Enterprise AI Adoption

Fabio Correa
Doctoral Candidate, Doctor of Business Administration
Touro University Worldwide
December 2025

---

## Slide 1: Title Slide

### Artificial Intelligence Readiness Scale

Extending UTAUT2 for Enterprise AI Adoption

Fabio Correa
Doctoral Candidate, Doctor of Business Administration
Touro University Worldwide

Dissertation Committee:

- Dr. Karina Kasztelnik (Chair)
- Dr. Jerome Jones
- Dr. Donna Day

December 2025

---

## Slide 2: The AI Adoption Paradox

Organizations are adopting AI at unprecedented rates, but struggling to capture value.

The gap between AI adoption and value realization represents one of the most pressing challenges facing organizations today. Despite massive investment and enthusiasm, most AI initiatives fail to deliver measurable returns.

### Chart Data: AI Adoption vs. Value Capture

| Metric | Value | Source |
|--------|-------|--------|
| Enterprise AI Adoption (2024) | 72% | McKinsey |
| Enterprise AI Adoption (2025) | 88% | McKinsey |
| Companies achieving measurable ROI | 5% | BCG |
| GenAI pilots that fail to scale | 90-95% | MIT Media Lab |
| "AI High Performers" | 6% | McKinsey 2025 |

**Key Insight**: Adoption ≠ Value. Understanding the psychology behind this gap is critical.

---

## Slide 3: Research Questions

**This dissertation addresses the fundamental question: Why do people adopt (or resist) AI tools?**

Traditional technology acceptance models like UTAUT have explained technology adoption for decades. But AI is different—it's opaque, probabilistic, and raises unique ethical concerns. Do our existing theories still apply?

**Research Questions:**

1. **RQ1**: Can we develop a psychometrically valid AI Readiness Scale extending UTAUT2?
2. **RQ2**: What factors most strongly predict AI adoption intention?
3. **RQ3**: Do traditional UTAUT predictors behave differently for AI?
4. **RQ4**: How do professional experience and population type moderate adoption?

---

## Slide 4: Theoretical Foundation - UTAUT2

**Building on 25+ years of technology acceptance research.**

The Unified Theory of Acceptance and Use of Technology (UTAUT/UTAUT2) synthesizes decades of research into a unified framework that has been validated across thousands of studies. This dissertation extends this established foundation for the AI context.

**UTAUT2 Core Constructs:**

| Construct | Definition | Meta-Analytic Effect (rc) |
|-----------|------------|--------------------------|
| Performance Expectancy (PE) | Belief technology improves performance | .64 (strongest) |
| Effort Expectancy (EE) | Perceived ease of use | .51 |
| Social Influence (SI) | Important others' opinions | .43 |
| Facilitating Conditions (FC) | Organizational/technical support | .39 |
| Hedonic Motivation (HM) | Enjoyment from use | .53 |
| Price Value (PV) | Cost-benefit assessment | .52 |
| Habit (HB) | Automaticity from repeated use | .66 |

*Source: Blut et al. (2022) meta-analysis, Table 1: 737,112 users, 1,935 samples. rc = corrected correlation.*

---

## Slide 5: Why AI May Be Different

**AI challenges fundamental assumptions of traditional acceptance models.**

Unlike conventional software, AI systems operate through mechanisms that users cannot directly observe or verify. This opacity, combined with probabilistic outputs and ethical implications, may require theoretical extension.

**Unique AI Characteristics:**

| Challenge | Traditional Tech | AI Systems |
|-----------|-----------------|------------|
| Transparency | Deterministic, observable | Opaque, "black box" |
| Outputs | Consistent, predictable | Probabilistic, variable |
| Errors | Traceable, fixable | Difficult to diagnose |
| Ethics | Minimal concern | Job displacement, bias, privacy |
| Trust basis | Reliability | Competence + integrity + benevolence |

**Proposed Extension**: AI Trust (TR) as a new construct capturing trust in AI systems

---

## Slide 6: Research Design

**A rigorous 10-phase psychometric validation approach.**

This study employed a split-sample cross-validation design rarely used in scale development research. The development sample (n=261) was used for exploratory factor analysis, while the holdout sample (n=262) provided independent confirmation.

**Methodology Overview:**

| Phase | Analysis | Sample |
|-------|----------|--------|
| 1 | Sample Split | N=523 -> 261/262 |
| 2 | Exploratory Factor Analysis (EFA) | Development (n=261) |
| 3 | Confirmatory Factor Analysis (CFA) | Holdout (n=262) |
| 4 | Measurement Invariance | Student vs Professional |
| 5 | Structural Equation Modeling (SEM) | Full sample |
| 6 | Mediation Analysis | Full sample |
| 7 | Moderation Analysis | Experience, Population |
| 8 | Behavioral Validation | Tool usage correlation |
| 9 | Qualitative Analysis | Open-ended responses |
| 10 | Final Synthesis | Integration |

---

## Slide 7: Sample Characteristics

**A diverse sample spanning the career development spectrum.**

The sample of 523 U.S. adults includes students, individual contributors, managers, and executives—enabling examination of AI adoption across professional contexts and providing sufficient power for multi-group analyses.

**Sample Demographics Chart Data:**

| Population | n | % |
|------------|---|---|
| Students | 216 | 41.3% |
| Professionals | 184 | 35.2% |
| Leaders | 123 | 23.5% |
| **Total** | **523** | **100%** |

**Additional Demographics:**

| Characteristic | Distribution |
|---------------|--------------|
| Education | Some college 35%, Bachelor's 27%, HS/less 19%, Master's 16%, Doctoral 3% |
| With Disability | 13% (n=68) |
| AI Tool Users | 89% use at least one AI tool |

---

## Slide 8: Instrument Validation Results

**Excellent psychometric properties across all indices.**

The 8-factor, 16-item AIRS instrument demonstrated exceptional model fit on the independent holdout sample, exceeding all conventional thresholds. This provides strong evidence for the reliability and validity of the scale.

**Model Fit Indices:**

| Index | Value | Threshold | Result |
|-------|-------|-----------|--------|
| CFI | .975 | ≥ .95 | [OK] Excellent |
| TLI | .960 | ≥ .95 | [OK] Excellent |
| RMSEA | .065 | ≤ .08 | [OK] Good |
| SRMR | .046 | ≤ .08 | [OK] Excellent |
| χ²/df | 2.10 | < 3.0 | [OK] Excellent |

**Reliability Chart Data:**

| Construct | α | CR | AVE |
|-----------|-----|-----|-----|
| Performance Expectancy | .803 | .804 | .673 |
| Effort Expectancy | .859 | .861 | .756 |
| Social Influence | .752 | .763 | .621 |
| Facilitating Conditions | .743 | .750 | .601 |
| Hedonic Motivation | .864 | .865 | .763 |
| Price Value | .883 | .883 | .790 |
| Habit | .909 | .909 | .833 |
| AI Trust | .891 | .891 | .804 |

---

## Slide 9: Constructs Excluded

**Four AI-specific constructs failed to demonstrate adequate reliability.**

An important finding of this research is that four proposed constructs—despite theoretical importance—could not be reliably measured with two-item scales. This represents a measurement challenge, not a theoretical failure, and guides future research.

**Excluded Constructs:**

| Construct | Cronbach's α | Reason for Exclusion |
|-----------|--------------|---------------------|
| Voluntariness (VO) | .406 | Items measured choice vs. freedom—distinct dimensions |
| Explainability (EX) | .582 | Items measured understanding vs. preference—distinct facets |
| Ethical Risk (ER) | .546 | Items measured job displacement vs. privacy—distinct risk types |
| AI Anxiety (AX) | .301 | Items measured avoidance vs. approach anxiety—distinct motivations |

**Implication**: These constructs require 3-4 items per dimension for future operationalization. The theoretical importance remains; only the measurement proved insufficient.

---

## Slide 10: KEY FINDING - Hypothesis Testing Results

**Price Value, not Performance Expectancy, drives AI adoption.**

This is the most striking finding of the dissertation. In traditional UTAUT research, Performance Expectancy is consistently the strongest predictor. For AI tools, users care more about whether AI is "worth it" than whether it's "useful."

**Structural Model Results - Bar Chart Data:**

| Hypothesis | Path | β | p | Result |
|------------|------|-----|------|--------|
| H1f | PV -> BI | **.505** | <.001 | [OK] **STRONGEST** |
| H1e | HM -> BI | **.217** | .014 | [OK] Supported |
| H1c | SI -> BI | **.136** | .024 | [OK] Supported |
| H2 | TR -> BI | .106 | .064 | [!] Marginal |
| H1d | FC -> BI | .059 | .338 | [X] Not Supported |
| H1g | HB -> BI | .023 | .631 | [X] Not Supported |
| H1b | EE -> BI | -.008 | .875 | [X] Not Supported |
| H1a | PE -> BI | -.028 | .791 | [X] Not Supported |

**Model R² = .852** (85.2% of variance explained, 8-factor diagnostic model)

---

## Slide 11: Traditional Predictors Don't Work for AI

**Performance Expectancy and Effort Expectancy, typically the strongest predictors, are non-significant for AI.**

This finding challenges fundamental assumptions about technology adoption. In Blut et al.'s (2022) meta-analysis of 737,112 users, PE had the strongest effect (rc = .64). In our AI context, PE was essentially zero (β = -.028, p = .791).

### Comparison Chart Data: Traditional UTAUT vs. AI Adoption

| Construct | Meta-Analytic rc (Traditional) | AIRS β (AI) | Change |
|-----------|------------------------------|-------------|--------|
| Performance Expectancy | .64 | -.028 | [DOWN] Collapsed |
| Effort Expectancy | .51 | -.008 | [DOWN] Collapsed |
| Price Value | .52 | **.505** | [UP] Dominant |
| Hedonic Motivation | .53 | .217 | Similar |
| Social Influence | .43 | .136 | [DOWN] Reduced |

**Interpretation**: For AI, utility is assumed or uncertain; users evaluate through a value lens ("Is it worth it?") rather than a utility lens ("Will it help me?").

---

## Slide 12: What Drives AI Adoption?

**Three factors significantly predict AI adoption intention.**

The structural model reveals that AI adoption is driven primarily by cost-benefit perceptions, followed by intrinsic enjoyment and social influence. This pattern suggests organizations should lead with value propositions rather than capability demonstrations.

**Significant Predictors (Ranked):**

1. **Price Value (β = .505, p < .001)**: "Is the value worth the cost/effort?"
   - The cognitive trade-off between benefits received and resources invested
   - Includes time, learning curve, workflow disruption—not just money

2. **Hedonic Motivation (β = .217, p = .014)**: "Is it engaging and enjoyable?"
   - Intrinsic satisfaction from using AI tools
   - Curiosity and stimulation drive continued engagement

3. **Social Influence (β = .136, p = .024)**: "Do important others support AI use?"
   - Peer influence and organizational norms
   - AI champions and visible leadership matter

**Near-Significant:**

- AI Trust (β = .106, p = .064): Approaching significance, larger samples may confirm

---

## Slide 13: Experience Moderates Hedonic Motivation

**As professionals gain experience, enjoyment matters more.**

The moderation analysis revealed a significant interaction between professional experience and Hedonic Motivation. More experienced professionals weight intrinsic satisfaction more heavily when evaluating AI tools.

**Experience Moderation Chart Data:**

| Interaction | β | p | Result |
|-------------|------|------|--------|
| HM × Experience | **.136** | **.009** | [OK] Significant |
| PE × Experience | .112 | .055 | [!] Marginal |
| EE × Experience | .122 | .161 | [X] Not significant |
| TR × Experience | .081 | .145 | [X] Not significant |

**Interpretation**: Early-career professionals may prioritize proving productivity; experienced professionals can afford to value enjoyment. This connects technology acceptance to career development theory in novel ways.

---

## Slide 14: Population Differences

**Students value enjoyment; Professionals value outcomes.**

Multi-group analysis revealed significant population moderation for the Hedonic Motivation path. Students show a strong positive effect of enjoyment on adoption intention, while professionals show a negative effect.

**Population Moderation Chart Data:**

| Path | Student β | Professional β | Δβ | p |
|------|-----------|----------------|-----|------|
| HM -> BI | **+0.449** | -0.301 | -0.750 | **.041** |
| PV -> BI | +0.638 | +0.808 | +0.170 | ns |
| SI -> BI | +0.007 | +0.239 | +0.232 | ns |
| TR -> BI | -0.011 | +0.153 | +0.164 | ns |

**Key Insight**:

- **Students**: "Make it fun and I'll use it"
- **Professionals**: "Show me the value and I'll use it"

Different messaging strategies may be needed for different populations.

---

## Slide 15: Four User Segments Identified

**Cluster analysis reveals distinct adoption readiness profiles.**

K-means clustering identified four user segments with distinct psychological profiles. This typology provides a framework for future research on tailored intervention strategies.

**User Typology Chart Data:**

| Segment | n | % | Profile | Organizational Role |
|---------|---|---|---------|---------------------|
| AI Enthusiasts | 84 | 16% | High trust, high intention, low anxiety | Champions |
| Cautious Adopters | 157 | 30% | High adoption but also high anxiety | Need reassurance |
| Moderate Users | 191 | 37% | Neutral stance, average engagement | Can be influenced |
| Anxious Avoiders | 91 | 17% | Low adoption, high anxiety | Need intervention |

**Cluster Centroids (Standardized):**

| Segment | PE (z) | Trust (z) | Anxiety (z) | Intention (z) |
|---------|--------|-----------|-------------|---------------|
| AI Enthusiasts | +1.42 | +1.37 | -0.86 | +1.32 |
| Cautious Adopters | +1.16 | +0.86 | +0.84 | +0.88 |
| Moderate Users | +0.26 | +0.01 | +0.42 | -0.07 |
| Anxious Avoiders | -1.16 | -1.49 | +0.76 | -1.53 |

---

## Slide 16: Behavioral Validation

**Intentions strongly predict actual AI tool usage.**

To validate the AIRS instrument against real behavior, we correlated Behavioral Intention with self-reported AI tool usage patterns. The strong correlation provides criterion validity evidence.

**Validation Results:**

| Metric | Value |
|--------|-------|
| BI-Usage Correlation | ρ = .69, p < .001 |
| Interpretation | Strong positive relationship |

**Tool Usage by Role (Effect Sizes):**

| Comparison | Cohen's d | Interpretation |
|------------|-----------|----------------|
| Leaders vs. Students | 1.14 | Very large |
| Leaders vs. Professionals | 0.74 | Large |
| Professionals vs. Students | 0.43 | Medium |

**Key Insight**: Organizational leaders demonstrate substantially higher AI tool usage, suggesting leadership engagement may be critical for organizational AI adoption.

---

## Slide 17: Theoretical Contributions

**Four primary contributions to technology acceptance theory.**

This dissertation advances scholarly understanding of technology acceptance in the AI era, providing both theoretical extension and empirical evidence for context-specific modifications.

### Contribution 1: UTAUT2 Extension for AI

- Demonstrated that traditional frameworks require modification for AI contexts
- AI Trust approaches significance, warranting further investigation

### Contribution 2: Price Value Dominance

- First empirical evidence that PV > PE for AI adoption
- Challenges 25+ years of UTAUT findings where PE consistently dominates

### Contribution 3: Career Development Integration

- Experience moderates HM effect (p = .009)
- Connects technology acceptance to vocational psychology

### Contribution 4: User Typology Framework

- Four-segment model for adoption heterogeneity
- Foundation for future intervention research

---

## Slide 18: Practical Implications

**Evidence-informed recommendations for organizations.**

While the cross-sectional design limits causal claims, the findings suggest several hypotheses for organizational practice that warrant experimental validation.

**For AI Implementation:**

| Finding | Implication | Strategy |
|---------|-------------|----------|
| PV dominance (β=.505) | Lead with value, not features | Clear ROI demonstrations |
| HM significance (β=.217) | Design for engagement | Gamification, curiosity |
| SI significance (β=.136) | Leverage social proof | Champions, peer communities |
| PE non-significance | Don't assume utility sells | Focus on value proposition |

**For Different Populations:**

| Population | Priority | Approach |
|------------|----------|----------|
| Students | Hedonic Motivation | Make it engaging and fun |
| Professionals | Price Value | Demonstrate clear ROI |
| Leaders | Already high adopters | Leverage as champions |

---

## Slide 19: Study Limitations

**Important boundaries for interpretation.**

As with all empirical research, these findings should be interpreted within the context of methodological limitations that guide future research directions.

**Design Limitations:**

| Limitation | Impact | Mitigation |
|------------|--------|------------|
| Cross-sectional design | Cannot establish causation | Longitudinal replication needed |
| Panel sampling | Limits generalizability | Topic-blinded recruitment via Centiment mitigates self-selection |
| Self-report measures | Common method variance | Behavioral validation included |
| U.S. only | Cultural specificity | International replication needed |

**Measurement Limitations:**

| Limitation | Impact | Future Direction |
|------------|--------|------------------|
| 4 constructs dropped | Incomplete theoretical extension | 3-4 items per dimension |
| 2 items per construct | Minimal measurement | Expand to 3-4 items |
| Trust marginal (p=.064) | Underpowered for small effects | n > 600 recommended |

---

## Slide 20: Future Research Agenda

**A systematic roadmap from validated scale to organizational applications.**

This dissertation establishes the validated AIRS instrument as a foundation for a multi-phase research program addressing the adoption-value gap.

**Research Roadmap:**

| Phase | Focus | Status |
|-------|-------|--------|
| Phase 0 | AIRS Scale Validation | [OK] THIS DISSERTATION |
| Phase 1 | AIRS Score Algorithm Development | Future Research |
| Phase 2 | Diagnostic Protocol Development | Future Research |
| Phase 3 | Intervention Framework Testing | Future Research |
| Phase 4 | Comprehensive AI Readiness Ecosystem | Future Research |

**Immediate Research Priorities:**

1. Replication with larger samples (n > 600) for small effect detection
2. Longitudinal design tracking intention -> behavior over 6-12 months
3. Redesign measures for excluded constructs (EX, ER, AX, VO)
4. Cross-cultural validation in collectivist cultures
5. Industry-specific adaptation studies

---

## Slide 21: The Validated AIRS Instrument

**A 16-item psychometrically sound scale for AI adoption research.**

The final AIRS instrument consists of 8 constructs measured with 2 items each, using a 5-point Likert scale (1 = Strongly Disagree to 5 = Strongly Agree).

**AIRS 16-Item Scale:**

| Construct | Item 1 | Item 2 |
|-----------|--------|--------|
| PE | AI tools help me accomplish tasks more quickly | Using AI improves the quality of my work |
| EE | Learning to use AI tools is easy for me | Interacting with AI tools is clear and understandable |
| SI | People whose opinions I value encourage AI use | Leaders in my organization support AI use |
| FC | I have access to training for AI tools | AI tools are compatible with other systems I use |
| HM | Using AI tools is stimulating and engaging | AI tools make my work more interesting |
| PV | I get more value from AI than the effort required | Using AI is worth the learning curve |
| HB | Using AI tools has become a habit for me | I tend to rely on AI tools by default |
| TR | I trust AI tools to provide reliable information | I trust the AI tools available to me |

---

## Slide 22: Key Takeaways

**What we learned about AI adoption psychology.**

This dissertation provides empirical evidence that AI represents a psychologically distinct technology category, requiring modified theoretical frameworks and differentiated organizational strategies.

**Five Key Findings:**

1. **Price Value dominates** (β = .505) — Users evaluate AI through a value lens, not a utility lens

2. **Performance Expectancy collapses** (β = -.028, ns) — Traditional utility assumptions don't apply to AI

3. **Experience moderates enjoyment** (p = .007) — Career development connects to technology acceptance

4. **Four user segments exist** — Heterogeneous adoption readiness requires tailored approaches

5. **The AIRS instrument is validated** — CFI = .975, R² = .861, strong psychometric properties

---

## Slide 23: Conclusion

**Bridging the gap between AI adoption and value realization.**

As AI transforms professional work, understanding adoption psychology becomes critical. This dissertation establishes that AI adoption operates through different mechanisms than previous technology adoption, with cost-benefit perceptions and intrinsic enjoyment mattering more than conventional utility considerations.

**Summary Statement:**

The validated AIRS instrument provides researchers with a psychometrically sound foundation for investigating AI adoption. The finding that Price Value—not Performance Expectancy—dominates AI adoption intention represents a significant theoretical departure that challenges fundamental assumptions about technology acceptance.

Organizations seeking to close the adoption-value gap should focus on demonstrating clear return on investment, designing engaging user experiences, and leveraging social influence through visible AI champions.

**Final Thought**: Understanding why people adopt AI is the first step toward ensuring AI adoption creates genuine value.

---

## Slide 24: Questions & Discussion

**Thank you for your attention.**

Fabio Correa
Doctoral Candidate, Doctor of Business Administration
Touro University Worldwide

**Contact Information:**

- Email: <fabio@correax.com>
- LinkedIn: [your profile]
- GitHub: github.com/fabioc-aloha/AIRS_Data_Analysis

**Resources:**

- Full dissertation available upon request
- AIRS instrument available for research use (CC BY 4.0)
- Analysis notebooks: Open source (MIT License)

---

## Appendix Slides

### Appendix A: Data Retention & Ethics

#### IRB Approval and Data Protection

This research was conducted with full IRB approval, ensuring ethical treatment of participants and proper data handling throughout the study.

**Ethics Summary:**

| Aspect | Details |
|--------|--------|
| IRB Status | Approved (Exempt Category 2) |
| Consent | Electronic informed consent obtained |
| PII Collected | None (no names, emails, IP addresses) |
| Data Format | Anonymized, de-identified |
| Retention Period | Until January 2028 (3 years per 45 CFR 46) |
| Public Availability | GitHub repository (MIT/CC BY 4.0 licenses) |

**Data Access:**

- **Repository**: github.com/fabioc-aloha/AIRS_Data_Analysis
- **Data File**: `data/AIRS_clean.csv` (N=523)
- **Analysis**: 10 Jupyter notebooks, fully reproducible
- **Thesis**: Complete dissertation in Markdown + PDF

---

### Appendix B: Complete Hypothesis Summary

| # | Hypothesis | β | p | Result |
|---|------------|---|---|--------|
| H1a | PE -> BI | -.028 | .791 | [X] Not Supported |
| H1b | EE -> BI | -.008 | .875 | [X] Not Supported |
| H1c | SI -> BI | .136 | .024 | [OK] Supported |
| H1d | FC -> BI | .059 | .338 | [X] Not Supported |
| H1e | HM -> BI | .217 | .014 | [OK] Supported |
| H1f | PV -> BI | .505 | <.001 | [OK] Supported (Strongest) |
| H1g | HB -> BI | .023 | .631 | [X] Not Supported |
| H2 | TR -> BI | .106 | .064 | [!] Marginal |
| H3 | AIRS > UTAUT2 | ΔAIC +2.01 | — | [X] Not Supported |
| H4 | BI ↔ Usage | ρ = .69 | <.001 | [OK] Supported |

Summary: 3/7 UTAUT paths supported, Trust marginal, Behavioral validation confirmed

### Appendix C: Model Fit Comparison

| Model | Factors | Items | CFI | TLI | RMSEA | SRMR | Selected |
|-------|---------|-------|-----|-----|-------|------|----------|
| A | 7 | 21 | .938 | .923 | .078 | .058 | No |
| B | 8 | 20 | .952 | .940 | .070 | .052 | No |
| C | 8 | 18 | .964 | .953 | .066 | .048 | No |
| **D** | **8** | **16** | **.975** | **.960** | **.065** | **.046** | **Yes** |

### Appendix D: Bibliography Highlights

- **92 references** spanning technology acceptance, AI adoption, scale development, and psychometrics
- Key foundational sources:
  - Venkatesh et al. (2003, 2012) — UTAUT/UTAUT2
  - Blut et al. (2022) — UTAUT meta-analysis
  - Hair et al. (2019) — Multivariate data analysis
  - DeVellis & Thorpe (2021) — Scale development
- Industry sources:
  - McKinsey State of AI (2024, 2025)
  - BCG AI Adoption (2024, 2025)
  - MIT Media Lab NANDA Initiative

---

<!-- End of Presentation -->
