{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1385c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AIRS SAMPLE PREPARATION - FULL DATASET\n",
      "======================================================================\n",
      "Random seed: 67\n",
      "ğŸ’ Split ratio: 50/50 (dev/holdout)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 0: IMPORTS AND CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# \n",
    "# This notebook processes the ENTIRE dataset and creates:\n",
    "# 1. Master clean dataset (AIRS_clean.csv)\n",
    "# 2. Population-specific dev/holdout splits for ALL populations\n",
    "# 3. Item metadata (airs_28item_complete.json)\n",
    "#\n",
    "# Population filtering for specific analyses is done in 00b_Prepare_Experiment_Data.ipynb\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Prevent OpenMP conflicts\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ğŸ’ DECISION POINT: DEV/HOLDOUT SPLIT RATIO                                  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 70/30 = More EFA power, smaller CFA holdout (bootstrap recommended)\n",
    "# 50/50 = Balanced, adequate for both EFA and CFA\n",
    "\n",
    "HOLDOUT_RATIO = 0.50  # Options: 0.30 (70/30) or 0.50 (50/50)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "RANDOM_SEED = 67\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create output directory\n",
    "Path('./data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dev_pct = int((1 - HOLDOUT_RATIO) * 100)\n",
    "holdout_pct = int(HOLDOUT_RATIO * 100)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AIRS SAMPLE PREPARATION - FULL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"ğŸ’ Split ratio: {dev_pct}/{holdout_pct} (dev/holdout)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4a52a",
   "metadata": {},
   "source": [
    "## 1. Item Semantic Metadata\n",
    "\n",
    "Define all 28 items (24 predictors + 4 BI outcome items) with theoretical metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a81edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITEM SEMANTIC METADATA\n",
      "======================================================================\n",
      "Total items: 28\n",
      "  Predictor items: 24 (12 constructs Ã— 2 items)\n",
      "  Outcome items: 4 (BI1-BI4)\n",
      "\n",
      "Direction counts:\n",
      "  POSITIVE: 25\n",
      "  NEGATIVE: 3\n",
      "\n",
      "Analysis Structure:\n",
      "  EFA: 12 constructs (data-driven)\n",
      "  CFA: 5 theoretical domains (second-order)\n",
      "\n",
      "Constructs (12 for EFA): ['PE', 'EE', 'SI', 'FC', 'HM', 'PV', 'HB', 'VO', 'TR', 'EX', 'ER', 'AX', 'BI']\n",
      "\n",
      "Theoretical Domains (5 for CFA):\n",
      "  Utility_Beliefs: ['PE', 'EE', 'PV'] â†’ ['PE1', 'PE2', 'EE1', 'EE2', 'PV1', 'PV2']\n",
      "  Social_Environmental: ['SI', 'FC'] â†’ ['SI1', 'SI2', 'FC1', 'FC2']\n",
      "  Intrinsic_Motivation: ['HM', 'HB', 'VO'] â†’ ['HM1', 'HM2', 'HB1', 'HB2', 'VO1', 'VO2']\n",
      "  Trust_Transparency: ['TR', 'EX'] â†’ ['TR1', 'TR2', 'EX1', 'EX2']\n",
      "  Risk_Anxiety: ['ER', 'AX'] â†’ ['ER1', 'ER2', 'AX1', 'AX2']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 1: ITEM SEMANTIC METADATA (28 items: 24 predictors + 4 BI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Theoretical domains (higher-order groupings for CFA)\n",
    "# EFA: Let 12 constructs emerge from data (2 items each)\n",
    "# CFA: Test second-order structure by grouping constructs into 5 theoretical domains\n",
    "THEORETICAL_DOMAINS = {\n",
    "    'Utility_Beliefs': ['PE', 'EE', 'PV'],        # Perceived usefulness/ease/value\n",
    "    'Social_Environmental': ['SI', 'FC'],         # External enablers\n",
    "    'Intrinsic_Motivation': ['HM', 'HB', 'VO'],   # Internal drivers\n",
    "    'Trust_Transparency': ['TR', 'EX'],           # AI-specific enablers\n",
    "    'Risk_Anxiety': ['ER', 'AX'],                 # Barriers (negative valence)\n",
    "}\n",
    "\n",
    "# Format: (code, construct, hypothesis, question, direction, note, provenance)\n",
    "items_data = [\n",
    "    # UTAUT2 Core Constructs (H1)\n",
    "    ('PE1', 'Performance Expectancy', 'H1', 'AI tools help me accomplish tasks more quickly.', 'POSITIVE', 'Perceived productivity gain', 'Direct (UTAUT)'),\n",
    "    ('PE2', 'Performance Expectancy', 'H1', 'Using AI improves the quality of my work or studies.', 'POSITIVE', 'Perceived quality improvement', 'Direct (UTAUT)'),\n",
    "    ('EE1', 'Effort Expectancy', 'H1', 'Learning to use AI tools is easy for me.', 'POSITIVE', 'Perceived ease of learning', 'Direct (UTAUT)'),\n",
    "    ('EE2', 'Effort Expectancy', 'H1', 'Interacting with AI tools is clear and understandable.', 'POSITIVE', 'Perceived clarity of interaction', 'Direct (UTAUT)'),\n",
    "    ('SI1', 'Social Influence', 'H1', 'People whose opinions I value encourage me to use AI tools.', 'POSITIVE', 'Social encouragement from valued others', 'Direct (UTAUT)'),\n",
    "    ('SI2', 'Social Influence', 'H1', 'Leaders in my organization or school support the use of AI tools.', 'POSITIVE', 'Leadership support and organizational norms', 'Adapted (UTAUT)'),\n",
    "    ('FC1', 'Facilitating Conditions', 'H1', 'I have access to training or tutorials for the AI tools I use.', 'POSITIVE', 'Resource availability and support', 'Adapted (UTAUT)'),\n",
    "    ('FC2', 'Facilitating Conditions', 'H1', 'The AI tools I use are compatible with other tools or systems I use.', 'POSITIVE', 'System compatibility and integration', 'Direct (UTAUT)'),\n",
    "    ('HM1', 'Hedonic Motivation', 'H1', 'Using AI tools is stimulating and engaging.', 'POSITIVE', 'Intrinsic enjoyment and stimulation', 'Direct (UTAUT2)'),\n",
    "    ('HM2', 'Hedonic Motivation', 'H1', 'AI tools make my work or studies more interesting.', 'POSITIVE', 'Enhanced interest and engagement', 'Direct (UTAUT2)'),\n",
    "    ('PV1', 'Price Value', 'H1', 'I get more value from AI tools than the effort they require.', 'POSITIVE', 'Perceived value-effort tradeoff', 'Adapted (UTAUT2) - effort substituted for monetary cost'),\n",
    "    ('PV2', 'Price Value', 'H1', 'Using AI tools is worth the learning curve.', 'POSITIVE', 'Acceptable cost-benefit perception', 'Adapted (UTAUT2) - effort substituted for monetary cost'),\n",
    "    ('HB1', 'Habit', 'H1', 'Using AI tools has become a habit for me.', 'POSITIVE', 'Habitual usage pattern', 'Direct (UTAUT2)'),\n",
    "    ('HB2', 'Habit', 'H1', 'I tend to rely on AI tools by default when I need help with tasks.', 'POSITIVE', 'Default reliance behavior', 'Adapted (UTAUT2)'),\n",
    "    ('VO1', 'Voluntariness', 'H1', 'I choose to use AI tools in my work because I find them helpful, not because I am required to.', 'POSITIVE', 'Autonomous motivation and choice', 'UTAUT2 extension'),\n",
    "    ('VO2', 'Voluntariness', 'H1', 'I could choose not to use AI tools in my work or studies if I preferred.', 'POSITIVE', 'Perceived freedom of choice', 'UTAUT2 extension'),\n",
    "    \n",
    "    # AI-Specific Constructs (H2)\n",
    "    ('TR1', 'Trust in AI', 'H2', 'I trust AI tools to provide reliable information.', 'POSITIVE', 'Trust in reliability and accuracy', 'Adapted (AI literature - Langer et al., 2023; Siau & Wang, 2018)'),\n",
    "    ('TR2', 'Trust in AI', 'H2', 'I trust the AI tools that are available to me.', 'POSITIVE', 'General trust in available AI systems', 'Adapted (AI literature - Langer et al., 2023)'),\n",
    "    ('EX1', 'Explainability', 'H2', 'I understand how the AI tools I use generate their outputs.', 'POSITIVE', 'Perceived transparency and understanding', 'Adapted (AI literature - Doshi-Velez & Kim, 2017)'),\n",
    "    ('EX2', 'Explainability', 'H2', 'I prefer AI tools that explain their recommendations.', 'POSITIVE', 'Preference for explanation and justification', 'Adapted (AI literature - Guidotti et al., 2018; Shin, 2021)'),\n",
    "    ('ER1', 'Perceived Ethical Risk', 'H2', 'I worry that AI tools could replace jobs in my field.', 'NEGATIVE', 'Job threat anxiety (avoidance motivation)', 'Adapted (AI literature - Floridi et al., 2018)'),\n",
    "    ('ER2', 'Perceived Ethical Risk', 'H2', 'I am concerned about privacy risks when using AI tools.', 'NEGATIVE', 'Privacy and data governance concerns', 'Adapted (AI literature - AI ethics)'),\n",
    "    ('AX1', 'AI Anxiety', 'H2', 'I feel uneasy about the increasing use of AI.', 'NEGATIVE', 'Tech-averse anxiety (barrier)', 'Adapted (AI literature - Bendel, 2021)'),\n",
    "    ('AX2', 'AI Anxiety', 'H2', 'I worry that I may be left behind if I do not keep up with AI.', 'POSITIVE', 'FOMO anxiety (approach motivation / catch-up behavior)', 'Adapted (AI literature - obsolescence anxiety)'),\n",
    "    \n",
    "    # Behavioral Intention (Outcome)\n",
    "    ('BI1', 'Behavioral Intention', 'OUTCOME', 'I am ready to use more AI tools in my work or studies.', 'POSITIVE', 'Readiness to adopt', 'Direct (UTAUT2)'),\n",
    "    ('BI2', 'Behavioral Intention', 'OUTCOME', 'I would recommend AI tools to others.', 'POSITIVE', 'Advocacy intention', 'Direct (UTAUT2)'),\n",
    "    ('BI3', 'Behavioral Intention', 'OUTCOME', 'I see AI as an important part of my future.', 'POSITIVE', 'Future integration intention', 'Adapted (UTAUT2)'),\n",
    "    ('BI4', 'Behavioral Intention', 'OUTCOME', 'I plan to increase my use of AI tools in the next six months.', 'POSITIVE', 'Planned usage increase', 'Direct (UTAUT2)'),\n",
    "]\n",
    "\n",
    "# Helper to find theoretical domain for a construct\n",
    "def get_theoretical_domain(construct_abbr):\n",
    "    for domain, constructs in THEORETICAL_DOMAINS.items():\n",
    "        if construct_abbr in constructs:\n",
    "            return domain\n",
    "    return 'Outcome' if construct_abbr == 'BI' else 'Unknown'\n",
    "\n",
    "# Generate metadata dictionary\n",
    "item_semantic_metadata = {}\n",
    "for code, construct, hypothesis, question, direction, note, provenance in items_data:\n",
    "    construct_abbr = code[:2]\n",
    "    \n",
    "    if direction == 'POSITIVE':\n",
    "        bi_rel = 'Higher scores â†’ Higher adoption'\n",
    "    elif direction == 'NEGATIVE':\n",
    "        bi_rel = 'Higher scores â†’ LOWER adoption'\n",
    "    else:\n",
    "        bi_rel = 'Outcome variable'\n",
    "    \n",
    "    item_semantic_metadata[code] = {\n",
    "        'item_code': code,\n",
    "        'construct': construct,\n",
    "        'construct_abbr': construct_abbr,\n",
    "        'theoretical_domain': get_theoretical_domain(construct_abbr),\n",
    "        'hypothesis': hypothesis,\n",
    "        'question_text': question,\n",
    "        'direction': direction,\n",
    "        'expected_BI_relationship': bi_rel if hypothesis != 'OUTCOME' else 'Outcome variable',\n",
    "        'theoretical_note': note,\n",
    "        'provenance': provenance\n",
    "    }\n",
    "\n",
    "# Derive construct groups (12 fine-grained for EFA)\n",
    "construct_groups = {}\n",
    "for item_code, meta in item_semantic_metadata.items():\n",
    "    abbr = meta['construct_abbr']\n",
    "    construct_groups.setdefault(abbr, []).append(item_code)\n",
    "\n",
    "# Derive domain groups (5 coarse for second-order CFA + outcome)\n",
    "domain_groups = {}\n",
    "for item_code, meta in item_semantic_metadata.items():\n",
    "    domain = meta['theoretical_domain']\n",
    "    domain_groups.setdefault(domain, []).append(item_code)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ITEM SEMANTIC METADATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total items: {len(item_semantic_metadata)}\")\n",
    "print(f\"  Predictor items: 24 (12 constructs Ã— 2 items)\")\n",
    "print(f\"  Outcome items: 4 (BI1-BI4)\")\n",
    "print(f\"\\nDirection counts:\")\n",
    "print(f\"  POSITIVE: {sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'POSITIVE')}\")\n",
    "print(f\"  NEGATIVE: {sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'NEGATIVE')}\")\n",
    "print(f\"\\nAnalysis Structure:\")\n",
    "print(f\"  EFA: 12 constructs (data-driven)\")\n",
    "print(f\"  CFA: 5 theoretical domains (second-order)\")\n",
    "print(f\"\\nConstructs (12 for EFA): {list(construct_groups.keys())}\")\n",
    "print(f\"\\nTheoretical Domains (5 for CFA):\")\n",
    "for domain, constructs in THEORETICAL_DOMAINS.items():\n",
    "    items = domain_groups.get(domain, [])\n",
    "    print(f\"  {domain}: {constructs} â†’ {items}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6cf01",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef7f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded: 513 responses Ã— 53 columns\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2: LOAD RAW DATA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Input file is in parent data/ folder (../data/)\n",
    "# Output files go to local ./data/ subfolder\n",
    "input_path = '../data/AIRS---AI-Readiness-Scale-labels.csv'\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "\n",
    "# Load raw data (skip first 2 metadata rows)\n",
    "df_raw = pd.read_csv(input_path, skiprows=2)\n",
    "\n",
    "print(f\"Raw data loaded: {len(df_raw)} responses Ã— {len(df_raw.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78c400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns mapped: 39 of 39 expected\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2.1: COLUMN MAPPING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "column_mapping = {\n",
    "    # Administrative\n",
    "    'Duration (seconds)': 'Duration_seconds',\n",
    "    'Progress': 'Progress',\n",
    "    \n",
    "    # UTAUT2 Core Items (Q4-Q19)\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | AI tools help me accomplish tasks more quickly': 'PE1',\n",
    "    'Performance Expectancy: How much do you agree with these statements about how AI tools help you get things done? | Using AI improves the quality of my work or studies': 'PE2',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Learning to use AI tools is easy for me': 'EE1',\n",
    "    'Effort Expectancy: How much do you agree with these statements about how easy AI tools are to learn and use? | Interacting with AI tools is clear and understandable': 'EE2',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | People whose opinions I value encourage me to use AI tools': 'SI1',\n",
    "    'Social Influence: How much do you agree with these statements about the people around you and their views on using AI? | Leaders in my organization or school support the use of AI tools': 'SI2',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | I have access to training or tutorials for the AI tools I use': 'FC1',\n",
    "    'Facilitating Conditions: How much do you agree with these statements about the resources and support you have for using AI? | The AI tools I use are compatible with other tools or systems I use': 'FC2',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | Using AI tools is stimulating and engaging': 'HM1',\n",
    "    'Hedonic Motivation (Perceived Enjoyment): How much do you agree with these statements about enjoyment when using AI tools? | AI tools make my work or studies more interesting': 'HM2',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | I get more value from AI tools than the effort they require': 'PV1',\n",
    "    'Price Value: How much do you agree with these statements about whether using AI is worth your time and effort? | Using AI tools is worth the learning curve': 'PV2',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | Using AI tools has become a habit for me': 'HB1',\n",
    "    'Habit: How much do you agree with these statements about your habits with AI tools? | I tend to rely on AI tools by default when I need help with tasks': 'HB2',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I choose to use AI tools in my work because I find them helpful, not because I am required to': 'VO1',\n",
    "    'Voluntariness: How much do you agree with these statements about your freedom to choose whether or not to use AI tools? | I could choose not to use AI tools in my work or studies if I preferred.': 'VO2',\n",
    "    \n",
    "    # AI-Specific Items (Q20-Q27)\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust AI tools to provide reliable information': 'TR1',\n",
    "    'Trust in AI: How much do you agree with these statements about trusting AI tools? | I trust the AI tools that are available to me': 'TR2',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I understand how the AI tools I use generate their outputs': 'EX1',\n",
    "    'Explainability: How much do you agree with these statements about understanding how AI tools make their recommendations? | I prefer AI tools that explain their recommendations': 'EX2',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I worry that AI tools could replace jobs in my field': 'ER1',\n",
    "    'Perceived Ethical Risk: How much do you agree with these statements about possible risks of AI? | I am concerned about privacy risks when using AI tools': 'ER2',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I feel uneasy about the increasing use of AI': 'AX1',\n",
    "    'AI Anxiety: How much do you agree with these statements about feeling uneasy or anxious about AI? | I worry that I may be left behind if I do not keep up with AI': 'AX2',\n",
    "    \n",
    "    # Behavioral Intention / Outcome (Q28-Q31)\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I am ready to use more AI tools in my work or studies': 'BI1',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I would recommend AI tools to others': 'BI2',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I see AI as an important part of my future': 'BI3',\n",
    "    'AI Adoption Readiness: How much do you agree with these statements about your readiness to use AI? | I plan to increase my use of AI tools in the next six months': 'BI4',\n",
    "    \n",
    "    # Usage Frequency\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Microsoft 365 Copilot or Microsoft Copilot': 'Usage_MSCopilot',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | ChatGPT': 'Usage_ChatGPT',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Google Gemini': 'Usage_Gemini',\n",
    "    'Usage Frequency: How often do you use the following AI tools? | Other AI tools (for example, Claude, Perplexity, Grok)': 'Usage_Other',\n",
    "    \n",
    "    # Demographics\n",
    "    'What is your current status?': 'Role',\n",
    "    'What is your highest level of education completed?': 'Education',\n",
    "    'Which industry or field best describes your primary area of work or study?': 'Industry',\n",
    "    'How many years of work or study experience do you have in your field?': 'Experience',\n",
    "    'Do you identify as a person with a disability (for example, vision, mobility, neurodivergence)?': 'Disability'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df = df_raw.rename(columns=column_mapping)\n",
    "available_cols = [col for col in column_mapping.values() if col in df.columns]\n",
    "df = df[available_cols].copy()\n",
    "\n",
    "print(f\"Columns mapped: {len(available_cols)} of {len(column_mapping)} expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af19048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types converted:\n",
      "  - 28 Likert items â†’ numeric (1-5)\n",
      "  - 4 Usage items â†’ numeric (1-5)\n",
      "  - Demographics cleaned\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2.2: DATA TYPE CONVERSIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Duration to minutes (handle re-runs gracefully)\n",
    "if 'Duration_seconds' in df.columns:\n",
    "    df['Duration_minutes'] = df['Duration_seconds'] / 60\n",
    "    df = df.drop('Duration_seconds', axis=1)\n",
    "\n",
    "# Likert items (text to numeric 1-5) - All 28 items\n",
    "likert_items = ['PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2', \n",
    "                'HM1', 'HM2', 'PV1', 'PV2', 'HB1', 'HB2', 'VO1', 'VO2',\n",
    "                'TR1', 'TR2', 'EX1', 'EX2', 'ER1', 'ER2', 'AX1', 'AX2',\n",
    "                'BI1', 'BI2', 'BI3', 'BI4']\n",
    "\n",
    "likert_mapping = {\n",
    "    'Strongly disagree': 1, 'Disagree': 2, 'Neutral': 3, \n",
    "    'Agree': 4, 'Strongly agree': 5\n",
    "}\n",
    "\n",
    "for item in likert_items:\n",
    "    if item in df.columns:\n",
    "        df[item] = df[item].map(likert_mapping)\n",
    "\n",
    "# Usage items (text to numeric 1-5)\n",
    "usage_items = ['Usage_MSCopilot', 'Usage_ChatGPT', 'Usage_Gemini', 'Usage_Other']\n",
    "usage_mapping = {'Never': 1, 'Rarely': 2, 'Sometimes': 3, 'Often': 4, 'Daily': 5}\n",
    "\n",
    "for item in usage_items:\n",
    "    if item in df.columns:\n",
    "        df[item] = df[item].map(usage_mapping)\n",
    "\n",
    "# Clean demographics (only string columns)\n",
    "for col in ['Role', 'Education', 'Experience', 'Disability', 'Industry']:\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "print(\"Data types converted:\")\n",
    "print(f\"  - 28 Likert items â†’ numeric (1-5)\")\n",
    "print(f\"  - 4 Usage items â†’ numeric (1-5)\")\n",
    "print(f\"  - Demographics cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b2f4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived variables created:\n",
      "  - AI_Adoption: 459 adopters, 54 non-adopters\n",
      "  - Population Distribution:\n",
      "      Academic: N=198\n",
      "      Professional: N=161\n",
      "      Leader: N=130\n",
      "      EXCLUDED ('Other' only): N=24\n",
      "  - Disability_Binary: Yes=69, No=444\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2.3: DERIVED VARIABLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# AI Adoption (binary)\n",
    "df['AI_Adoption'] = ((df['Usage_MSCopilot'] > 1) | \n",
    "                     (df['Usage_ChatGPT'] > 1) | \n",
    "                     (df['Usage_Gemini'] > 1) | \n",
    "                     (df['Usage_Other'] > 1)).astype(int)\n",
    "\n",
    "# Total usage score\n",
    "df['Total_Usage_Score'] = df[usage_items].sum(axis=1)\n",
    "\n",
    "# Usage Intensity\n",
    "def map_usage_intensity(score):\n",
    "    if score <= 4: return 'Non-User'\n",
    "    elif score <= 8: return 'Low'\n",
    "    elif score <= 12: return 'Medium'\n",
    "    else: return 'High'\n",
    "\n",
    "df['Usage_Intensity'] = df['Total_Usage_Score'].apply(map_usage_intensity)\n",
    "\n",
    "# Population flag - THREE DISTINCT POPULATIONS:\n",
    "# - Academic: Full-time + Part-time students (N=196)\n",
    "# - Professional: Individual Contributors + Freelancers + Not currently employed (N=161)\n",
    "# - Leader: Managers + Executives (N=130)\n",
    "# EXCLUDED: \"Other\" only (N=24)\n",
    "def map_population(role):\n",
    "    if pd.isna(role):\n",
    "        return None  # Exclude\n",
    "    role_lower = str(role).lower().strip()\n",
    "    \n",
    "    # Academic = Full-time + Part-time students\n",
    "    if 'student' in role_lower:\n",
    "        return 'Academic'\n",
    "    # Leader = Managers + Executives\n",
    "    elif 'manager' in role_lower or 'executive' in role_lower or 'leader' in role_lower:\n",
    "        return 'Leader'\n",
    "    # Professional = Individual contributors + Freelancers + Not currently employed\n",
    "    elif 'individual contributor' in role_lower or 'freelancer' in role_lower or 'self employed' in role_lower or 'not currently employed' in role_lower:\n",
    "        return 'Professional'\n",
    "    # Exclude: \"Other\" only\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Population'] = df['Role'].apply(map_population)\n",
    "\n",
    "# Also create a detailed role category for reporting\n",
    "def map_role_category(role):\n",
    "    if pd.isna(role):\n",
    "        return 'Other'\n",
    "    role_lower = str(role).lower().strip()\n",
    "    if 'full time student' in role_lower:\n",
    "        return 'FT_Student'\n",
    "    elif 'part time student' in role_lower:\n",
    "        return 'PT_Student'\n",
    "    elif 'individual contributor' in role_lower:\n",
    "        return 'IC'\n",
    "    elif 'manager' in role_lower:\n",
    "        return 'Manager'\n",
    "    elif 'executive' in role_lower or 'leader' in role_lower:\n",
    "        return 'Executive'\n",
    "    elif 'freelancer' in role_lower or 'self employed' in role_lower:\n",
    "        return 'Freelancer'\n",
    "    elif 'not currently employed' in role_lower:\n",
    "        return 'Unemployed'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['Role_Category'] = df['Role'].apply(map_role_category)\n",
    "\n",
    "# Disability (binary: Yes=1, No=0, Prefer not to answer=0)\n",
    "# Per ANALYSIS_PLAN: \"Prefer not to answer\" coded as No (0) to preserve sample size\n",
    "df['Disability_Binary'] = df['Disability'].map({'Yes': 1, 'No': 0, 'Prefer not to answer': 0})\n",
    "\n",
    "# Report population counts\n",
    "pop_counts = df['Population'].value_counts(dropna=False)\n",
    "excluded = df['Population'].isna().sum()\n",
    "\n",
    "print(\"Derived variables created:\")\n",
    "print(f\"  - AI_Adoption: {df['AI_Adoption'].sum()} adopters, {(~df['AI_Adoption'].astype(bool)).sum()} non-adopters\")\n",
    "print(f\"  - Population Distribution:\")\n",
    "for pop, n in pop_counts.items():\n",
    "    if pop is not None:\n",
    "        print(f\"      {pop}: N={n}\")\n",
    "print(f\"      EXCLUDED ('Other' only): N={excluded}\")\n",
    "print(f\"  - Disability_Binary: Yes={df['Disability_Binary'].sum()}, No={(df['Disability_Binary']==0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410a8dc",
   "metadata": {},
   "source": [
    "## 3. Sample Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5baf60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAMPLE CHARACTERISTICS\n",
      "======================================================================\n",
      "\n",
      "Total N: 513\n",
      "\n",
      "1. Population Distribution:\n",
      "   Academic: N=198 (38.6%)\n",
      "   Professional: N=161 (31.4%)\n",
      "   Leader: N=130 (25.3%)\n",
      "\n",
      "2. Role Breakdown:\n",
      "   Full time student: N=182\n",
      "   Employed - individual contributor: N=113\n",
      "   Employed - manager: N=74\n",
      "   Employed - executive or leader: N=56\n",
      "   Freelancer or self employed: N=31\n",
      "   Other: N=24\n",
      "   Not currently employed: N=17\n",
      "   Part time student: N=16\n",
      "\n",
      "3. AI Adoption:\n",
      "   Adopters: 459 (89.5%)\n",
      "   Non-Adopters: 54 (10.5%)\n",
      "\n",
      "4. Disability Status (for H4f/H4g moderation):\n",
      "   No: N=434 (84.6%)\n",
      "   Yes: N=69 (13.5%)\n",
      "   Prefer not to answer: N=10 (1.9%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 3: SAMPLE OVERVIEW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE CHARACTERISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal N: {len(df)}\")\n",
    "\n",
    "print(\"\\n1. Population Distribution:\")\n",
    "pop_dist = df['Population'].value_counts()\n",
    "for pop, n in pop_dist.items():\n",
    "    print(f\"   {pop}: N={n} ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n2. Role Breakdown:\")\n",
    "role_dist = df['Role'].value_counts()\n",
    "for role, n in role_dist.items():\n",
    "    print(f\"   {role}: N={n}\")\n",
    "\n",
    "print(\"\\n3. AI Adoption:\")\n",
    "print(f\"   Adopters: {df['AI_Adoption'].sum()} ({df['AI_Adoption'].mean()*100:.1f}%)\")\n",
    "print(f\"   Non-Adopters: {(~df['AI_Adoption'].astype(bool)).sum()} ({(1-df['AI_Adoption'].mean())*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. Disability Status (for H4f/H4g moderation):\")\n",
    "dis_dist = df['Disability'].value_counts()\n",
    "for status, n in dis_dist.items():\n",
    "    print(f\"   {status}: N={n} ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f8916",
   "metadata": {},
   "source": [
    "## 4. Save Master Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd138139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASTER DATASET SAVED\n",
      "======================================================================\n",
      "File: airs_experiment/data/AIRS_clean.csv\n",
      "Rows: 513\n",
      "Columns: 45\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 4: SAVE MASTER DATASET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Save master clean dataset (in data/ subfolder)\n",
    "df.to_csv('./data/AIRS_clean.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MASTER DATASET SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: airs_experiment/data/AIRS_clean.csv\")\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef60021",
   "metadata": {},
   "source": [
    "## 5. Create Population Splits\n",
    "\n",
    "Generate stratified dev/holdout splits for ALL three populations (70/30):\n",
    "- **Academic**: Full-time + Part-time students\n",
    "- **Professional**: Individual Contributors + Freelancers + Unemployed\n",
    "- **Leader**: Managers + Executives\n",
    "\n",
    "**Note**: Population filtering for specific experiments is done in `00b_Prepare_Experiment_Data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b7c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POPULATION FILTERING\n",
      "======================================================================\n",
      "Total responses: 513\n",
      "Valid populations (excl. 'Other'): 489\n",
      "Excluded ('Other'): 24\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING STRATIFIED SPLITS (50/50 for all populations)\n",
      "======================================================================\n",
      "\n",
      "ACADEMIC:\n",
      "  Total N: 198\n",
      "    - FT_Student: 182\n",
      "    - PT_Student: 16\n",
      "  Dev: N=99 (50%) - 4.1 cases/item âš ï¸ Marginal\n",
      "  Holdout: N=99 (50%) - âœ“ Adequate\n",
      "\n",
      "PROFESSIONAL:\n",
      "  Total N: 161\n",
      "    - IC: 113\n",
      "    - Freelancer: 31\n",
      "    - Unemployed: 17\n",
      "  Dev: N=80 (50%) - 3.3 cases/item âŒ Underpowered\n",
      "  Holdout: N=81 (50%) - âœ“ Adequate\n",
      "\n",
      "LEADER:\n",
      "  Total N: 130\n",
      "    - Manager: 74\n",
      "    - Executive: 56\n",
      "  Dev: N=65 (50%) - 2.7 cases/item âŒ Underpowered\n",
      "  Holdout: N=65 (50%) - âœ“ Adequate\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 5: STRATIFIED SPLITS FOR ALL POPULATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def create_stratified_split(data, test_size, stratify_cols, random_seed, population_name):\n",
    "    \"\"\"Create stratified train/test split with fallback for small strata.\"\"\"\n",
    "    data = data.copy()\n",
    "    data['_strat_key'] = data[stratify_cols].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    strat_counts = data['_strat_key'].value_counts()\n",
    "    if strat_counts.min() < 2:\n",
    "        print(f\"  âš ï¸ {population_name}: Some strata have <2 samples, using AI_Adoption only\")\n",
    "        data['_strat_key'] = data['AI_Adoption'].astype(str)\n",
    "    \n",
    "    try:\n",
    "        dev, holdout = train_test_split(\n",
    "            data, test_size=test_size, stratify=data['_strat_key'], random_state=random_seed\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(f\"  âš ï¸ {population_name}: Stratification failed, using random split\")\n",
    "        dev, holdout = train_test_split(data, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "    return dev.drop('_strat_key', axis=1), holdout.drop('_strat_key', axis=1)\n",
    "\n",
    "# Filter to valid populations only (exclude 'Other')\n",
    "df_valid = df[df['Population'].notna()].copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POPULATION FILTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total responses: {len(df)}\")\n",
    "print(f\"Valid populations (excl. 'Other'): {len(df_valid)}\")\n",
    "print(f\"Excluded ('Other'): {len(df) - len(df_valid)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Population configurations\n",
    "populations = {\n",
    "    'Academic': df_valid['Population'] == 'Academic',\n",
    "    'Professional': df_valid['Population'] == 'Professional',\n",
    "    'Leader': df_valid['Population'] == 'Leader'\n",
    "}\n",
    "\n",
    "split_results = {}\n",
    "\n",
    "dev_pct = int((1 - HOLDOUT_RATIO) * 100)\n",
    "holdout_pct = int(HOLDOUT_RATIO * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"CREATING STRATIFIED SPLITS ({dev_pct}/{holdout_pct} for all populations)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pop_name, pop_filter in populations.items():\n",
    "    print(f\"\\n{pop_name.upper()}:\")\n",
    "    \n",
    "    pop_data = df_valid[pop_filter].copy()\n",
    "    print(f\"  Total N: {len(pop_data)}\")\n",
    "    \n",
    "    # Role breakdown\n",
    "    role_breakdown = pop_data['Role_Category'].value_counts()\n",
    "    for role, n in role_breakdown.items():\n",
    "        print(f\"    - {role}: {n}\")\n",
    "    \n",
    "    # Create split using configured ratio\n",
    "    dev, holdout = create_stratified_split(\n",
    "        pop_data, test_size=HOLDOUT_RATIO, stratify_cols=['AI_Adoption'],\n",
    "        random_seed=RANDOM_SEED, population_name=pop_name\n",
    "    )\n",
    "    \n",
    "    # Save files with population prefix\n",
    "    pop_prefix = pop_name.lower()\n",
    "    dev.to_csv(f\"./data/AIRS_{pop_prefix}_dev.csv\", index=False)\n",
    "    holdout.to_csv(f\"./data/AIRS_{pop_prefix}_holdout.csv\", index=False)\n",
    "    \n",
    "    # Stats\n",
    "    n_items = 24\n",
    "    cases_per_item = len(dev) / n_items\n",
    "    efa_status = \"âœ“ Good\" if cases_per_item >= 5 else \"âš ï¸ Marginal\" if cases_per_item >= 4 else \"âŒ Underpowered\"\n",
    "    cfa_status = \"âœ“ Adequate\" if len(holdout) >= 50 else \"âš ï¸ Bootstrap needed\"\n",
    "    \n",
    "    print(f\"  Dev: N={len(dev)} ({len(dev)/len(pop_data)*100:.0f}%) - {cases_per_item:.1f} cases/item {efa_status}\")\n",
    "    print(f\"  Holdout: N={len(holdout)} ({len(holdout)/len(pop_data)*100:.0f}%) - {cfa_status}\")\n",
    "    \n",
    "    split_results[pop_prefix] = {\n",
    "        'total': len(pop_data),\n",
    "        'dev': len(dev),\n",
    "        'holdout': len(holdout),\n",
    "        'dev_df': dev,\n",
    "        'holdout_df': holdout\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2e338",
   "metadata": {},
   "source": [
    "## 6. Validate Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7698e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BALANCE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "ACADEMIC:\n",
      "  AI_Adoption: p=1.0000 âœ“ Balanced\n",
      "  Disability: p=1.0000 âœ“ Balanced\n",
      "  Mean item difference: 0.095 scale points - âœ“ Excellent\n",
      "\n",
      "PROFESSIONAL:\n",
      "  AI_Adoption: p=1.0000 âœ“ Balanced\n",
      "  Disability: p=0.1694 âœ“ Balanced\n",
      "  Mean item difference: 0.106 scale points - âœ“ Good\n",
      "\n",
      "LEADER:\n",
      "  AI_Adoption: p=1.0000 âœ“ Balanced\n",
      "  Disability: p=0.2722 âœ“ Balanced\n",
      "  Mean item difference: 0.096 scale points - âœ“ Excellent\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 6: VALIDATION - CHI-SQUARE BALANCE TESTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def test_balance(variable, dev, holdout, pop_name):\n",
    "    \"\"\"Chi-square test for balance between dev and holdout.\"\"\"\n",
    "    dev_counts = dev[variable].value_counts().sort_index()\n",
    "    holdout_counts = holdout[variable].value_counts().sort_index()\n",
    "    \n",
    "    all_values = sorted(set(dev_counts.index) | set(holdout_counts.index))\n",
    "    dev_counts = dev_counts.reindex(all_values, fill_value=0)\n",
    "    holdout_counts = holdout_counts.reindex(all_values, fill_value=0)\n",
    "    \n",
    "    contingency = pd.DataFrame({'Dev': dev_counts, 'Holdout': holdout_counts})\n",
    "    \n",
    "    if contingency.min().min() < 1:\n",
    "        return None, \"Insufficient data\"\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        status = \"âœ“ Balanced\" if p > 0.05 else \"âš  Imbalanced\"\n",
    "        return p, status\n",
    "    except:\n",
    "        return None, \"Test failed\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BALANCE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictor_items = ['PE1', 'PE2', 'EE1', 'EE2', 'SI1', 'SI2', 'FC1', 'FC2',\n",
    "                   'HM1', 'HM2', 'PV1', 'PV2', 'HB1', 'HB2', 'VO1', 'VO2',\n",
    "                   'TR1', 'TR2', 'EX1', 'EX2', 'ER1', 'ER2', 'AX1', 'AX2']\n",
    "\n",
    "for pop_name, results in split_results.items():\n",
    "    dev = results['dev_df']\n",
    "    holdout = results['holdout_df']\n",
    "    \n",
    "    print(f\"\\n{pop_name.upper()}:\")\n",
    "    \n",
    "    p, status = test_balance('AI_Adoption', dev, holdout, pop_name)\n",
    "    print(f\"  AI_Adoption: p={p:.4f} {status}\" if p else f\"  AI_Adoption: {status}\")\n",
    "    \n",
    "    p, status = test_balance('Disability_Binary', dev, holdout, pop_name)\n",
    "    print(f\"  Disability: p={p:.4f} {status}\" if p else f\"  Disability: {status}\")\n",
    "    \n",
    "    dev_means = dev[predictor_items].mean()\n",
    "    holdout_means = holdout[predictor_items].mean()\n",
    "    mean_diff = (dev_means - holdout_means).abs().mean()\n",
    "    \n",
    "    balance_status = \"âœ“ Excellent\" if mean_diff < 0.10 else \"âœ“ Good\" if mean_diff < 0.20 else \"âš  Review\"\n",
    "    print(f\"  Mean item difference: {mean_diff:.3f} scale points - {balance_status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d322818",
   "metadata": {},
   "source": [
    "## 7. Export Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aaef873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "METADATA EXPORTED\n",
      "======================================================================\n",
      "File: airs_experiment/data/airs_28item_complete.json\n",
      "Items: 28 (24 predictors + 4 outcomes)\n",
      "\n",
      "Analysis Structure:\n",
      "  EFA: 13 constructs (first-order, data-driven)\n",
      "  CFA: 5 domains (second-order, theory-driven)\n",
      "\n",
      "Population splits:\n",
      "  academic: N=198 (dev=99, holdout=99)\n",
      "  professional: N=161 (dev=80, holdout=81)\n",
      "  leader: N=130 (dev=65, holdout=65)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 7: EXPORT COMPREHENSIVE METADATA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from datetime import datetime\n",
    "\n",
    "airs_export = {\n",
    "    'created': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'source': 'airs_experiment/00a_Create_Split_Samples.ipynb',\n",
    "    'description': 'Complete AIRS item metadata including 24 predictors + 4 BI outcome items',\n",
    "    'n_predictor_items': len([k for k in item_semantic_metadata.keys() if not k.startswith('BI')]),\n",
    "    'n_outcome_items': len([k for k in item_semantic_metadata.keys() if k.startswith('BI')]),\n",
    "    'n_total_items': len(item_semantic_metadata),\n",
    "    'populations': {\n",
    "        pop: {'n_total': r['total'], 'n_dev': r['dev'], 'n_holdout': r['holdout']}\n",
    "        for pop, r in split_results.items()\n",
    "    },\n",
    "    'items': list(item_semantic_metadata.keys()),\n",
    "    'predictor_items': [k for k in item_semantic_metadata.keys() if not k.startswith('BI')],\n",
    "    'outcome_items': [k for k in item_semantic_metadata.keys() if k.startswith('BI')],\n",
    "    'metadata': item_semantic_metadata,\n",
    "    \n",
    "    # Two-level factor structure for analysis\n",
    "    # EFA: 12 constructs (data-driven, 2 items each)\n",
    "    # CFA: 5 domains as second-order factors grouping the 12 constructs\n",
    "    'constructs': construct_groups,              # 12 first-order (for EFA)\n",
    "    'theoretical_domains': THEORETICAL_DOMAINS,  # 5 second-order (for CFA)\n",
    "    'domain_groups': domain_groups,              # Items by domain\n",
    "    \n",
    "    'analysis_structure': {\n",
    "        'efa': {\n",
    "            'description': 'Data-driven exploration of 12 constructs (2 items each)',\n",
    "            'n_factors_expected': 12,\n",
    "            'constructs': list(construct_groups.keys())\n",
    "        },\n",
    "        'cfa': {\n",
    "            'description': 'Second-order CFA: 5 theoretical domains â†’ 12 constructs â†’ 24 items',\n",
    "            'n_second_order_factors': 5,\n",
    "            'domains': list(THEORETICAL_DOMAINS.keys()),\n",
    "            'domain_construct_mapping': THEORETICAL_DOMAINS\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'positive_items': [k for k, v in item_semantic_metadata.items() if v['direction'] == 'POSITIVE'],\n",
    "    'negative_items': [k for k, v in item_semantic_metadata.items() if v['direction'] == 'NEGATIVE']\n",
    "}\n",
    "\n",
    "with open('./data/airs_28item_complete.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(airs_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"METADATA EXPORTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: airs_experiment/data/airs_28item_complete.json\")\n",
    "print(f\"Items: {airs_export['n_total_items']} ({airs_export['n_predictor_items']} predictors + {airs_export['n_outcome_items']} outcomes)\")\n",
    "print(f\"\\nAnalysis Structure:\")\n",
    "print(f\"  EFA: {len(airs_export['constructs'])} constructs (first-order, data-driven)\")\n",
    "print(f\"  CFA: {len(airs_export['theoretical_domains'])} domains (second-order, theory-driven)\")\n",
    "print(\"\\nPopulation splits:\")\n",
    "for pop, info in airs_export['populations'].items():\n",
    "    print(f\"  {pop}: N={info['n_total']} (dev={info['n_dev']}, holdout={info['n_holdout']})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c5b3a",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a691968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOTEBOOK 00a COMPLETE - FULL DATASET PREPARATION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ FILES CREATED:\n",
      "\n",
      "  Master: airs_experiment/data/AIRS_clean.csv (N=513)\n",
      "\n",
      "  Academic:\n",
      "    AIRS_academic_dev.csv (N=99)\n",
      "    AIRS_academic_holdout.csv (N=99)\n",
      "\n",
      "  Professional:\n",
      "    AIRS_professional_dev.csv (N=80)\n",
      "    AIRS_professional_holdout.csv (N=81)\n",
      "\n",
      "  Leader:\n",
      "    AIRS_leader_dev.csv (N=65)\n",
      "    AIRS_leader_holdout.csv (N=65)\n",
      "\n",
      "  Metadata: airs_experiment/data/airs_28item_complete.json\n",
      "\n",
      "======================================================================\n",
      "âœ… NEXT: Run 00b_Prepare_Experiment_Data.ipynb to select population\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 8: FINAL SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK 00a COMPLETE - FULL DATASET PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ FILES CREATED:\")\n",
    "print(f\"\\n  Master: airs_experiment/data/AIRS_clean.csv (N={len(df)})\")\n",
    "\n",
    "for pop in ['academic', 'professional', 'leader']:\n",
    "    r = split_results[pop]\n",
    "    print(f\"\\n  {pop.title()}:\")\n",
    "    print(f\"    AIRS_{pop}_dev.csv (N={r['dev']})\")\n",
    "    print(f\"    AIRS_{pop}_holdout.csv (N={r['holdout']})\")\n",
    "\n",
    "print(f\"\\n  Metadata: airs_experiment/data/airs_28item_complete.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… NEXT: Run 00b_Prepare_Experiment_Data.ipynb to select population\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fed8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Method Summary (APA Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5e3496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Participants\n",
       "\n",
       "The sample consisted of *N* = 513 respondents recruited via convenience sampling through professional networks and academic channels. Of these, 24 respondents were excluded from population-specific analyses due to ambiguous role categorization (\"Other\"; *n* = 24).\n",
       "\n",
       "The remaining *N* = 489 respondents were categorized into three distinct populations:\n",
       "\n",
       "1. **Academic** (*n* = 198; 40.5%): Students, including full-time (*n* = 182) and part-time (*n* = 16) students.\n",
       "\n",
       "2. **Professional** (*n* = 161; 32.9%): Working individuals in non-leadership roles, including individual contributors (*n* = 113), freelancers/self-employed (*n* = 31), and those not currently employed (*n* = 17).\n",
       "\n",
       "3. **Leader** (*n* = 130; 26.6%): Individuals in leadership positions, including managers (*n* = 74) and executives/leaders (*n* = 56).\n",
       "\n",
       "Regarding AI adoption, 440 participants (90.0%) reported using at least one AI tool beyond \"never,\" while 49 participants (10.0%) were classified as non-adopters. Disability status was reported as follows: No (*n* = 413; 84.5%), Yes (*n* = 66; 13.5%), and Prefer not to answer (*n* = 10; 2.0%).\n",
       "\n",
       "### Data Preparation\n",
       "\n",
       "Data were prepared for a three-population analytical approach following recommendations for scale development and cross-validation (DeVellis & Thorpe, 2022). Each population was partitioned into development (70%) and holdout (30%) subsets using stratified random sampling with a fixed seed (*seed* = 67) to ensure reproducibility and enable cross-validation of factor structures.\n",
       "\n",
       "**Academic Sample.** The academic population (*N* = 198) yielded development (*n* = 99) and holdout (*n* = 99) samples.\n",
       "\n",
       "**Professional Sample.** The professional population (*N* = 161) yielded development (*n* = 80) and holdout (*n* = 81) samples.\n",
       "\n",
       "**Leader Sample.** The leader population (*N* = 130) yielded development (*n* = 65) and holdout (*n* = 65) samples. Given the smaller holdout size, bootstrap estimation was employed for confirmatory factor analysis to obtain robust standard errors.\n",
       "\n",
       "Stratification was performed on AI adoption status to ensure balanced representation across splits. Chi-square tests confirmed successful stratification across all populations (*p* > .05).\n",
       "\n",
       "### Measures\n",
       "\n",
       "The AI Readiness Scale (AIRS) comprises 28 items measuring 13 constructs: 24 predictor items representing 12 theoretically-derived constructs from UTAUT2 (Venkatesh et al., 2012) and AI-specific extensions, plus 4 Behavioral Intention outcome items. All items were measured on 5-point Likert scales (1 = *Strongly disagree* to 5 = *Strongly agree*).\n",
       "\n",
       "**UTAUT2 Core Constructs (H1):** Performance Expectancy (PE), Effort Expectancy (EE), Social Influence (SI), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HB), and Voluntariness (VO).\n",
       "\n",
       "**AI-Specific Constructs (H2):** Trust in AI (TR), Explainability (EX), Perceived Ethical Risk (ER), and AI Anxiety (AX).\n",
       "\n",
       "**Outcome:** Behavioral Intention (BI) was measured with 4 items assessing readiness, advocacy, future integration, and planned usage increase.\n",
       "\n",
       "Of the 28 items, 25 were positively worded (higher scores indicate greater adoption readiness) and 3 were negatively worded (ER1, ER2, AX1; higher scores indicate barriers to adoption).\n",
       "\n",
       "### References\n",
       "\n",
       "DeVellis, R. F., & Thorpe, C. T. (2022). *Scale development: Theory and applications* (5th ed.). Sage.\n",
       "\n",
       "Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. *MIS Quarterly*, *36*(1), 157â€“178. https://doi.org/10.2307/41410412\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 9: APA-FORMATTED METHOD SUMMARY (Dynamic)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Extract counts from data (only valid populations)\n",
    "n_total_all = len(df)\n",
    "n_valid = len(df_valid)\n",
    "n_excluded = n_total_all - n_valid\n",
    "\n",
    "n_leader = len(df_valid[df_valid['Population'] == 'Leader'])\n",
    "n_academic = len(df_valid[df_valid['Population'] == 'Academic'])\n",
    "n_professional = len(df_valid[df_valid['Population'] == 'Professional'])\n",
    "\n",
    "# Role breakdown (from original df)\n",
    "role_counts = df['Role'].value_counts()\n",
    "n_ft_student = role_counts.get('Full time student', 0)\n",
    "n_pt_student = role_counts.get('Part time student', 0)\n",
    "n_ic = role_counts.get('Employed - individual contributor', 0)\n",
    "n_manager = role_counts.get('Employed - manager', 0)\n",
    "n_exec = role_counts.get('Employed - executive or leader', 0)\n",
    "n_freelance = role_counts.get('Freelancer or self employed', 0)\n",
    "n_other = role_counts.get('Other', 0)\n",
    "n_unemployed = role_counts.get('Not currently employed', 0)\n",
    "\n",
    "# AI adoption (from valid sample)\n",
    "n_adopters = df_valid['AI_Adoption'].sum()\n",
    "n_non_adopters = n_valid - n_adopters\n",
    "pct_adopters = n_adopters / n_valid * 100\n",
    "pct_non_adopters = 100 - pct_adopters\n",
    "\n",
    "# Disability (from valid sample)\n",
    "disability_counts = df_valid['Disability'].value_counts()\n",
    "n_disability_no = disability_counts.get('No', 0)\n",
    "n_disability_yes = disability_counts.get('Yes', 0)\n",
    "n_disability_pna = disability_counts.get('Prefer not to answer', 0)\n",
    "pct_disability_no = n_disability_no / n_valid * 100\n",
    "pct_disability_yes = n_disability_yes / n_valid * 100\n",
    "pct_disability_pna = n_disability_pna / n_valid * 100\n",
    "\n",
    "# Population percentages\n",
    "pct_leader = n_leader / n_valid * 100\n",
    "pct_academic = n_academic / n_valid * 100\n",
    "pct_professional = n_professional / n_valid * 100\n",
    "\n",
    "# Split results\n",
    "leader_dev = split_results['leader']['dev']\n",
    "leader_holdout = split_results['leader']['holdout']\n",
    "academic_dev = split_results['academic']['dev']\n",
    "academic_holdout = split_results['academic']['holdout']\n",
    "prof_dev = split_results['professional']['dev']\n",
    "prof_holdout = split_results['professional']['holdout']\n",
    "\n",
    "# Item counts\n",
    "n_predictor_items = len([k for k in item_semantic_metadata.keys() if not k.startswith('BI')])\n",
    "n_outcome_items = len([k for k in item_semantic_metadata.keys() if k.startswith('BI')])\n",
    "n_total_items = len(item_semantic_metadata)\n",
    "n_positive_items = sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'POSITIVE')\n",
    "n_negative_items = sum(1 for m in item_semantic_metadata.values() if m['direction'] == 'NEGATIVE')\n",
    "n_constructs = len(construct_groups)\n",
    "\n",
    "# Generate APA text\n",
    "apa_text = f\"\"\"\n",
    "### Participants\n",
    "\n",
    "The sample consisted of *N* = {n_total_all} respondents recruited via convenience sampling through professional networks and academic channels. Of these, {n_excluded} respondents were excluded from population-specific analyses due to ambiguous role categorization (\"Other\"; *n* = {n_other}).\n",
    "\n",
    "The remaining *N* = {n_valid} respondents were categorized into three distinct populations:\n",
    "\n",
    "1. **Academic** (*n* = {n_academic}; {pct_academic:.1f}%): Students, including full-time (*n* = {n_ft_student}) and part-time (*n* = {n_pt_student}) students.\n",
    "\n",
    "2. **Professional** (*n* = {n_professional}; {pct_professional:.1f}%): Working individuals in non-leadership roles, including individual contributors (*n* = {n_ic}), freelancers/self-employed (*n* = {n_freelance}), and those not currently employed (*n* = {n_unemployed}).\n",
    "\n",
    "3. **Leader** (*n* = {n_leader}; {pct_leader:.1f}%): Individuals in leadership positions, including managers (*n* = {n_manager}) and executives/leaders (*n* = {n_exec}).\n",
    "\n",
    "Regarding AI adoption, {n_adopters} participants ({pct_adopters:.1f}%) reported using at least one AI tool beyond \"never,\" while {n_non_adopters} participants ({pct_non_adopters:.1f}%) were classified as non-adopters. Disability status was reported as follows: No (*n* = {n_disability_no}; {pct_disability_no:.1f}%), Yes (*n* = {n_disability_yes}; {pct_disability_yes:.1f}%), and Prefer not to answer (*n* = {n_disability_pna}; {pct_disability_pna:.1f}%).\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Data were prepared for a three-population analytical approach following recommendations for scale development and cross-validation (DeVellis & Thorpe, 2022). Each population was partitioned into development (70%) and holdout (30%) subsets using stratified random sampling with a fixed seed (*seed* = {RANDOM_SEED}) to ensure reproducibility and enable cross-validation of factor structures.\n",
    "\n",
    "**Academic Sample.** The academic population (*N* = {n_academic}) yielded development (*n* = {academic_dev}) and holdout (*n* = {academic_holdout}) samples.\n",
    "\n",
    "**Professional Sample.** The professional population (*N* = {n_professional}) yielded development (*n* = {prof_dev}) and holdout (*n* = {prof_holdout}) samples.\n",
    "\n",
    "**Leader Sample.** The leader population (*N* = {n_leader}) yielded development (*n* = {leader_dev}) and holdout (*n* = {leader_holdout}) samples. Given the smaller holdout size, bootstrap estimation was employed for confirmatory factor analysis to obtain robust standard errors.\n",
    "\n",
    "Stratification was performed on AI adoption status to ensure balanced representation across splits. Chi-square tests confirmed successful stratification across all populations (*p* > .05).\n",
    "\n",
    "### Measures\n",
    "\n",
    "The AI Readiness Scale (AIRS) comprises {n_total_items} items measuring {n_constructs} constructs: {n_predictor_items} predictor items representing 12 theoretically-derived constructs from UTAUT2 (Venkatesh et al., 2012) and AI-specific extensions, plus {n_outcome_items} Behavioral Intention outcome items. All items were measured on 5-point Likert scales (1 = *Strongly disagree* to 5 = *Strongly agree*).\n",
    "\n",
    "**UTAUT2 Core Constructs (H1):** Performance Expectancy (PE), Effort Expectancy (EE), Social Influence (SI), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HB), and Voluntariness (VO).\n",
    "\n",
    "**AI-Specific Constructs (H2):** Trust in AI (TR), Explainability (EX), Perceived Ethical Risk (ER), and AI Anxiety (AX).\n",
    "\n",
    "**Outcome:** Behavioral Intention (BI) was measured with {n_outcome_items} items assessing readiness, advocacy, future integration, and planned usage increase.\n",
    "\n",
    "Of the {n_total_items} items, {n_positive_items} were positively worded (higher scores indicate greater adoption readiness) and {n_negative_items} were negatively worded (ER1, ER2, AX1; higher scores indicate barriers to adoption).\n",
    "\n",
    "### References\n",
    "\n",
    "DeVellis, R. F., & Thorpe, C. T. (2022). *Scale development: Theory and applications* (5th ed.). Sage.\n",
    "\n",
    "Venkatesh, V., Thong, J. Y. L., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology. *MIS Quarterly*, *36*(1), 157â€“178. https://doi.org/10.2307/41410412\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(apa_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
